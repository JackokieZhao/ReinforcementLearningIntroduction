\chapter{第十六章 应用和案例研究}

\begin{summary}
	在这一章中，我们提出了一些强化学习的案例研究。其中一些是具有潜在经济意义的重要应用。其中之一，塞缪尔的西洋跳棋手，主要是历史兴趣。我们的演示旨在说明在实际应用程序中出现的一些权衡和问题。例如，我们强调领域知识是如何融入到问题的制定和解决中去的。我们还强调了对成功的应用程序通常至关重要的表示问题。在这些案例研究中使用的算法要比我们在本书其余部分中介绍的算法复杂得多。强化学习的应用还远未达到常规，通常需要和科学一样多的艺术。使应用程序更简单、更直接是当前强化学习研究的目标之一。
\end{summary}


\section{TD-Gammon}
迄今为止最令人印象深刻的强化学习应用之一是由杰拉德·特劳罗(Gerald Tesauro)在《backgammon游戏》(Tesauro, 1992, 1994, 1995, 2002)的游戏。Tesauro的计划TD-Gammon只需要很少的西洋双陆棋知识，但却能很好地发挥，接近世界上最强的大师水平。TD-Gammon的学习算法的一个简单的组合TD(λ)算法和非线性函数近似使用多层人工神经网络(ANN)的训练,backpropagating TD错误。
西洋双陆棋是一种主要的游戏，在世界各地都有，有许多比赛和常规的世界锦标赛比赛。这在一定程度上是一场碰运气的游戏，而且它是一种流行的工具，因为它能筹集大量的资金。也许有更多的职业西洋双陆棋棋手比有职业棋手。这款游戏由24个地点组成的棋盘上的15个白棋和15个黑棋组成。在下一页的右边显示了游戏早期的一个典型位置，从白人玩家的角度来看。怀特刚刚掷出骰子，得到5和2。这意味着他可以移动他的作品5步1步

一片白色逆时针方向移动
12 3 3 4 5 6 7 8 9 10 11 12 18 17 16 14 13 19 19 20 21 22 23 24
   黑块移动顺时针

西洋双陆棋的位置
(可能是同一件)2个步骤。例如，他可以从12点移动两段，一段移动到17点，另一段移动到14点。怀特的目标是把他所有的棋子推进最后一个象限(19-24分)，然后离开棋盘。第一个把所有棋子都拿走的玩家获胜。一个复杂的问题是，这些碎片相互作用，因为它们在不同的方向上彼此擦肩而过。例如，如果它是布莱克的移动，他可以用骰子滚2把一块从24点移动到22点，
“击中”白色部分。被击中的棋子被放置在棋盘中间的“条”上(我们已经在那里看到了一个先前被击中的黑色棋子)，在那里他们从一开始就重新进入了比赛。但是，如果一个点上有两个棋子，那么对手就不能移动到那个点;这些碎片不受撞击。因此，白棋不能使用他的5-2掷骰在1点上移动他的任何一个棋子，因为它们可能产生的点数被一组黑色棋子占据。形成连续的被占据点块来阻挡对手是游戏的基本策略之一。
《西洋双陆棋》涉及到更多的并发症，但是上面的描述提供了基本的思想。有了30个零件和24个可能的位置(26个，包括杆和板)，应该可以清楚地看到，可能的西洋双陆棋位置的数量是巨大的，远远超过任何物理可实现的计算机的内存元素的数量。从每个位置移动的可能性也很大。对于一个典型的骰子滚动，可能有20种不同的游戏方式。考虑到未来的动作，比如对手的反应，你必须考虑到可能的掷骰子。结果是，博弈树的有效分支因子约为400。这实在是太大了，以至于不能有效地使用传统的启发式搜索方法，这种方法在象棋和跳棋等游戏中是如此有效。
另一方面，这个游戏与TD学习方法的能力是很好的匹配。虽然游戏是高度随机的，但是游戏状态的完整描述在任何时候都是可用的。游戏通过一系列的移动和位置进行演变，最终以一方或另一方的胜利告终，结束游戏。这个结果可以被解释为一个最终的回报。另一方面，我们迄今所描述的理论结果不能有效地应用于这项任务。状态的数量如此之多，以至于无法使用查找表，而对手是不确定性和时间变化的来源。
TD-Gammon使用一种非线性的TD(λ)。估计价值,v̂(s,w),任何国家(板位置)是为了赢的概率估计从状态。为了达到这个目标,奖励被定义为零的时间所有步骤除游戏赢了。为了实现value函数，TD-Gammon使用一个标准的多层ANN，就像下一页右边显示的那样。(真正的

Vt + 1 Vt
隐藏的单位

西洋双陆棋位置(198输入单元)
预测获胜的可能性


TD错误,
………
圣+ 1 v̂(w)−v̂(St,w)
TD的错误
v̂(St,w)
隐藏的单位(40 - 80)
图16.1:TD-Gammon ANN
网络在其最后一层有两个额外的单元来估计每个玩家以一种特殊的方式获胜的概率，这种方式被称为“gammon”或“backgammon”。该网络的输入是一个backgammon位置的表示，输出是该位置的值的估计值。
在第一个版本的TD-Gammon, TD-Gammon 0.0中，西洋双陆棋的位置以一种相对直接的方式呈现在网络上，这种方式涉及到很少的西洋双陆棋知识。那样,但是，要对人工神经网络的工作原理和信息的最佳呈现方式有深入的了解。注意Tesauro所选择的精确表示是很有意义的。网络总共有198个输入单元。对于西洋双陆棋棋盘上的每一个点，四个单位表示在这个点上的白色棋子的数量。如果没有白色部分，那么所有四个单位的值都是0。如果有一件，那么第一个单位就值为1。这编码了“污点”的基本概念，即。一种可以被对手击中的棋子。如果有两个或多个部件，则第二个单元设置为1。这就编码了一个基本概念，即对手不能在其上着陆。如果这个点上正好有3个单位，那么第三个单位就设为1。这就编码了“单一备用”的基本概念，即。除了这两件作品之外，还有另外一件作品。最后，如果有三个以上的部件，那么第四个部件将被设置成与超过三个部件的数量成比例的值。让n表示点金币的总数,如果n > 3,那么第四单元的值(n−3)/ 2。它在给定的点上编码了“多个备件”的线性表示。
在这24个点上，白人4个单位，黑人4个单位，总共192个单位。两个额外的单位编码黑白块在酒吧的数量(每个值n / 2,其中n是碎片的数量栏),和两个编码黑白作品的数量已经成功地从棋盘上拿掉(这些值n / 15,其中n是碎片的数量已经承担了)。最后，两个单位以二进制的方式表示，无论是白色的还是黑色的。这些选择背后的一般逻辑应该是清楚的。基本上，Tesauro试图以一种简单的方式表示位置，同时保持单位的数量相对较少。他为每一个可能相关的概念上不同的可能性提供了一个单位，他将它们的范围大致扩大到相同的范围，在这个例子中是0到1之间。
给定一个西洋双陆棋位置的表示，网络以标准的方式计算其估计值。对应于从输入单元到隐藏单元的每个连接都是实值权重。每个输入单元的信号被乘以
它们相应的权重，在隐藏的单位上求和。隐藏单元j的输出h(j)是加权和的一个非线性的s - id函数:
 
习近平在第i的值是输入单元和维琪j隐藏连接单元的重量(所有网络传感器组合在一起形成了参数的权重向量w)。乙状结肠的输出总是在0和1之间,并有一个自然的解释是基于概率求和的证据。从隐藏单元到输出单元的计算完全类似。从隐藏单元到输出单元的每个连接都有一个单独的权重。输出单元形成加权和，然后通过相同的s形非线性进行传递。
TD-Gammon TD的semi-gradient形式使用(λ)12.2节中描述的算法,与梯度计算的误差反向传播算法(Rumel-hart、辛顿和威廉姆斯,1986)。回想一下，这种情况的一般更新规则是

wt + 1
.
= wt +α

Rt + 1 +γv̂(wt)圣+ 1−v̂(St,wt)

zt型, 					(16.1)

其中wt是所有可修改参数的矢量(在本例中是网络的权重)，而zt是一个合格跟踪的矢量，每个wt的每个组件都有一个，由?

zt型
。=γλzt−1 +∇v̂(St,wt),
与z0
。= 0。这个方程的梯度可以用反相法有效地计算出来
gation过程。西洋双陆棋应用程序中,γ= 1和奖励总是零除了获胜,TD的错误部分的学习规则通常只是v̂(w)圣+ 1−v̂(St,w),是显示在图16.1。
为了应用学习规则，我们需要一个西洋双陆棋游戏的来源。通过在比赛中学习西洋双足球员，他获得了连续不断的比赛。为了选择它的动作，TD-Gammon考虑了大约20种掷骰子的方式以及相应的位置。产生的位置是第6.8节所讨论的后态。该网络被用来评估他们的价值。然后选择将导致估计值最高的位置的移动。继续以这种方式，随着TD-Gammon对双方的移动，很容易产生大量的西洋双陆棋游戏。每个游戏都被视为一个插曲，位置序列作为状态，S0, S1, S2，…Tesauro使用非线性TD规则(16.1)完全增量地，也就是说，在每个单独的移动之后。
网络的权值最初设定为小的随机值。因此，最初的评价完全是武断的。因为这些动作是在这些评价的基础上进行的，所以最初的动作是不可避免的，而最初的游戏通常会在一方或另一方获胜之前，持续数百或数千步，几乎是偶然的。然而，在几十场比赛之后，成绩迅速提高。
在与自己打了大约30万场比赛之后，TD-Gammon 0.0就像上面描述的那样，学会了和之前最好的西洋双陆棋一样玩

项目。这是一个惊人的结果，因为所有先前的高性能计算机程序都使用了大量的西洋双陆棋知识。举个例子，当时的冠军项目，可以说是“神经gammon”，另一个由Tesauro编写的程序，使用的是ANN，而不是TD学习。Neurogammon的网络被训练在一个由西洋双陆棋专家提供的大型训练语料库上，此外，开始时还为西洋双陆棋特别设计了一套特征。Neurogammon是一个高调频的，高效的backgammon项目，在1989年决定性地赢得了世界backgammon奥运会。另一方面，TD-Gammon 0.0的构造基本为零。它能做的和神经gammon一样，所有其他的方法都证明了自我游戏学习方法的潜力。
TD- gammon 0.0的锦标赛成功与零专家的backgammon知识建议一个明显的修改:增加专门的backgammon特性，但保留自玩TD学习方法。这就造成了TD-Gammon 1.0。TD-Gammon 1.0显然比以前所有的backgammon程序都要好，并且只在人类专家中发现了严重的竞争。后来版本的程序，TD-Gammon 2.0(40个隐藏单元)和TD-Gammon 2.1(80个隐藏单元)，通过一个选择性的两层搜索程序进行扩充。要选择动作，这些程序不仅要看马上会出现的位置，还要看对手可能掷出的骰子和动作。假设对手总是采取对他最有利的动作，每个候选动作的预期值都被计算出来，最好的结果被选中。为了节省计算机时间，第二次搜索只对在第一次搜索之后排名较高的候选动作进行，平均大约有四五个动作。双层搜索只影响选定的移动;学习过程和以前一样。最终版本的TD-Gammon 3.0和3.1使用了160个隐藏单元和选择性的三层搜索。TD-Gammon举例说明学习值函数和决策时间搜索在启发式搜索和MCTS方法中的结合。在后续的工作中,Tesauro和加尔佩林(1997)探讨了轨迹采样方法代替宽屏搜索,减少错误率的生活玩大数值因素(4 x-6x)同时保持∼5 - 10秒的思考时间合理的举动。
在20世纪90年代，特索罗能够在许多比赛中与世界级的人类选手比赛。结果摘要载于表16.1。




程序	隐藏单位40 80 40 80 80 80 80 80 80	培训游戏
30万80万150万	对手	结果
TD-Gammon 0.0 TD-Gammon 1.0 TD-Gammon 2.0 TD-Gammon 2.1 TD-Gammon 3.0			其他节目罗伯特，马格里尔，……各种大师罗伯特·卡扎罗斯的节目	并列最佳−−13分/ 51游戏7分/ 38游戏−1 pt / 40游戏+ 6分/ 20场比赛

表16.1:TD-Gammon结果摘要

基于这些结果和西洋双陆棋大师的分析(Robertie, 1992;TD-Gammon 3.0似乎在接近或可能比世界上最优秀的人类选手发挥的力量要好。Tesauro在随后的一篇文章(Tesauro, 2002)中报道了对TD-Gammon相对于顶级玩家的移动决策和加倍决策进行了广泛的分析的结果。得出的结论是，TD-Gammon 3.1在计件运动决策中具有“不平衡优势”，在加倍决策中具有“微弱优势”，超过了人类。
TD-Gammon对最优秀的人类玩家的游戏方式产生了重大影响。例如，它学会了在某些开局位置上的表现与人类最好球员之间的惯例不同。基于TD-Gammon的成功和进一步的分析，现在最优秀的人类玩家可以像TD-Gammon一样扮演这些角色(Tesauro, 1995)。当其他几个受TD-Gammon启发的ANN backgammon项目，如水母、Snowie和GNUBackgammon被广泛使用时，对人类游戏的影响大大加快。这些项目使ANNs产生的新知识得以广泛传播，从而大大提高了人类锦标赛的整体水平(Tesauro, 2002)。


\section{塞缪尔跳棋的球员}

泰索罗的TD-Gammon的一个重要先驱是阿瑟·萨缪尔(1959年，1967年)在构建跳棋程序方面的开创性工作。塞缪尔是第一个有效利用启发式搜索方法的人，我们现在称之为时间性差异学习。他的跳棋棋手除了具有历史意义外，还是很有启发性的个案研究。我们强调撒母耳方法与现代强化学习方法的关系，并试图传达撒母耳使用这些方法的动机。

1952年，塞缪尔首次为IBM 701编写了一个下棋程序。他的第一个学习项目在1955年完成，并在1956年在电视上展示。后来版本的程序获得了良好的，但不是专家，演奏技巧。塞缪尔被游戏作为机器学习的一个研究领域所吸引，因为游戏比“从生活中拿走”的问题要简单，同时还允许对启发式过程和学习如何一起使用进行富有成果的研究。他选择学习跳棋而不是象棋，因为它的相对简单性使人们有可能更注重学习。

塞缪尔的程序通过在每个当前位置执行一个前瞻搜索来发挥作用。他们使用我们现在所说的启发式搜索方法来确定如何扩展搜索树以及何时停止搜索。每个搜索的终端位置都被一个值函数或“得分多项式”用线性函数逼近来评价或“得分”。在这方面和其他方面，塞缪尔的作品似乎受到了香农(1950)的建议的启发。特别是，塞缪尔的计划是基于香农的极小化过程，以找到最佳的移动从目前的位置。在搜索树中从得分终端位置向后移动，每一个位置都得到最佳移动的位置的分数，假设机器总是试图最大化分数，而对手总是试图最大化分数

最小化。塞缪尔称这个位置为“支撑得分”。当极小极大过程到达搜索树的根——当前位置——它产生了最佳的移动，假设对手将使用相同的评价标准，转移到它的观点。一些版本的Samuel的程序使用复杂的搜索控制方法，类似于所谓的“alpha-beta”cutoffs(例如，参见Pearl, 1984)。
塞缪尔采用了两种主要的学习方法，最简单的一种叫做死记硬背。它只包括保存游戏中遇到的每个棋盘位置的描述，以及由极大极小化过程确定的支撑值。结果是，如果已经遇到的位置再次作为搜索树的终端位置出现，那么搜索的深度就会被有效地放大，因为这个位置的存储值缓存了之前进行的一个或多个搜索的结果。最初的一个问题是，该项目没有被鼓励沿着最直接的途径取得胜利。萨缪尔给了它一种“方向感”，即在极小极大分析期间，当一个位置每次被备份到一个水平(称为层)时，它的价值就会减少一小部分。“如果程序现在面临的选择是董事会的职位，其分数只与层数不同，它将自动做出最有利的选择，如果赢了，选择低层的选择;如果输了，选择高层的选择”(Samuel, 1959，第80页)。塞缪尔发现这种类似折扣的技巧对于成功的学习至关重要。死记硬背的学习产生了缓慢但持续的改进，这对于开始和结束游戏是最有效的。他的程序在从许多游戏中学习到对抗自己、各种各样的人类对手，以及在监督学习模式下的书本游戏之后，成为了一个“比一般人更好的新手”。

死记硬背和塞缪尔作品的其他方面强烈地暗示了时间差异学习的基本思想——一个国家的价值应该等于可能遵循的国家的价值。Samuel在他的第二种学习方法中最接近这个想法，他的“泛化学习”程序修改了值函数的参数。塞缪尔的方法在概念上和后来泰索罗在《泰德-加蒙》中使用的方法是一样的。他在自己的程序中与另一个版本的程序进行了很多游戏，每次移动后都进行更新。Samuel更新的想法是由图16.2中的备份图建议的。每个开环表示程序下一步移动的位置，一个正在移动的位置，每个实环表示对手下一步移动的位置。对每一方移动后的每个移动位置的值进行更新，从而产生第二个移动位置。这次更新是针对从第二个移动位置启动的搜索的极小值。因此，总体效果是对真实事件的一个完整移动进行备份，然后对可能的事件进行搜索，如图16.2所示。由于计算的原因，塞缪尔的实际算法要比这个复杂得多，但这是基本的思想。
塞缪尔没有明确的奖励。相反,他固定的重量最重要的功能,这篇文章的优势特性,测量块的数量项目相对于其对手有多少,给国王,更高的权重,包括改进,以便更好的贸易当胜利比失败。因此，塞缪尔计划的目标是提高他的计分优势，这在跳棋中与获胜高度相关。

图16.2:萨缪尔跳棋运动员的备份图。

然而，塞缪尔的学习方法可能漏掉了一个声音时间差算法的一个重要部分。时间性差异学习可以看作是一种使价值函数与自身一致的方法，这一点我们可以从塞缪尔的方法中清楚地看到。但还需要一种将值函数与状态的真实值绑定的方法。我们通过奖励和贴现或给终端状态一个固定的值来实现这一点。但萨缪尔的方法没有奖励，也没有对游戏的最终位置进行特殊处理。正如塞缪尔自己所指出的那样，他的价值函数仅仅通过给所有的位置赋予一个恒定的值就可以变得一致。他希望通过给他的计件优势条款一个巨大的、不可改变的权重来阻止这种解决方案。但是，尽管这可能会降低找到无用的评估函数的可能性，但它并没有禁止它们。例如，一个常数函数仍然可以通过设置可修改的权值来获得，从而抵消不可修改权值的影响。

因为塞缪尔的学习过程并不局限于寻找有用的评价函数，所以它本应该随着经验而变得更糟。事实上，萨缪尔报告说他在大量的自我游戏训练中观察到了这一点。为了让项目再次得到改善，塞缪尔不得不进行干预，将绝对值最大的权重设为零。他的解释是，这种激烈的干预使程序脱离了局部最优状态，但另一种可能性是，它使程序脱离了评价函数，这些函数是一致的，但与游戏的成败无关。

尽管有这些潜在的问题，萨缪尔的西洋跳棋玩家使用泛化学习方法接近“好于平均水平”的游戏。相当不错的业余对手将其描述为“狡猾但可击败”(塞缪尔，1959)。与死记硬背的版本相比，这个版本能够开发一个好的中间游戏，但是在开始和结束的游戏中仍然很弱。这个程序还包含了一种搜索一系列特性的能力，以找到在形成value函数时最有用的特性。后来的版本(Samuel, 1967)在搜索过程中加入了一些改进，比如alpha-beta剪枝，广泛使用被称为“书本学习”的监督学习模式，以及称为“签名表”(Griffith, 1966)的分层查找表，以表示值函数而不是线性函数近似。这个版本比1959年的版本学得好多了，虽然还没有达到大师的水平。塞缪尔的跳棋程序被广泛认为是人工智能和机器学习方面的重大成就。


\section{沃森的双倍下注}

IBM Watson1是一个由IBM研究人员开发的系统，用于播放广受欢迎的电视智力竞赛节目《危险边缘》。2011年，它在与人类冠军的表演赛中获得一等奖，一举成名。尽管沃森所展示的主要技术成就是它能够快速准确地回答自然语言的问题，而不是广泛的知识领域，但它的胜利是危险的!游戏的关键部分还依赖于复杂的决策策略。Tesauro, Gondek, Lechner, Fan和Prager(2012, 2013)采用了上面描述的Tesauro的TD-Gammon系统，创造了Watson在《每日加倍》(Daily-Double) (DD)中所使用的策略，在与人类冠军的比赛中赢得胜利。这些作者报告说，这种下注策略的有效性远远超出了人类玩家在实时游戏中所能做的，而且它与其他高级策略一起，是沃森令人印象深刻的获胜表现的重要因素。在这里，我们只关注DD的赌注，因为沃森的组件更多地依赖于强化学习。

冒险!由三名选手扮演，他们面对着一个显示30个方块的棋盘，每个方块都隐藏着一个线索，并且有一美元的价值。正方形被分成六列，每一列对应一个不同的类别。参赛者选择一个方块，主持人阅读方块的提示，每个参赛者可以选择按蜂鸣器(“嗡嗡”)回应提示。第一个参与比赛的选手试图对这个线索做出回应。如果这个参赛者的回答是正确的，那么他们的分数将以正方形的美元值增加;如果他们的回答不正确，或者他们在五秒内没有回答，他们的分数就会下降这么多，而其他参赛者就有机会对同样的线索做出回应。一个或两个方块(取决于游戏当前的回合)是特殊的DD方块。选择其中一个的选手得到一个独家的机会来回应这个方的线索，并且必须在线索被揭示之前做出决定——赌多少钱，赌多少。赌注必须大于5美元，但不能超过选手的当前分数。如果参赛者对DD线索反应正确，他们的分数将增加赌注金额;否则，它会因下注金额而减少。每一场比赛的结尾都是一场“最后的危险”(FJ)游戏，每位参赛者都要写下一个密封的赌注，然后在阅读线索后写下一个答案。三轮比赛后得分最高的选手(每一轮包含30条线索)获胜。游戏还有很多其他的细节，但是这些已经足够让人欣赏了

1 IBM公司注册商标。
2 .《危险产品》的注册商标。

DD赌博的重要性。输赢往往取决于参赛者的DD赌博策略。
当沃森选择DD广场,它选择了赌值,通过比较行动问̂(年代,赌),估计赢的概率从当前游戏状态,为每个round-dollar年代,法律选择。除了下面描述的一些降低风险的措施，Watson选择了具有最大动作值的赌注。当需要进行博彩决策时，通过使用两种估计来计算动作值，这两种估计是在任何现场游戏发生之前就已经学会的。第一个是选择每个合法赌注后各州的估计值(第6.8节)。这些估计从州值函数,获得v̂(·w),定义为参数w,让赢的概率的估计沃森从任何游戏状态。第二个用于计算动作值的估计值给出了“类内DD置信度”pDD，该估计值估计了Watson对尚未发现的DD线索做出正确响应的可能性。

Tesauro等人使用的强化学习方法学习上述TD-Gammon v̂(·w):一个简单的非线性组合TD(λ)使用多层安重量w培训backpropagating TD错误在很多模拟游戏。状态由专门为Jeopardy设计的特征向量表示到网络上。特征包括三名玩家当前的分数、剩余的DDs数量、剩余线索的总价以及与游戏剩余游戏数量相关的其他信息。与TD-Gammon self-play学到,沃森的v̂就是数以百万计的模拟比赛精明狡猾的人类玩家的模型。在类别内的信心估计取决于沃森在当前类别中给出的正确答案r和错误答案w的数量。对(r, w)的依赖关系是根据沃森对数千个历史类别的实际准确性来估计的。
以前学到的价值函数v̂,集中DD信心pDD,沃森计算q̂(年代,打赌)为每个法律round-dollar选择如下:

问̂(年代,打赌)= pDD×v̂(SW +打赌。)+(1−pDD)×v̂(SW−打赌。)(16.2),西南是沃森的当前的分数,和v̂给游戏状态的估计价值沃森的DD反应线索后,这是正确的或不正确的。计算这样一个行动值对应的洞察力从3.19运动动作值给定的期望下一个状态值操作(除了在这儿下afterstate预期值,因为整个游戏的下一个状态取决于下一个平方的选择)。

Tesauro等人发现，通过最大化动作值来选择赌注会产生“令人恐惧的风险”，这意味着如果Watson对线索的反应恰好是错误的，那么这个损失对于它获胜的机会来说可能是灾难性的。为了降低错误答案的下行风险，Tesauro等人通过减去Watson正确/错误的后态评估的一小部分标准差来调整(16.2)。他们进一步降低了风险，因为他们禁止了将导致错误回答的事后价值降低到一定限度以下的赌注。这些措施略微降低了沃森对获胜的预期，但它们显著降低了下跌风险，不仅是在平均风险的风险下，而且在极端风险的情况下，风险中性的沃森会押注大部分或全部的资金。

为什么TD-Gammon self-play不习惯学习临界值的方法函数v̂?在危险中自娱自乐!因为沃森和其他任何人类选手都不一样，所以不会有很好的表现。自我游戏将导致探索国家空间区域，这不是对抗人类对手，特别是人类冠军的典型方式。此外，不像西洋双陆棋，危险!这是一个信息不完全的游戏，因为参赛者不能接触到所有影响他们对手比赛的信息。特别是,冒险!选手们不知道他们的对手对各种各样的线索有多大的信心。自玩就像和一个和你持相同牌的人玩扑克一样。

由于这些复杂性，开发沃森的DD-wagering策略的大部分努力都被用于创建人类对手的良好模型。这些模型没有涉及到游戏的自然语言方面，而是游戏中可能发生的事件的随机过程模型。统计数据是从一个广泛的粉丝创建的存档游戏信息从展览开始到现在。档案包括信息，如排序的线索，正确和错误的参赛者答案，DD地点，DD和FJ的赌注，为近30万线索。构建了三个模型:一个平均参赛模型(基于所有数据)，一个冠军模型(基于100个最佳玩家的游戏统计)，一个大冠军模型(基于10个最佳玩家的游戏统计)。除了在学习过程中充当对手外，这些模型还被用来评估学习的DD-wagering策略所带来的好处。沃森在模拟中使用基线启发式的DD-wagering策略时的胜率为61\%;当它使用学习值和一个默认的置信值时，它的赢球率增加到64\%;而对生活类别的置信度，则是67\%。Tesauro等人认为这是一个显著的改进，因为DD在每场比赛中只需要1.5到2次。

因为沃森只有几秒钟的时间来打赌，以及选择方块并决定是否加入，所以做出这些决定所需的计算时间是一个关键因素。的安实现v̂允许DD的赌注是足够快以满足时间约束的生活。然而,一旦游戏可以通过改善仿真软件模拟的速度不够快,在比赛快结束的时候估计是可行的投资的价值在许多蒙特卡罗试验平均每个选择的结果是由模拟发挥游戏的结束。选择基于蒙特卡罗试验而不是ANN的现场比赛中的endgame DD投注明显提高了Watson的表现，因为在终场比赛中价值估计的错误会严重影响Watson获胜的机会。通过蒙特卡罗试验做出所有的决定可能会带来更好的赌注决定，但考虑到游戏的复杂性和实时游戏的时间限制，这是完全不可能的。
虽然它快速准确地回答自然语言问题的能力是沃森的主要成就，但它所有复杂的决策策略都导致了它令人印象深刻的击败人类冠军。根据Tesauro等(2012):

…很明显，我们的策略算法实现了超过人类能力的定量精确和实时性能。

这一点在DD赌博和游戏结束时的嗡嗡声中尤为明显，在这种情况下，人类根本无法与沃森所做的精确的公平、信心估计以及复杂的决策计算相匹配。



\section{优化内存控制}

大多数计算机以动态随机存取存储器(DRAM)作为主要存储器，因为它的成本低、容量大。DRAM存储器控制器的工作是有效地利用处理器芯片和非芯片DRAM系统之间的接口，提供高速程序执行所需的高带宽和低延迟数据传输。内存控制器需要处理动态变化的读/写请求模式，同时遵守硬件所需的大量时间和资源限制。这是一个可怕的调度问题，特别是对于具有多个内核共享相同DRAM的现代处理器来说。
我̇油漆、Mutlu集市́ınez,(2008)和Caruana(也集市́ınez我̇油漆,2009)设计了一种强化学习内存控制器和显示,它可以大大提高程序执行的速度是可能的与传统控制器的时候他们的研究。它们的动机是现有最先进的控制器的限制，这些控制器使用的策略没有利用过去的调度经验，也没有考虑到调度决策的长期后果。我̇油漆等的项目是由模拟的手段,但是他们设计控制器的硬件实现,包括所需的详细级别学习algorithm-directly处理器芯片。
访问DRAM涉及许多步骤，这些步骤必须根据严格的时间限制来完成。DRAM系统由多个DRAM芯片组成，每个芯片都包含以行和列排列的多个矩形存储单元阵列。每个电池在电容器上储存一点电荷。由于电荷随时间减少，每个DRAM单元需要每隔几毫秒重新充电一次，以防止内存内容丢失。这需要刷新单元格，这就是为什么DRAM被称为“dynamic”。
每个单元格数组都有一个行缓冲区，其中包含一行位元，这些位元可以转移到数组的某一行或其中的某一行。激活命令“打开一行”，这意味着将其地址由命令指示的行内容移动到行缓冲区。当一行打开时，控制器可以向单元格数组发出读写命令。每个read命令将行缓冲区中的一个单词(连续位的短序列)传输到外部数据总线，每个write命令将外部数据总线中的一个单词传输到行缓冲区。在打开不同的行之前，必须发出一个precharge命令，该命令将行缓冲区中的(可能更新的)数据传输回单元阵列的寻址行。在此之后，另一个activate命令可以打开要访问的新行。读和写命令是列命令，因为它们顺序地将位传递到行缓冲区的列或列之外;可以在不重新打开行的情况下传输多个位。对当前打开的行执行读写命令比访问不同的行要快，这将涉及额外的行命令:预充和激活;这是有时


被称为“行局部性”。“内存控制器维护一个内存事务队列，该队列存储共享内存系统的处理器的内存访问请求。控制器必须通过向内存系统发出命令来处理请求，同时遵守大量的时间限制。
一个控制器的调度访问请求的策略可以对内存系统的性能产生很大的影响，比如可以满足请求的平均延迟，以及系统能够实现的吞吐量。最简单的调度策略按照请求到达的顺序处理访问请求，方法是在开始为下一个请求提供服务之前发出请求所需的所有命令。但是，如果系统没有为这些命令之一做好准备，或者执行一个命令会导致资源被充分利用(例如，由于服务一个命令而导致的时间限制)，那么在完成旧的请求之前就开始处理新请求是有意义的。策略可以通过重新排序请求来提高效率，例如，将读请求的优先级放在写请求之上，或者将读/写命令的优先级放在已经打开的行上。称为“先到先得”、“先到先得”(FR-FCFS)的策略将列命令(读和写)置于行命令(激活和预充)之上，并且在连接时优先级为最老的命令。在通常遇到的情况下，FR-FCFS在平均内存访问延迟方面表现优于其他调度策略(Rixner, 2004)。
图16.3是我的高级视图̇油漆等的强化学习内存控制器。他们将DRAM访问过程建模为MDP，其状态是事务队列的内容，其操作是对DRAM系统的命令:预充电、激活、读取、写入和NoOp。当动作被读或写时，奖励信号为1，否则为0。状态转换被认为是随机的，因为系统的下一个状态不仅依赖于调度程序的命令，而且还取决于调度程序无法控制的系统行为的各个方面，比如处理器内核访问DRAM系统的工作负载。
 

图16.3:增强学习DRAM控制器的高级视图。调度程序是强化学习代理。它的环境由事务队列的特性表示，它的操作是对DRAM系统的命令。c ? IEEE 2009。经许可转载,从j . f .集市́ınez和大肠我̇油漆,动态多核资源管理:机器学习的方法,微,IEEE,29(5),p。12。

MDP的关键是对每个状态中可用的操作的约束。回忆从第三章组可用的操作依赖于状态:∈(St),在时间步的行动t和可用的行动(St)是一组在州圣在这个应用程序中,DRAM的完整性保证系统不允许违反时间或资源约束的行为。虽然我̇油漆等人并不明确,他们有效地完成这种根据所有可能状态的设置(St)。
这些约束解释了为什么MDP有一个NoOp操作，为什么奖励信号是0，除了发出读或写命令。NoOp是指一个州内唯一的法律行为。最大化利用的内存系统,控制器的任务是驱动的系统状态可以选择读或写操作:只有这些行动导致通过外部数据总线发送数据,所以只有这些,有助于系统的吞吐量。虽然precharge和activate不会立即产生回报，但是代理需要选择这些操作，以使以后选择奖励的读和写操作成为可能。

调度代理使用Sarsa(第6.4节)学习动作-值函数。国家由6个整数值的特征表示。为了逼近动值函数，算法采用线性函数近似，采用散列编码实现线性函数近似(第9.5.4节)。瓷砖编码有32个倾斜，每一个都存储256个动作值作为16位定点数字。探索ε-greedyε= 0.05。
状态特性包括事务的读请求队列的数量,数量的事务中写请求队列,写请求的数量的事务队列等待他们的行被打开,和读请求的数量在事务队列等待他们行被打开,他们发行的最古老的请求处理器。(其他特性取决于DRAM如何与缓存内存交互，这里省略了一些细节)。国家特征的选择是基于我̇油漆等的DRAM性能因素的影响的理解。例如，根据事务队列中每个事务队列的数量来平衡服务读和写的速率可以帮助避免延迟DRAM系统与缓存内存的交互。作者实际上生成了一个相对较长的潜在特性列表，然后使用逐步特性选择指导的模拟将它们缩减为几个。
将调度问题作为MDP表示的一个有趣的方面是，用于定义动作值函数的块编码的特性输入与用于指定动作约束集A(St)的特性不同。虽然块编码输入来自事务队列的内容，但是约束集依赖于许多与时间和资源约束相关的其他特性，这些特性必须由整个系统的硬件实现来满足。这样，动作约束保证了学习算法的探索不会危及物理系统的完整性，而学习被有效地限制在硬件实现更大状态空间的“安全”区域。

因为这项工作的一个目标是，学习控制器可以在芯片上实现，以便在计算机运行时在线学习，硬件实现细节是重要的考虑因素。设计包括两个五阶段管道，用于计算和比较每个处理器时钟周期的两个动作值，以及更新适当的操作值。这包括访问存储在静态RAM中的芯片上的块代码。配置我̇油漆等人模拟,这是一个4 ghz基于芯片的典型高端工作站时他们的研究,有10个处理器周期为每个DRAM周期。考虑到填充管道所需的周期，在每个DRAM循环中最多可以评估12个操作。我̇油漆等人发现法律命令,任何一个国家都是很少的数量大于,性能损失是微不足道的如果足够的时间并不总是可以考虑所有合法的命令。这些巧妙的设计细节使得在多处理器芯片上实现完整的控制器和学习算法成为可能。

我̇油漆等人评价他们的学习控制器仿真通过比较它与其他三个控制器:1)上述FR-FCFS控制器产生最好的平均性能,2)常规控制器处理每个请求,和3)理想无法实现的控制器,控制器称为乐观,能够维持100\% DRAM吞吐量如果有足够的需求,忽略了所有的时间和资源约束,否则建模DRAM延迟(如行缓冲撞击)和带宽。他们模拟了由科学和数据挖掘应用程序组成的9个内存密集型并行工作负载。图16.4显示了9个应用程序的每个控制器的性能(执行时间的倒数，归一化为FR-FCFS的性能)，以及它们在应用程序中的性能的几何平均值。在图中标注为RL的学习控制器比FR-FCFS提高了7\%到33\%，平均提高了19\%。
由于芯片实现的基本原理的学习算法是允许在线调度策略适应不断变化的工作负载,我̇油漆等人分析了在线学习的影响比之前学的固定的政策。他们训练
 
 

图16.4:在一组9个模拟基准应用程序中，4个控制器的性能。
控制器是:最简单的“有序”控制器、FR-FCFS、学习控制器RL和不可实现的乐观控制器，它们忽略了所有的时间和资源约束
提供一个性能上限。性能，归一化到FR-FCFS的性能，是倒数
的执行时间。最右边是每个控制器在9个基准应用程序上性能的几何平均值。控制器RL最接近理想的性能。2009 c ?
IEEE。经许可转载,从j . f .集市́ınez和大肠我̇油漆,动态多核资源
管理:机器学习方法，Micro, IEEE, 29(5)， p. 13。


它们的控制器使用来自所有9个基准应用程序的数据，然后在整个应用程序的模拟执行过程中保持结果操作值不变。他们发现，在线学习的控制器的平均性能比使用固定策略学习的控制器的平均性能好8\%，这使他们得出结论，在线学习是他们方法的一个重要特性。
这种学习记忆体控制器从来没有致力于物理硬件，因为制造成本很高。不过,我̇油漆等人可以令人信服地说他们的仿真结果的基础上,一个内存控制器,通过强化学习网上学习有可能提高性能水平,否则需要更复杂和更昂贵的记忆系统,同时删除从人类设计师的一些负担需要手动设计高效的调度策略。Mukundan和集市́ınez(2012)把这个项目提前调查学习控制器与额外的动作,其他性能标准,使用遗传算法和更复杂的奖励函数派生。他们考虑了与能源效率有关的其他性能标准。这些研究的结果超过了前面描述的结果，并且在他们考虑的所有性能标准上显著地超过了2012年的最先进水平。这种方法对于开发复杂的power-aware DRAM接口特别有希望。


\section{人机级的游戏}

将增强学习应用于实际问题的最大挑战之一是决定如何表示和存储价值函数和/或策略。除非状态集是有限的，并且足够小，可以通过查找表来进行详尽的表示——就像我们的许多示例示例一样——必须使用参数化函数近似方案。无论是线性的还是非线性的，函数逼近都依赖于一些特性，这些特性必须易于学习系统获得，并且能够传递熟练性能所必需的信息。强化学习的大多数成功应用，在很大程度上都归功于基于人类知识和对要解决的特定问题的直觉而精心设计的一系列特性。
谷歌DeepMind的一组研究人员开发了一个令人印象深刻的演示，即一个深层多层ANN可以自动完成特性设计过程(Mnih et al.， 2013, 2015)。自1986年将反向传播算法推广为学习内部表示方法以来，多层ANNs被用于增强学习中的函数逼近(Rumelhart, Hinton, Williams, 1986);见9.6节)。将强化学习与反推结合，得到了显著的效果。特索罗和同事们用TD-Gammon和Watson得到的结果是值得注意的例子。这些应用程序和其他应用程序得益于多层ANNs学习任务相关特性的能力。然而，在我们所知道的所有示例中，最令人印象深刻的演示要求网络的输入以针对给定问题手工制作的特殊特性来表示。这在TD-Gammon结果中表现得很明显。TD-Gammon 0.0，其网络输入本质上是西洋双陆棋棋盘上的“原始”表示，这意味着它几乎不涉及西洋双陆棋的知识，因此也学会了近似地玩西洋双陆棋

以前最好的西洋双陆棋计算机程序。添加专门的backgammon特性产生了TD-Gammon 1.0，它比以前所有的backgammon程序都要好，并且与人类专家的竞争也很好。
Mnih等人开发了一种名为deep Q-network (DQN)的增强学习代理，它将Q-learning与深度卷积神经网络(deep tional ANN)结合在一起。我们在9.6节中描述了深层的卷积神经网络。在Mnih等人对DQN的研究中，深层ANNs，包括深层卷积ANNs，在很多应用中都产生了令人印象深刻的结果，但它们并没有被广泛应用于强化学习中。

Mnih等人使用DQN来展示增强学习代理如何在不使用不同问题的特性集的情况下，在不同问题的集合中获得高水平的性能。为了证明这一点，他们让DQN通过与游戏模拟器交互来学习玩49个不同的Atari 2600视频游戏。DQN学到了不同的政策的49场比赛(因为安被重置为随机值的权重在学习每个游戏),但它使用相同的原始输入,网络体系结构,参数值(如步长,折现率,探索参数,以及更多特定于实现)的游戏。在这些游戏中，DQN在很大程度上达到或超越了人类水平。虽然这些游戏都是通过观看视频图像来玩的，但它们在其他方面有很大的不同。他们的行为有不同的效果，他们有不同的状态转换动力，他们需要不同的政策来学习高分。深度卷积神经网络学会了将所有游戏的原始输入转换为特征，以表示在大多数游戏中达到的高级DQN所需的动作值。

雅达利2600是一款家庭视频游戏机，从1977年到1992年，雅达利公司(Atari Inc.)一直在销售各种版本的游戏机。它引入或推广了许多现在被认为是经典的街机电子游戏，比如Pong, Breakout, Space入侵者，和小行星。虽然比现代电子游戏简单得多，但雅达利2600游戏对人类玩家来说仍然是娱乐和具有挑战性的，而且它们已经成为开发和评估强化学习方法的试验台(Diuk, Cohen, Littman, 2008;Naddaf,2010;Cobo, Zang, Isbell和Thomaz, 2011;贝勒玛尔，《活力》和《保龄球》，2013年)。Bellemare、Naddaf、Veness和Bowling(2012)开发了公开的街机学习环境(ALE)，以鼓励和简化使用Atari 2600游戏来学习和规划算法。
这些先前的研究和ALE的可用性使Atari2600游戏集成为Mnih等人演示的一个很好的选择，这也受到TD-Gammon在backgammon中令人印象深刻的人类水平表现的影响。DQN类似于TD-gammon，使用多层ANN作为TD算法的半梯度形式的函数逼近方法，梯度由反向传播算法计算。然而,而不是使用TD(λ)TD-Gammon一样,DQN semi-gradient的q学习的形式使用。TD-Gammon估计后态的值，很容易从《西洋双陆棋规则》中得到。为Atari游戏使用相同的算法，需要为每个可能的操作生成下一个状态(在这种情况下，这是不可能的)。

这可以通过使用游戏仿真器对所有可能的操作(ALE使之成为可能)运行单步模拟来实现。或者每个游戏的状态转换函数的模型可以被学习并用于预测下一个状态(Oh, Guo, Lee, Lewis, and Singh, 2015)。虽然这些方法可能产生与DQN类似的结果，但它们的实现会更加复杂，并且会显著增加学习所需的时间。使用Q-learning的另一个动机是，DQN使用了如下所述的经验重播方法，这需要一个非策略算法。无模型和偏离策略使Q-learning成为自然选择。
在描述DQN的细节和如何进行实验之前，我们先看看DQN能够达到的技能水平。Mnih等人将DQN的分数与当时文献中表现最好的学习系统的分数、一个专业的人类游戏测试者的分数、一个随机选择行为的代理人的分数进行了比较。来自文献的最好的系统使用了线性函数近似的特性，利用一些关于Atari 2600游戏的知识(Bellemare, Naddaf，， and Bowling, 2013)。DQN通过与游戏模拟器进行5000万帧的交互来学习每一款游戏，相当于38天的游戏体验。在每个游戏开始学习时，DQN网络的权值被重置为随机值。为了评估DQN在学习后的技能水平，每一场比赛的平均得分超过30次，每次持续5分钟，开始于随机初始状态。专业的人类测试人员使用相同的模拟器(声音关闭，以删除不处理音频的DQN的任何可能的优势)。经过2个小时的训练，人类在每个游戏中播放大约20集，每次最多5分钟，在此期间不允许休息。DQN在所有游戏中都比之前最好的强化学习系统玩得更好，除了6个，并且在22个游戏中比人类玩家玩得更好。Mnih等人认为，任何在人类得分或超过75\%以上的成绩都可以与人类水平的比赛相媲美，或者比人类水平的表现更好，这一结果表明，在46场比赛中的29场比赛中，DQN的得分达到或超过了人类的水平。Mnih等人(2015)对这些结果有更详细的描述。
人工学习系统来实现这些水平的发挥将足够令人印象深刻,但是让这些结果引人注目,许多当时被认为是人工智能是突破性的结果,同样的学习系统实现这些水平的不同游戏不依赖任何游戏的修改。

人类这些49雅达利游戏看到210×160像素图像帧与128颜色60赫兹。原则上,到底这些图像可能形成DQN原始输入,但减少内存和处理要求,Mnih等人预处理每一帧产生84×84的亮度值。因为许多雅达利的游戏的完整状态不完全可观测的图像帧,Mnih et al。“堆叠”最近的四帧,以便输入到网络尺寸84×84×4。这并没有消除所有游戏的部分可观测性，但它有助于使其中许多游戏更具有马尔可夫性。
这里关键的一点是，这些预处理步骤对于所有46个都是完全相同的

游戏。除了一般的理解之外，没有涉及特定于游戏的先验知识，即仍然有可能通过这种减少的维度学习到良好的策略，并且叠加相邻帧应该有助于某些游戏的部分可观察性。因为没有游戏先验知识除此之外最少用于预处理图像帧,我们能想到的84×84×4输入向量作为“原始”DQN的输入。
DQN的基本架构类似于图9.15中所示的深层卷积神经网络(尽管与该网络不同，DQN中的子采样被视为每个卷积层的一部分，其中的特征映射由只选择可能接受域的单元组成)。DQN有三个隐藏的卷积层，然后是一个完全连接的隐藏层，然后是输出层。DQN产生的三个连续卷积隐藏层32 20×20特征图,64 9×9特征图,64 7×7特征图。每个feature map的单位的激活函数是一个整流器非线性(max(0,x))。3136(64×7×7)单位在这个第三卷积层所有连接到每个512单位的完全连接隐藏层,然后每个连接到输出层中的所有18个单位,每个可能的行动在雅达利一个游戏。
DQN输出单元的激活水平为相应状态-动作对的估计最优动作值，以网络输入为代表。游戏动作的输出单位的分配因游戏的不同而不同，由于游戏的有效动作数量在4到18之间不同，并不是所有的输出单位在所有游戏中都有功能角色。把网络想象成18个独立的网络，一个用来估计每个可能的行动的最佳行动值。实际上，这些网络共享它们的初始层，但是输出单元学会了使用这些层以不同的方式提取的特性。

DQN奖赏信号显示游戏的分数如何改变从一个时间到下一个步:+ 1时增加,−1每当它减少,否则和0。这使整个游戏的奖励信号标准化，并使一个单步大小的参数在所有游戏中都有效，尽管它们的分数范围不同。DQNε-greedy政策,与ε减少线性第一个百万帧和保持在一个低价值的学习会议。通过进行非正式的搜索，我们选择了各种其他参数的值，比如学习步长、贴现率和其他特定于实现的参数，以查看哪些值最适合于少量的游戏。这些价值观在所有的比赛中都是固定的。
DQN选择一个动作后，该动作由游戏模拟器执行，该模拟器返回一个奖励和下一个视频帧。帧被预处理并添加到作为网络下一个输入的四帧堆栈中。暂时跳过Mnih等人对基本Q-learning过程的修改，DQN使用Q-learning的以下半梯度形式更新网络的权值:

wt + 1 = wt +α

Rt + 1 +γmax
一个
问̂(圣+ 1,wt)−问̂(圣,在wt)

wt∇问̂(圣),(16.3)

其中wt为网络权值向量，At为时间步t选取的动作，St和St+1分别为时间步t和t+1对网络进行预处理的图像堆栈输入。
(16.3)的梯度是通过反向传播计算的。再想象,

对于每个动作都是一个单独的网络，对于t时刻的更新，反向传播只应用于与at对应的网络。Mnih等人在应用于大型网络时，利用显示的技术改进了基本的反向传播算法。他们使用了一个小批处理方法，在一小批图像上(这里是32张图像之后)积累梯度信息后，才更新权重。与在每次操作之后更新权重的通常过程相比，这产生了更平滑的示例梯度。他们还使用了一种名为RMSProp (Tieleman和Hinton, 2012)的梯度上升算法，它通过调整每个权重的步长参数来加速学习，这是基于该权重的最近梯度的运行平均值。

Mnih等人以三种方式修改了基本的Q-learning过程。首先，他们使用了Lin(1992)首先研究的经验重播方法。该方法将代理的经验存储在重播内存中的每个时间步骤中，重播内存被访问以执行重量更新。它在DQN中是这样工作的。游戏仿真器以图像堆栈St表示的状态执行动作后，返回奖励Rt+1和图像堆栈St+1，将tuple (St, At, Rt+1, St+1)添加到重播内存中。这种记忆在同一游戏的许多游戏中积累了经验。在每次步骤中，根据从重播内存中随机抽取的经验，执行多个Q-learning更新(一个微型批处理)。不像通常的Q-learning方式那样，St+1成为下一个更新的新St，而是从重播内存中提取一种新的无连接体验，为下一次更新提供数据。由于Q-learning是一种非策略算法，因此不需要沿着连通轨迹进行应用。

与传统的Q-learning方式相比，带经验重播的Q-learning提供了一些优势。能够将每个存储的经验用于许多更新，使DQN能够更有效地从经验中学习。体验重播降低了更新的方差，因为连续的更新彼此之间没有相关性，就像标准Q-learning一样。通过消除连续经验对当前权重的依赖，经验重复消除了不稳定性的一个来源。

Mnih等改进了标准Q-learning，以提高其稳定性。与其他引导方法一样，Q-learning更新的目标依赖于当前的动作-值函数估计。当使用参数化函数逼近方法来表示操作值时，目标是正在更新的相同参数的函数。例如,更新的目标(16.3)给出的是γmaxa q̂(圣+ 1,wt)。它对wt的依赖与目标不依赖于被更新的参数的更简单的监督学习情况相比，使过程更加复杂。正如第11章所讨论的那样，这会导致振荡和/或散度。

为了解决这个问题，Mnih等人使用了一种技术，使Q-learning更接近于更简单的监督学习案例，同时仍然允许它自我引导。每当一定数量,C,更新所做的重量w行为价值网络,他们将网络目前的重量到另一个网络,这些重复的权重固定C更新(w)。这个重复的输出网络未来C更新(w)被用作q学习的目标。让问̃表示这个复制网络的输出,然后代替(16.3)更新规则是:

wt + 1 = wt +α

Rt + 1 +γmax
一个
问̃(圣+ 1,wt)−问̂(圣,在wt)

wt∇问̂(St)。

最后对标准Q-learning进行了修改，以提高稳定性。他们剪了误差项Rt + 1 +γmaxa问̃(圣+ 1,wt)−问̂(圣,在wt)因此留在间隔(−1,1)。
Mnih等人对其中的5个游戏进行了大量的学习，以深入了解DQN的各种设计特性对其性能的影响。他们运行DQN，包含或不包含四种体验重播组合和重复目标网络。尽管从游戏到游戏的结果各不相同，但单独的这些特性都显著地提高了性能，并且在一起使用时，性能得到了极大的提高。Mnih等人还通过比较DQN的深度卷积版本和一个只有一个线性层的网络的版本(两者都接收相同的叠置预处理视频帧)来研究深度卷积神经网络在DQN学习能力中的作用。在这里，深度卷积版本比线性版本的改进在所有5个测试游戏中尤其引人注目。

人工智能的一个长期目标是创造出一种能够在各种挑战性任务中脱颖而出的人工智能。机器学习作为实现这一目标的一种方式的承诺，由于需要解决特定问题的表述而受挫。DeepMind的DQN是向前迈出的重要一步，它证明单个代理可以学习问题特定的特性，从而使它能够在一系列任务中获得人类竞争技能。这个演示并没有产生一个同时擅长所有任务的代理(因为每个任务都是单独学习的)，但是它表明深度学习可以减少、甚至可能消除针对问题的设计和调优的需求。然而，正如Mnih等指出的那样，DQN并不能完全解决任务自主学习的问题。虽然要在雅达利游戏中表现出色所需要的技能是多种多样的，但所有的游戏都是通过观察视频图像来进行的，这使得深度卷积神经网络成为完成这些任务的自然选择。此外，DQN在一些雅达利2600游戏上的表现也大大低于这些游戏的人类技能水平。对于DQN来说，最困难的游戏——尤其是DQN学会了如何像随机玩家一样进行报复的Montezuma的游戏——需要进行比DQN设计的更深入的规划。此外，通过广泛的实践来学习控制技能，就像DQN学会了如何玩雅达利游戏一样，这只是人类常规完成的学习类型之一。尽管存在这些局限性，DQN在机器学习方面取得了长足的进步，但却令人印象深刻地展示了将强化学习与现代深度学习方法相结合的承诺。


\section{掌握围棋}

几十年来，中国古代围棋一直挑战着人工智能研究者。在其他游戏中获得人类水平技能，甚至超人类水平技能的方法，在生成强大的围棋程序方面并不成功。由于围棋程序员和国际比赛的活跃，围棋程序的水平在过去几年中有了显著的提高。然而，直到最近，还没有围棋程序能达到人类围棋大师的水平。
DeepMind (Silver et al.， 2016)的一个团队开发了AlphaGo程序，该程序已经崩溃

这种障碍结合了深度ANNs(第9.6节)、监督学习、蒙特卡罗树搜索(MCTS，第8.11节)和强化学习。在Silver等人2016年出版之前，AlphaGo已经被证明比目前其他围棋程序更强大，并以5比0击败了欧洲围棋冠军樊麾。这是围棋程序第一次在围棋游戏中击败专业的人类棋手而没有任何障碍。此后不久，类似的阿尔法狗击败了18届世界冠军李世石(Lee Sedol)，在5场挑战赛中赢得4场，成为全球新闻头条。人工智能研究人员认为，一个程序要达到这样的水平还需要许多年甚至几十年的时间。

这里我们描述AlphaGo和一个叫做AlphaGo Zero (Silver et al. 2017a)的后续程序。除了强化学习之外，AlphaGo还依赖于从一个大型人类动作专家数据库中进行的监督学习，而AlphaGo Zero只使用强化学习，除了游戏的基本规则之外，不使用任何人类数据或指导(因此得名“零”)。我们首先对AlphaGo进行了一些详细的描述，以突出AlphaGo Zero的相对简单性。

在很多方面，AlphaGo和AlphaGo Zero都是Tesauro的TD-Gammon(第16.1节)的后代(第16.2节)。所有这些项目都包括强化学习，而不是模拟的自我游戏。AlphaGo和AlphaGo Zero也建立在DeepMind使用DQN程序(第16.5节)进行Atari游戏的进展上，该程序使用深度卷积ANNs来近似最优值函数。


一试板配置
围棋是两名棋手之间的一种游戏，他们交替地把黑和白的“石头”放在未占用的十字路口或“点”上，棋盘上有19条横线和19条垂线，以产生右边显示的位置。游戏的目标是捕获比对手捕获的面积更大的区域。石头是根据简单的规则捕获的。如果一个玩家的石头完全被另一个玩家的石头包围，那么他的石头就会被捕获，这意味着没有水平或垂直的相邻点没有被占用。例如，图16.5显示了左边的三颗白色石头，旁边有一个未被占用的点(标记为X)。然而,如果玩家white首先在X点上放置一块石头，那么这个捕获的可能性将被阻止(图16.5右边)。需要其他规则来防止无限捕获/重新捕获循环。当双方都不愿再放一块石头时，游戏就结束了。这些规则很简单，但是它们产生了一种非常复杂的游戏，而且非常广泛


图16.5:Go capture规则。左图:这三颗白色的石头并没有因为点而被包围
X是空置的。中间:如果黑色在X上放置一块石头，三个白色的石头就会被捕获
退出董事会。正确:如果白色首先在X点放置一块石头，则捕获被阻塞。

上千年的呼吁。
在围棋等其他游戏中发挥强大作用的方法，在围棋中表现得并不好。搜索空间去显著大于国际象棋,因为有更多的法律行动/位置比国际象棋(≈250和≈35),游戏往往涉及移动比国际象棋游戏(≈150与≈80)。但是，搜索空间的大小并不是导致搜索变得如此困难的主要因素。详尽的搜索是不可行的国际象棋,和继续小板(如9×9)已被证明是非常困难的。专家们一致认为，创建比业余围棋更强的围棋程序的主要障碍是很难定义一个合适的位置评估函数。一个好的评估函数允许搜索在一个可行的深度上被截断，通过提供相对容易计算的关于深度搜索可能产生的结果的预测。根据μ̈ll(2002):“不简单而合理的评价函数会被发现。一个重要的进步是引入了MCTS来进行程序。AlphaGo开发时最强大的程序都包括MCTS，但大师级别的技能仍然难以捉摸。

回想一下第8.11节，MCTS是一个决策时规划过程，它不尝试学习和存储全局评估函数。就像推出算法(第8.10节)一样，它运行许多蒙特卡罗模拟整个场景(在这里，整个围棋游戏)来选择每个动作(这里，每个动作:在哪里放置石头或辞职)。然而，与简单的rollout算法不同，MCTS是一个迭代过程，它递增地扩展搜索树，其根节点表示当前环境状态。如图8.10所示，每个迭代通过模拟与树的边缘相关的统计信息指导的操作来遍历树。在其基本版本中，当模拟到达搜索树的叶子节点时，MCTS通过向树中添加一些或全部的叶子节点子节点来扩展树。从叶子节点或新添加的子节点中执行一个rollout:一个通常一直执行到终端状态的模拟，其操作由rollout策略选择。当rollout完成时，与在此迭代中遍历的搜索树的边缘相关的统计信息将通过备份rollout产生的返回来更新。MCTS继续这个过程，每次从搜索树的根开始，在当前状态，在给定时间限制的情况下，尽可能多地进行迭代。然后，最后，根据根节点的传出边中累积的统计信息，选择根节点(仍然表示当前环境状态)的操作。这是代理所采取的行动。在环境转换到下一个状态之后，MCTS再次被执行，根节点集将代表新的当前状态。开始的搜索树

下一个执行可能仅仅是这个新的根节点，也可能包含MCTS以前执行时遗留下来的这个节点的后代。树的其余部分被丢弃。


\subsection{AlphaGo}
AlphaGo之所以成为如此强大的棋手，其主要创新之处在于，它采用了一种新型的MCTS棋类，这种棋类既遵循一种策略，又遵循一种价值函数，通过深度卷积神经网络提供的函数逼近进行强化学习。另一个关键特性是，它不是从随机的网络权重开始强化学习，而是从之前大量的人类专家动作的监督学习的结果开始。
DeepMind团队将AlphaGo对基本MCTS的修改称为“异步策略和值MCTS”，即APV-MCTS。它通过上面描述的基本的MCTS来选择动作，但是在如何扩展搜索树以及如何评估动作边缘方面有一些变化。基本mct相比,使用存储扩展当前的搜索树的行动值从一个叶节点选择一个未知的边缘,APV-MCTS,作为AlphaGo实现,扩大了选择一条边的树根据概率由13-layer深安卷积,称为SL-policy网络,由监督学习训练之前预测移动数据库中包含近3000万人类专家的动作。

然后,还与基本mct,评估新增状态返回的节点完全推出发起,APV-MCTS评估节点在两个方面:这个返回的推广,但也由价值函数,vθ,学会了以前的强化学习方法。如果s是新添加的节点，它的值就变成了。

v(s)=(1−η)vθ(s)+ηG, 					(16.4)

G是推出的回归和η值的混合控制造成这两种评价方法。在AlphaGo中，这些值由值网络提供，这是另一个13层的深度卷积神经网络，我们将其训练为输出板位的估计值。APV-MCTS在AlphaGo中进行了模拟游戏，两名玩家都使用一个简单的线性网络提供的快速推出策略，在游戏开始前还接受了监督学习的训练。在整个执行过程中，APV-MCTS记录了在搜索树的每个边缘通过了多少次模拟，当它的执行完成时，从根节点获得最多访问的边缘被选择为要采取的行动，在这里，AlphaGo实际上是在一个游戏中移动的。

值网络与深度卷积SL策略网络具有相同的结构，只是它有一个输出单元，提供游戏位置的估计值，而不是SL策略网络对法律行为的概率分布。理想情况下,网络输出值最优状态值,它可能是可能的近似最优值函数的TD-Gammon上面描述:self-play与TD(λ)耦合的非线性卷积安。但是DeepMind团队采用了一种不同的方法，对于像Go这样复杂的游戏来说，这种方法更有希望。他们把培训价值网络的过程分为两个阶段。在第一阶段，他们通过使用强化学习来训练一个RL，创造了他们能做的最好的策略

政策网络。这是一个与SL策略网络结构相同的深度卷积神经网络。通过监督学习得到的SL策略网络的最终权重初始化，然后使用策略梯度强化学习对SL策略进行改进。在价值网络训练的第二阶段，团队使用蒙特卡罗策略评价方法，对RL策略网络所选取的大量模拟自玩游戏的数据进行评价。
图16.6展示了AlphaGo使用的网络，以及在DeepMind团队所称的“AlphaGo管道”中训练它们的步骤。“所有这些网络都是在现场比赛开始前进行训练的，他们的重量在整个现场比赛中都是固定的。”
 
人类专家的职位 					Self-play位置

图16.6:AlphaGo管道。改编自麦克米伦出版社有限公司:自然，第529卷(7587页)，第485页，c ?2016年。


这里有一些关于AlphaGo的ANNs和他们的训练的细节。identically-structured SL和RL政策网络类似于DQN深卷积网络16.5节中描述的雅达利游戏,除了他们有13卷积层和最后一层组成的soft-max单位为每个点19×19板。网络的输入是一个19×19×48图像栈中每个点的董事会由48个二进制或整数值特性的值。例如，对于每一个点，一个特性表明这个点是被AlphaGo的一颗石头(对手的石头之一)占据的，还是没有占用的，从而提供了棋盘配置的“原始”表示。其他功能都是基于规则的,如相邻的点的数量是空的,对手石头的数量将被放置一块石头,石头以来匝数是放置在那里,和其他特性,设计团队认为是重要的。
使用50个处理器上的随机梯度提升的分布式实现，训练SL策略网络大约需要3周的时间。该网络的准确率达到57\%，这是其他组在出版时达到的最佳准确率

是44.4\%。RL策略网络的训练是在RL策略网络的当前策略与对手之间的模拟博弈中，通过策略梯度强化学习，从学习算法早期迭代生成的策略中随机选择策略。与随机挑选的反对者进行比赛，避免了对现行政策的过度配合。奖励信号+ 1如果当前政策赢得−1如果它丢失,否则为0。这些游戏直接将两种政策对立起来，而不涉及MCTS。DeepMind团队通过在50个处理器上同时模拟许多游戏，在一天内训练了100万场游戏的RL策略网络。在测试最终的RL政策时，他们发现它赢得了超过80\%的游戏，而在SL政策中，它赢得了85\%的游戏，而使用MCTS，模拟了每移动10万次游戏。
值网络的结构与除了单一输出单元外的SL和RL策略网络相似，但接收到与SL和RL策略网络相同的输入，但有一个附加的二进制特性可以显示当前的颜色。蒙特卡罗政策评估是利用RL策略从大量的自玩游戏中获得的数据来训练网络的。为了避免过度拟合和不稳定性，DeepMind团队构建了一个数据集，每个数据集都是从一个独特的自玩游戏中随机选择的3000万个位置。然后用5000万次小批量的训练，每个小批量的训练都是从这个数据集中抽取的32个位置。

在一个简单的线性网络训练之前，我们学习了一个简单的线性网络，这个网络是由一个800万人的移动的语体训练出来的。推出策略网络必须在输出动作迅速的同时仍然保持合理的准确性。原则上,SL或RL政策网络可以用于糊涂事,但远期通过这些深层网络传播他们花了太多时间在推广使用模拟,其中许多为每个移动决定在进行生活。由于这个原因，rollout策略网络没有其他策略网络那么复杂，它的输入特性可以比用于策略网络的特性更快地计算出来。rollout策略网络允许每秒在AlphaGo使用的每个处理线程上运行大约1000次完整的游戏模拟
。
人们可能想知道为什么在APV-MCTS的扩展阶段使用SL策略而不是更好的RL策略来选择操作。这些策略的计算时间相同，因为它们使用相同的网络架构。研究团队发现，当APV-MCTS使用SL策略而不是RL策略时，AlphaGo在对抗人类对手时表现得更好。他们推测，原因是后者是为了响应最优的动作而不是人类游戏的更广泛的动作。有趣的是，APV-MCTS使用的值函数的情况正好相反。他们发现当APV-MCTS使用从RL策略派生的值函数时，它的性能比使用从SL策略派生的值函数要好。

几种方法共同作用，产生了AlphaGo令人印象深刻的演奏技巧。DeepMind团队评估了不同版本的AlphaGo，以评估其贡献。

由这些不同的部件组成。参数η(16.4)的混合控制网络和游戏状态评估产生的价值糊涂事。η= 0,AlphaGo使用价值网络没有糊涂事,η= 1,评价只是依赖糊涂事。他们发现，只使用值网络的AlphaGo比只使用rollout的AlphaGo表现得更好，实际上也比当时所有强大的围棋程序表现得更好。最好的发挥造成设置η= 0.5,表明结合价值网络与电AlphaGo的成功尤为重要。这些评价方法相互补充:价值网络太慢的高性能RL政策评估用于生活,虽然糊涂事使用较弱但更快推出政策能够添加价值网络的精度评估的特定状态发生在游戏。
总的来说，AlphaGo的非凡成功激起了新一轮的热情，人们对人工智能的前景充满了热情，特别是对于将强化学习与深度人工智能相结合的系统，以解决其他具有挑战性领域的问题。


\subsection{AlphaGo零}

基于AlphaGo的经验，DeepMind团队开发了AlphaGo Zero (Silver et al. 2017a)。与AlphaGo不同的是，这个程序除了游戏的基本规则外，没有使用任何人类数据或指导(因此名称为0)。它只从自玩强化学习中学习，输入只给出对围棋棋盘上的石头位置的“原始”描述。AlphaGo Zero实现了一种形式的策略迭代(第4.3节)，将策略评估与策略改进交叉。图16.7是AlphaGo Zero算法的概述。AlphaGo 0和AlphaGo的一个显著区别是，AlphaGo Zero使用mct在自玩强化学习中选择移动，而AlphaGo则使用MCTS进行实时游戏，而不是持续学习。除了不使用任何人工数据或人工设计的特性之外，AlphaGo Zero还存在其他的不同，即它只使用了一个深度卷积神经网络，并使用了一个更简单的MCTS版本。

AlphaGo Zero的MCTS比AlphaGo使用的版本更简单，因为它不包含完整游戏的推出，因此不需要推出策略。AlphaGo Zero的MCTS每一次迭代都运行一个模拟，最后在当前搜索树的叶子节点上结束，而不是在完整的游戏模拟的终端位置。但AlphaGo,每次迭代AlphaGo零mct的指导下的输出卷积网络深处,贴上fθ在图16.7中,θ是网络的权向量。网络的输入,其架构下面我们描述,包括董事会的原始表示位置,及其输出有两个部分:一个标量值,v,估计当前的球员会赢的概率从现任董事会的位置,和一个矢量,p,转移概率,每个可能的石头放置在当前一个板,加上通过,或者辞职,那就动起来吧。
而不是选择self-play行动根据概率p,然而,AlphaGo零概率使用,加上网络的价值输出,直接每个特定的执行,返回新的移动概率,政策πi如图16.7所示。这些政策受益于MCTS所做的许多模拟。

图16.7:AlphaGo零级自我强化学习。a)程序对自己进行了许多游戏，这里显示的一个是棋盘位置序列si, i = 1,2，…T,
移动ai i = 1 2…,T,赢家z。每个移动概率πi ai是由行动从根节点返回的特定执行si和指导下深卷积网络,在这里贴上fθ,最新的重量θ。这里只显示了一个位置s，但对所有si都是重复的，网络的输入是板位置si的原始表示(连同几个过去的位置，虽然这里没有显示)，它的输出是移动概率的向量p
这指导了MCTS的正向搜索，标量值v估计了当前玩家从每个位置si中获胜的概率。b)深度卷积网络训练。培训我们从最近的自玩游戏中随机抽取了一些例子。权重θ更新将政策矢量p对mct返回的概率π,包括估计的赢家z赢得概率诉转载草案的银et al。(2017)经作者许可和DeepMind。


每次执行。结果是，AlphaGo Zero实际上遵循的策略比网络输出p. Silver等人(2017a)给出的策略有所改进，“因此，MCTS可能被视为一个强大的策略改进操作符。”

下面是关于AlphaGo Zero的ANN的更多细节，以及它是如何训练的。网络作为输入了19×19×17飞机17二进制组成的图像叠加特性。前8个特征面是当前玩家的石头在当前和过去7个棋盘配置中的原始表示:如果玩家的石头在相应的点上，特征值为1，否则为0。接下来的8个特征面类似地编码了对手石头的位置。最后一个输入特征平面具有一个常量值，该值指示当前播放的颜色:1表示黑色;0为白色。因为围棋不允许重复，一个棋手因为没有得到第一步棋而被给予一定的“补偿点数”，所以当前棋盘的位置不是围棋的马尔可夫状态。这就是为什么需要描述过去董事会位置和颜色特征的特征。

网络是“双头”的，这意味着在经过了若干初始层之后，网络被分成了两个独立的“头”层，这些“头”层又被分成了两组输出单元。在这种情况下，一个人头输入362个输出单元，产生192 + 1的移动概率p，每一个可能的石头放置加通过;另一个头只给一个输出单元输入标量v，这是当前玩家从当前棋盘位置获胜的概率的估计值。分割之前的网络由41个卷积层组成，每个层之后都进行批处理规范化，并通过添加跳过连接实现对层的剩余学习(参见9.6节)。总的来说，移动概率和数值分别由43层和44层计算。

从随机权重开始，该网络通过随机梯度下降(随着训练的进行，动量、正则化和步长参数逐渐减小)对其进行训练，使用随机抽样的样本，这些样本来自最近50万场自玩游戏的所有步骤。额外的噪音被添加到网络的输出p，以鼓励探索所有可能的移动。西尔弗等人(2017a)在培训期间的定期检查点(时选择每1000个培训步骤)中，使用最新权重的ANN对当前最佳策略进行模拟400个游戏(使用1600次迭代的MCTS选择每个动作)来评估策略输出。如果新政策赢得了(以降低结果的噪音为目标)，那么它就成为了在随后的自演中使用的最佳政策。更新网络的权值，使网络的策略输出p更接近MCTS返回的策略，使其值输出v更接近于当前最佳策略从网络输入所代表的棋盘位置获胜的概率。

DeepMind团队对AlphaGo Zero进行了490多万次自我游戏的训练，耗时约3天。每个游戏的每次移动都是通过运行MCTS进行1600次迭代来选择的，每次移动大约需要0.4秒。网络重量更新超过70万批次，每批包含2048块板配置。然后，他们与训练有素的AlphaGo Zero进行比赛，与AlphaGo版本的比分是5比0，击败了樊麾;与AlphaGo版本的比分是4比1，击败了李世石。他们使用Elo评级系统来评估项目的相对表现。两个Elo评分之间的差异是为了预测玩家之间游戏的结果。AlphaGo Zero (AlphaGo Zero)和李世石(Lee Sedol)的Elo评分分别为4,308、3,144和3,739。Elo评级的差距转化为AlphaGo 0击败其他程序的概率非常接近于1的预测。在100场比赛中，AlphaGo 0和击败李世石(Lee Sedol)的AlphaGo的确切版本在100场比赛中都以相同的条件击败了AlphaGo。

DeepMind团队还将AlphaGo Zero与一个程序进行了比较，该程序使用具有相同结构的人工神经网络(ANN)，但经过监督学习(supervised learning)训练，在一个包含16万场游戏近3000万个位置的数据集中预测人类的移动。他们发现，最初AlphaGo的表现比AlphaGo Zero要好，而且更善于预测人类专家的动作，但在AlphaGo Zero接受一天训练后，他们的表现就不那么好了。这表明，AlphaGo Zero已经发现了一种游戏策略。

不同于人类的游戏方式。事实上，AlphaGo Zero发现并开始偏爱一些经典移动序列的新变种。

AlphaGo Zero算法的最终测试是在一个拥有更大的ANN的版本中进行的，并且训练了超过2900万的自玩游戏，这需要40天的时间，再一次从随机的重量开始。这个版本达到了5 185的Elo评级。研究小组将这一版本的AlphaGo Zero与当时最强大的AlphaGo Master程序进行了对比，该程序与AlphaGo Zero完全相同，但与AlphaGo一样，它使用了人类数据和功能。AlphaGo Master的Elo评分为4858，在网络游戏中以60比0击败了人类最强大的职业棋手。在100场比赛中，拥有更大网络和更广泛学习的AlphaGo Zero以11比89击败了AlphaGo大师，从而令人信服地证明了AlphaGo Zero的算法解决问题的能力。

AlphaGo Zero充分证明了通过纯粹的强化学习、简单的MCTS版本增强、深入的ANNs对领域的了解非常少、不依赖人类数据或指导就可以实现超人的表现。我们肯定会看到，AlphaGo和AlphaGo Zero的深度思维成就对其他领域的问题产生了启发。

最近，Silver等人(2017b)描述了一个更好的程序AlphaZero，它甚至没有包含围棋知识。AlphaZero是一种普遍的强化学习算法，它改进了迄今为止在围棋、棋类和shogi等多种游戏中最优秀的程序。


\section{个性化的Web服务}

个性化的web服务，如发布新闻文章或广告，是提高用户对网站的满意度或增加营销活动的收益的一种方法。策略可以根据用户的兴趣和偏好(从他们的在线活动历史中推断)，为每个特定用户推荐最适合他们的内容。这是机器学习的自然领域，尤其是强化学习。强化学习系统可以通过对用户反馈进行调整来改进推荐策略。获取用户反馈的一种方式是通过网站满意度调查，但为了实时获取反馈，通常会将用户点击作为链接的兴趣指标进行监控。

在市场营销中长期使用的一种叫做A/B测试的方法是一种简单的强化学习，用来决定网站用户喜欢的两个版本A或B中的哪个。因为它是非关联的，就像两个武装的强盗问题一样，这种方法没有个性化的内容交付。添加由描述单个用户和将要交付的内容的特性组成的上下文允许个性化服务。这已被形式化为一个上下文强盗问题(或一个关联强化学习问题，第2.9节)，目标是最大化用户点击的总数。Li, Chu, Langford和Schapire(2010)应用了语境的bandit算法来解决雅虎的个性化问题。今天的首页页面(在他们研究的时候是互联网上访问量最大的页面之一)通过选择新闻报道为特色。他们的目标是

最大限度地提高点击率(CTR)，这是指所有用户在网页上点击的总次数与访问页面的总次数之比。他们的上下文土匪算法比标准的非关联土匪算法提高了12.5\%。

Theocharous, Thomas和Ghavamzadeh(2015)认为，通过将个性化推荐作为一个马尔可夫决策问题(MDP)来实现更好的结果是可能的，目标是最大化用户在多次访问一个网站时的总点击量。从上下文土匪公式派生的策略是贪婪的，因为它们不考虑行动的长期影响。这些政策有效地对待每次访问一个网站，就好像它是由一个新访客统一抽样从网站的访客人口。由于没有利用许多用户反复访问相同网站的事实，贪婪策略没有利用与单个用户长期交互所提供的可能性。

Theocharous等人举了一个营销策略如何利用长期用户互动的例子，将贪婪的政策与长期的政策(比如汽车广告)进行对比。贪婪策略显示的广告可能会提供折扣，如果用户立即购买汽车。用户要么接受要约，要么离开网站，如果他们回到网站，他们很可能会看到同样的要约。另一方面，一项长期政策可以在提交最终协议之前，将用户“从销售渠道向下转移”。它可以从描述有利的融资条件开始，然后赞扬一个优秀的服务部门，然后在下次访问时，提供最终的折扣。这种类型的策略可以导致用户在多次访问站点时产生更多的点击，如果策略设计得当，那么最终的销售就会更大。

Theocharous等人在Adobe系统股份有限公司工作时，做了一些实验，看看设计用于长期最大化点击的策略是否真的能比短期的贪婪策略更好。Adobe营销云是许多公司用来开展数字营销活动的一套工具，它提供了自动化用户广告和筹款活动的基础设施。实际上，使用这些工具部署新策略会带来重大风险，因为新策略最终可能表现不佳。由于这个原因，研究团队需要评估一个策略在实际部署时的性能，但是要基于在执行其他策略时收集的数据。这项研究的一个关键方面是偏离政策的评估。此外，该小组希望以高度的信心来降低部署新政策的风险。虽然高信任度的政策外评价是本研究的中心组成部分(参见Thomas, 2015;Thomas, Theocharous，和Ghavamzadeh, 2015)，在这里我们只关注算法和它们的结果。

Theocharous等人比较了学习广告推荐策略的两种算法的结果。第一个算法，他们称之为“贪婪优化”，它的目标是最大化立即点击的概率。与标准上下文土匪公式一样，该算法没有考虑到推荐的长期影响。另一种算法是基于MDP公式的增强学习算法，目的是提高用户多次访问网站的点击量。他们称之为后一种算法生命周期值(LTV)优化。这两种算法都面临着挑战性的问题，因为这个领域的奖励信号非常稀疏，因为用户通常不会点击广告，而用户点击是非常随机的回报高方差。

来自银行业的数据集用于培训和测试这些算法。这些数据集包含了许多客户与银行网站互动的完整轨迹，这些轨迹显示了从可能的报价中选出的每个客户。如果顾客点击，奖励是1，否则是0。一组数据包含了一个银行发起的活动中大约20万次互动，该活动随机提供了7个提议中的一个。另一组数据来自另一家银行的活动，其中包含了4,000,000个互动，涉及12个可能的报价。所有的交互都包括客户特性，比如客户上次访问网站的时间、到目前为止的访问次数、客户最后一次点击的时间、地理位置、兴趣集合之一，以及提供人口统计信息的特性。

贪婪的优化是基于一个映射估计一个点击的概率作为一个用户功能的功能。这种映射是通过随机森林(RF)算法(Breiman, 2001)从一个数据集中的监督学习中获得的。RF算法在工业上广泛应用于大型应用中，因为它们是有效的预测工具，不会过度拟合，对异常值和噪声相对不敏感。Theocharous等人然后使用映射定义一个ε-greedy政策选择概率1-ε提供预测的RF算法产生点击的概率最高,和其他选择从其他提供随机均匀。

LTV优化使用了一种称为拟合Q迭代(fit Q iteration, FQI)的批处理模式增强学习算法。它是适合Q-learning的拟合值迭代(Gordon, 1999)的变体。批处理模式意味着学习的整个数据集从一开始就是可用的，而不是我们在本书中关注的算法的在线模式，在学习算法执行时，数据是按顺序获取的。当在线学习不实用时，批式强化学习算法有时是必要的，它们可以使用任何批式监督学习回归算法，包括已知的可扩展到高维空间的算法。FQI的收敛取决于函数逼近算法的性质(Gordon, 1999)。Theocharous等人在LTV优化中使用了与贪婪优化方法相同的RF算法。因为在这种情况下FQI收敛不单调,Theocharous等人记录最好的FQI政策off-policy评估使用验证训练集。最后政策测试LTV方法ε-greedy政策是基于最好的政策由FQI最初的行为价值函数集映射产生的射频贪婪优化方法。

为了度量贪婪和LTV方法生成的策略的性能，Theocharous等人使用了CTR度量和一个他们称为LTV度量的度量。这些指标是相似的，除了LTV指标在个别网站访问者中有显著的区别:
总点击次数。
访问总次数，

LTV =总点击数
总人数。

图16.8展示了这些度量的不同之处。每个圆圈代表用户对站点的访问;黑圈是用户点击的访问。每一行表示一个特定用户的访问。由于没有区分访问者，这些序列的CTR为0.35，而LTV为1.5。因为LTV比CTR要大，以至于用户可以重新访问站点，所以它是鼓励用户与站点进行扩展交互的政策成功与否的一个指标。
 

图16.8:单击rate (CTR)和life-time值(LTV)。每个圆圈代表用户访问;黑圈是用户点击的访问。改编自Theocharous等(2015)。

对贪婪和LTV方法生成的策略进行测试时，在一个测试数据集上使用了一种高度可信的离策略评估方法，该测试数据集包含与随机策略服务的银行网站的真实交互。结果表明，贪心优化在CTR指标下表现最佳，LTV优化在LTV指标下表现最佳。此外，尽管我们忽略了它的细节，但高可信度的非政策评估方法提供了概率性的保证，即LTV优化方法能够在高概率的情况下制定政策，从而改进当前部署的政策。由这些概率保证担保,Adobe宣布在2016年新的LTV算法将一个标准组件的Adobe营销云,零售商可以发行后的序列提供了政策可能产生更高的回报,而不是政策,是对长期的结果。


\section{热飙升}

鸟类和滑翔机利用上升的气流来获得高度，以保持飞行，同时消耗很少的能量。这种行为被称为热升，是一种复杂的技能，需要对微妙的环境信号做出反应，尽可能长时间地利用上升的空气柱来增加海拔高度。Reddy、Celani、Sejnowski和Vergassola(2016)利用强化学习研究了热飙升政策，这些政策在通常伴随上升气流的强大气湍流中是有效的。他们的主要目标是提供线索，了解鸟类的感觉，以及它们如何利用它们来达到令人印象深刻的热性能，但这些结果也有助于自动滑翔机相关的技术。强化学习以前被应用于有效导航的问题

靠近热上升气流(伍德伯里，邓恩和瓦拉瑟克，2014)但不是更有挑战性的问题，在上升气流本身的湍流中上升。

Reddy等人将飞涨的问题建模为持续的MDP，并进行了贴现。这名特工与一架在湍流空气中飞行的滑翔机的详细模型进行了互动。他们投入了大量的努力，使模型产生了真实的热膨胀条件，包括研究几种不同的大气模拟方法。在学习实验中，一个有一公里边的三维盒子里的空气流动，其中一个在地面上，用一组复杂的基于物理的偏微分方程来模拟，这些偏微分方程涉及空气速度、温度和压力。在数值模拟中引入小的随机扰动，导致模型产生热上升气流模拟和伴随的湍流(图16.9左)，滑翔机飞行采用气动方程建模，包括速度、升力、阻力和其他控制固定翼飞机无动力飞行的因素。操纵滑翔机需要改变它的攻击角度(滑翔机的机翼和气流方向之间的角度)和它的倾斜角度(图16.9右侧)。
 
 
图16.9热升模型:左:垂直速度场的快照
模拟空气立方体:在红色(蓝色)是一个大的向上(向下)流动的区域。右:无能为力飞行图显示银行角度μ和攻角α。改编自《美国科学院院刊》第113(22)卷，第E4879页，2016年，Reddy, Celani, Sejnowski和Vergassola，学习飞翔
在动荡的环境中。


代理和环境之间的接口需要定义代理的操作、代理从环境接收的状态信息和奖励信号。通过尝试各种可能性,Reddy等人决定三个动作每一个攻角和倾斜角足以让他们的目的:盈亏当前银行角度和攻角5◦◦和2.5,分别,或者让他们改变。这导致了32种可能的行动。银行角度之间有界保持−15◦◦+ 15。
因为他们的研究的目标是试图确定有效飞翔所需要的最微小的感官线索，既要阐明鸟类可能用于飞翔的线索，又要尽量减少自动滑翔飞行所需的感知复杂性，

作者尝试了各种各样的信号作为增强学习代理的输入。他们首先使用四维状态空间的状态聚合(第9.3节)，维度给出局部垂直风速，局部垂直风加速度，扭矩取决于左右翼尖垂直风速的差异，以及局部温度。每个维度被离散成三个箱子:正高，负高，小。下面描述的结果表明，这些维度中只有两个对有效的飞翔行为至关重要。

热升的总体目标是从每一根上升的空气柱中获得尽可能多的高度。Reddy等人尝试了一个简单的奖励信号，在每一集结束时，根据在这一集中获得的高度对特工进行奖励，如果滑翔机触地，则给出一个较大的负面奖励信号，否则为零。他们发现，在实际持续时间的片段中，利用这个奖励信号学习是不成功的，而且资格追踪也没有帮助。通过对各种奖励信号的实验，他们发现学习最好的方法是使用奖励信号，奖励信号在每个时间步上线性地结合上一个时间步上观察到的垂直风速和垂直风速加速度。

学习是一步萨尔萨(Sarsa)，根据基于规范化行为值的软最大值分布选择动作。具体来说，动作概率是根据(13.2)的动作偏好来计算的:
h(年代,θ)=问̂(s,a,θ)−minb问̂(年代,b,θ)
τ

maxb问̂(年代,b,θ)−minb问̂(年代,b,θ)?,
θ是每个动作的参数向量和一个组件和聚合组,和q̂(s,a,θ)仅仅返回组件对应的年代,在通常状态聚合方法。上面的方程形式的行动偏好正常化的近似动作值区间[0,1]然后除以τ,积极“温度参数。“3随着τ的增加,选择一个行动的概率变得不那么依赖自己的选择;的概率,τ减少向零,选择最优先选择的操作方法,使政策贪婪的政策。温度参数τ是初始化在学习2.0和逐步下降到0.2。行动偏好是计算从当前估计的行动值:行动偏好给出的最大估计行动值是1 /τ,行动的最低估计行动值给出了偏好0,和其他操作的偏好是介于这两种极端情况之间。步长和折扣率参数分别固定在0.1和0.98。

每一个学习过程都是由控制模拟飞行的agent在模拟湍流的独立生成期间进行的。每一集的时长为2.5分钟，每一集的时长为1秒。几百集之后，学习就有效地融合了。图16.10的左面板显示了在学习代理随机选择动作之前的样本轨迹。从所示体积的顶部开始，滑翔机的轨迹与箭头指示的方向一致，并迅速失去高度。图16.10右面板是学习后的轨迹。滑翔机从同一个地方开始(这里出现在体积的底部)，通过螺旋上升获得高度

3Reddy等人对此的描述略有不同，但我们的版本和他们的版本是一样的。

图16.10:热飞越轨迹样本，箭头显示从相同起始点出发的飞行方向(注意，高度标尺被移动)。左:在学习之前:代理随机选择动作，滑翔机下降。右图:学习后:滑翔机沿着螺旋轨迹上升高度。改编自PNAS vol. 113(22)， p. E4879, 2016, Reddy, Celani, Sejnowski和Vergassola，学习在动荡的环境中飞翔。

在上升的空气中。虽然Reddy等人发现，在不同的模拟气流周期中，性能差异很大，但随着学习的进行，滑翔机接触地面的次数持续减少，几乎为零。
在尝试了不同的特性集之后，研究人员发现，仅仅是垂直的风加速度和扭矩的组合效果最好。作者推测，由于这些特征提供了两种不同方向上的垂直风速梯度的信息，它们允许控制器通过改变银行角度来选择转弯或沿着同一路线继续前进，只留下银行角度。这使得滑翔机可以停留在上升的空气柱内。垂直风速指示了热的强度，但并不有助于保持在流中。他们发现对温度的敏感性没有什么帮助。他们还发现，控制攻击角度对保持在特定的温度下没有帮助，相反，当跨越较大的距离时，比如越野滑翔和鸟类迁徙时，它对在不同的温度之间旅行很有用。

由于不同程度的湍流需要不同的政策，所以训练在弱到强湍流的条件下进行。在强湍流中，快速变化的风和滑翔机速度使控制器的反应时间更短。这就减少了可能的控制量，而不是在波动很弱的时候。Reddy等人研究了Sarsa在这些不同条件下学到的政策。在所有制度下学习到的政策的共同特点是:当感应到负风加速时，向机翼的方向以较高的升力急速倾斜;当感应到较大的正风加速度而无转矩时，什么也不做。然而，不同程度的动荡导致了政策上的分歧。

在强动荡中学会的政策更为保守，因为它们更喜欢小银行角度，而在弱动荡中，最好的做法是通过银行大幅转向尽可能多。系统地研究了不同条件下政策选择的银行角度，作者认为，通过检测垂直风加速度何时超过一定阈值，控制器可以调整其政策以应对不同的湍流状态。
Reddy等人也进行了实验调查贴现率的影响参数γ对学习策略的性能。他们发现在一集获得的高度增加γ增加,达到最大值为γ= 0,表明有效热飙升需要考虑长期影响的控制决策。
这一热飞的计算研究说明了强化学习如何进一步朝着不同的目标前进。学习政策可以获得不同的环境线索和控制措施，这有助于设计自动滑翔机的工程目标和提高对鸟类飞行技能的理解的科学目标。在这两种情况下，通过检测真实的滑翔机，并将预测与观察到的鸟类飞翔行为进行比较，可以在实地测试学习实验得出的假设。
