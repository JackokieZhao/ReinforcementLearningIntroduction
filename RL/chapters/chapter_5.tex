\chapter{第五章 蒙特卡罗方法}
\begin{summary}
	在本章中，我们考虑了我们的第一个评估价值函数和发现最优策略的学习方法。与前一章不同的是，这里我们不假设对环境有完整的了解。蒙特卡罗方法只需要与环境进行实际或模拟交互时的状态、动作和奖励的经验样本序列。从实际经验中学习是惊人的，因为它不需要事先了解环境的动态，但仍然可以获得最佳行为。从模拟经验中学习也很有用。虽然需要一个模型，但是模型只需要生成示例转换，而不需要生成动态编程所需的所有可能转换的完整概率分布。令人惊讶的是，在许多情况下，根据期望的概率分布很容易生成经验，但以显式的形式获取分布是不可行的。
	蒙特卡罗方法是一种基于平均样本收益率的增强学习方法。为了确保定义良好的回报是可用的，这里我们定义蒙特卡罗方法仅用于情景任务。也就是说，我们假设经验被分为不同的阶段，所有的阶段最终都结束了，不管选择什么动作。只有在事件完成时，价值评估和策略才会改变。因此，蒙特卡罗方法可以循序渐进地逐步进行，但不是一步一步(在线)的意义。“Monte Carlo”一词通常用于任何估计方法，其操作涉及一个重要的随机组件。在这里，我们特别将它用于基于平均完整回报的方法(与从部分回报中学习的方法相反，我们将在下一章中进行讨论)。
	蒙特卡罗方法样本和每个状态-动作对的平均回报就像我们在第二章的样本和每个动作的平均回报一样。主要的区别是，现在有多个状态，每个状态都像一个不同的强盗问题(比如关联搜索或上下文强盗)，而不同的强盗问题是相互关联的。也就是说，在一个州采取行动后的回报取决于在同一事件中在后来的州采取的行动。由于所有的动作选择都在进行学习，从早期状态的角度来看，问题变得非平稳的。
	
	为了处理非平稳性，我们采用了第4章为DP开发的通用策略迭代(GPI)的思想。这里我们从MDP知识中计算值函数，这里我们从MDP的样本返回中学习值函数。值函数和相应的策略仍然以基本相同的方式相互作用以获得最优性(GPI)。DP的章,我们首先考虑预测的问题(vπ和qπ固定的计算任意政策π)然后政策改进,,最后,通过谷歌控制问题及其解决方案。从DP中获得的每一个想法都扩展到蒙特卡罗的情况，在这种情况下只有样本经验可用。
	
\end{summary}

\subsection{蒙特卡罗预测}

我们首先考虑蒙特卡罗方法来学习给定策略的状态值函数。回想一下，一个状态的值是预期的回报预期累积未来折扣奖励——从这个状态开始。从经验中估计它的一个明显的方法，就是简单地对访问该国后观察到的收益进行平均。当观察到更多的回报时，平均值应该收敛到期望值。这个想法是所有蒙特卡罗方法的基础。
特别是,假设我们希望估计vπ(s)的价值状态下政策π,给定一组集通过π和通过年代。每个发生的状态在一集被称为访问年代。当然,年代可能多次访问相同的事件;让我们叫它第一次访问在一个事件的首次访问。第一次访问MC方法估计vπ(s)的平均回报后第一个访问,而平均每次访问MC方法后返回所有访问。这两个蒙特卡罗(MC)方法非常相似,但是略有不同的理论属性。第一次拜访MC的研究最为广泛，可以追溯到20世纪40年代，这也是我们在本章关注的重点。每一次访问MC都更自然地扩展到函数逼近和资格跟踪，如第9章和第12章所讨论的那样。第一次参观的MC以程序形式显示在方框中。每一次拜访的主持人都是一样的，除了没有检查St是否在之前发生过。

第一次访问MC和每次访问MC收敛于vπ(s)访问的数量(或第一次访问)年代趋于无穷。这是第一次访问的情况下容易看到MC。在这种情况下每个返回是一个独立同分布的估计vπ(s)与有限方差。根据大数定律，这些估计的平均数序列收敛于它们的期望值。每个平均本身就是一个无偏估计,其误差的标准差为1 /√n,其中n是返回平均的数量。每次访问MC并不简单,但其估计也成平方收敛vπ(s)(辛格和萨顿,1996)。
通过一个例子可以很好地说明蒙特卡罗方法的使用。
例5.1:Blackjack流行的赌场纸牌游戏的目标是在不超过21的情况下，获得数值尽可能大的纸牌的总和。所有的牌都是10张，ace可以是1张或11张。我们考虑的是每个玩家与经销商独立竞争的版本。游戏开始时，发牌者和牌手各发两张牌。其中一张牌是正面朝上的，另一张是正面朝下的。如果玩家马上有21张牌(一张王牌和一张10张牌)，这就叫做自然牌。然后他就赢了，除非发牌人也有自然牌，在这种情况下，游戏是平局。如果玩家没有一个自然牌，他可以要求额外的牌，一个接一个(点击)，直到他停止(粘贴)或超过21(崩溃)。如果他破产了，他就输了;如果他坚持下去，就该轮到交易商了。经销商按照固定的策略击球或坚持，没有选择的余地:他坚持任何17或以上的点数，并击中其他。如果经销商破产，那么玩家获胜;否则，结果-赢、输或取-取决于谁的最终总数接近21。
玩blackjack自然是一种情景式的有限MDP。21点的每一场比赛都是一集。奖励+ 1−1和0的胜利,失去,分别和绘画。游戏内的所有奖励都是零,我们不要折扣(γ= 1);因此，这些终极奖励也是回报。玩家的动作是击中或卡住。状态取决于玩家的牌和经销商的显示卡。我们假设牌是从无限的牌牌(即:)，以便对已处理过的卡片进行跟踪没有任何好处。如果玩家手里拿着一张他可以数到11的王牌而不被击出，那么这张王牌就是可用的。在这种情况下，它总是被算作11，因为把它算作1就等于11或更少，在这种情况下，没有必要做出决定，因为显然，玩家应该总是命中。因此，玩家在三个变量的基础上做出决定:当前的总数(12-21)，经销商的一张展示卡(ac - 10)，以及他是否拥有一个可用的ace。总共有200个州。
考虑如果玩家的总数是20或21，如果不这样做的话，该策略就会失效。为了通过蒙特卡罗方法找到该策略的状态值函数，一个模拟了许多使用策略的blackjack游戏，并平均每个状态下的返回值。通过这种方式，我们得到了图5.1所示的状态值函数的估计值。使用ace的状态的估计不那么确定，也不那么规则，因为这些状态不太常见。无论如何，在50万场比赛后，价值函数是非常接近的。

10000集之后

500000集之后
 


图5.1:由蒙特卡罗策略评估计算的仅停留在20或21上的21点的21点策略的近似状态值函数。



练习5.1考虑图5.1中右边的图表。为什么估计值函数会在后面的两行中跳起来?为什么它在左边的最后一行会下降?为什么上面的图中最前面的值更高?
比低吗? 					?

练习5.2假设在21点任务中使用了每个访问MC而不是第一次访问MC。你认为结果会有很大不同吗?为什么或为什么不呢??


虽然我们对blackjack任务中的环境有完整的了解，但是使用DP方法来计算值函数并不容易。DP方法需要下一个事件的分布—特别是，它们需要四参数函数p所给出的环境动力学—而且对于21点来说，要确定这一点并不容易。例如，假设玩家的和是14，他选择坚持。他终止的概率是+1作为经销商的显示卡的函数?在应用DP之前，必须计算所有的概率，而这种计算通常是复杂的，容易出错的。相反，生成蒙特卡罗方法所需的示例游戏是很容易的。这种情况经常发生;蒙特卡罗方法能够单独使用样本集，这是一个非常重要的优势，即使我们已经完全了解了环境的动态。
我们能把备份图的概念推广到蒙特卡罗算法吗?备份图的一般思想是在顶部显示要更新的根节点，并在下面显示所有的转换和叶节点，它们的奖励和估计值对更新有贡献。蒙特卡罗估计vπ,根节点是一个状态,下面是过渡的整个轨迹沿着一个特定的一集,结局

在终端状态，如右边所示。DP图(第59页)显示了所有可能的转换，而蒙特卡罗图只显示了在一集中抽样的转换。而DP图只包含一步转换，蒙特卡罗图一直到这一集的结尾。图中的这些差异准确地反映了算法之间的基本差异。
蒙特卡罗方法的一个重要事实是每个状态的估计是独立的。一个国家的估计数不以任何其他国家的估计数为基础，如DP的估计数。换句话说，蒙特卡罗方法不像我们在前一章中定义的那样引导。
特别要注意，估计单个状态值的计算开销与状态数无关。这可以使蒙特卡罗方法在只需要一个或一个子集的值时特别具有吸引力。
的状态。可以从感兴趣的状态开始生成许多样例片段，只对这些状态的收益进行平均，忽略其他所有状态。这是蒙特卡罗方法的第三个优势，它可以使用DP方法(在从实际经验和模拟经验中学习的能力之后)。

线圈上的气泡。
Hersh and Griego(1969)。与许可转载。c ?1969《科学美国人》，《自然美国公司》，版权所有。
例5.2:肥皂泡假设一个线框形成一个封闭的环，在肥皂水里泡，形成一个肥皂表面或气泡符合它的边缘到线框。如果线框的几何形状是不规则但已知的，那么如何计算曲面的形状呢?形状具有这样的性质:相邻点施加在每个点上的总力为0(否则形状会改变)。这意味着任何一点上的表面高度都是在这个点周围的小圆上的高度的平均值。此外，表面必须满足其边界与线框。处理此类问题的通常方法是在所覆盖的区域上设置一个网格
在网格点上通过迭代计算得到其高度。边界上的网格点被强制到线框，其他所有的点都被调整到它们四个最近邻的平均高度。然后这个过程迭代，就像DP的迭代策略评估一样，最终收敛到期望的表面。
这类似于蒙特卡罗方法最初设计的问题。与上面描述的迭代计算不同，想象一下站在地面上，随机行走，从网格点随机走向相邻的网格点，概率相等，直到你到达边界。结果表明，边界处高度的期望值是接近起始点理想表面高度的近似值(实际上，它正是上面描述的迭代法计算的值)。因此，我们可以近似地估计出

通过简单地平均许多次行走的边界高度，在一点开始。如果只关心某一点上的值，或任何固定的小点集，那么蒙特卡罗方法可以比基于局部一致性的迭代方法有效得多。


\section{行为值的蒙特卡罗估计}

如果一个模型不可用，那么估计动作值(状态-动作对的值)而不是状态值是特别有用的。对于模型，状态值本身就足以确定策略;我们只需要向前看一步，然后选择任何能带来奖励和下一个状态的最佳组合的行为，就像我们在关于DP的章节中所做的那样。但是，如果没有模型，仅使用状态值是不够的。必须明确地估计每个操作的值，以便这些值在建议策略时有用。因此,我们的一个主要目标估计问∗蒙特卡罗方法。为此，我们首先考虑行动价值的政策评估问题。
行动值的政策评估问题是估计qπ(,),开始时的预期收益在国家年代,采取行动,然后遵循政策π。蒙特卡罗方法在本质上和描述国家价值是一样的，除了现在我们讨论的是对国家行动的访问而不是对国家的访问。一个状态-动作对，据说在一集中被访问，如果有任何一个状态被访问并且在其中被采取行动。每一次访问MC方法估计状态-动作对的值，作为所有访问后返回的平均值。第一次访问MC方法在每一集访问状态并选择动作的第一次之后平均返回。这些方法与以前一样，以四倍的方式收敛于实际期望值，因为每个状态-动作对的访问次数趋于无穷大。
唯一的复杂之处在于许多状态-动作对可能永远不会被访问。如果π是一个确定性的政策,然后在π人将只返回后从每个国家的行动。在没有平均回报的情况下，蒙特卡罗对其他行动的估计不会因经验而有所改善。这是一个严重的问题，因为学习动作值的目的是帮助在每个状态中选择可用的动作。为了比较替代方案，我们需要估计来自每个状态的所有操作的值，而不仅仅是我们当前喜欢的。
这是在第2章k -武装土匪问题上下文中讨论的维持勘探的一般问题。为了使政策评估工作的行动价值，我们必须保证持续的探索。一种方法是指定以状态-动作对开始，并且每对都有非零的被选择为开始的可能性。这保证了所有的状态-动作对将被访问无限次，在无限次的集数中。我们称之为探索开始的假设。
探索开始的假设有时是有用的，但通常不能依赖它，特别是当从与环境的实际交互中直接学习时。在这种情况下，启动条件不太可能有这么大的帮助。确保遇到所有状态操作对的最常见的替代方法是

只考虑具有非零概率的随机策略，在每个状态中选择所有操作。我们将在后面的小节中讨论这种方法的两个重要变体。目前，我们保留了探索开始和完成完整的蒙特卡罗控制方法的假设。
练习5.3备份是什么图蒙特卡罗估计qπ吗??


\section{蒙特卡罗控制}

我们现在准备考虑如何在控制中使用蒙特卡罗估计，即近似最优策略。总体思路是按照与DP章节相同的模式进行，也就是说，按照广义政策迭代的思想进行
评价

改进
π问
π吗?贪婪(Q)问?qπ
(GPI)。在GPI中，一个函数同时维护一个近似策略和一个近似值函数。值函数被反复修改以更接近当前策略的值函数，而策略在当前值函数方面被反复改进，如图右所示。这两种变化在某种程度上是相互作用的，因为每个变化都为另一个创建了一个移动的目标，但它们共同导致策略和价值函数趋于最优。
首先，让我们考虑一下经典策略迭代的蒙特卡罗版本。在这种方法中,我们表现的交替政策评估和政策改进的完整步骤,从任意政策π0开始和结束的最优策略和最优行为价值功能:
π0
子邮件
−→qπ0
我
−→π1
子邮件
−→qπ1
我
−→π2
子邮件
−→···
我
−→π∗
子邮件
−→问∗

在E−→表示一个完整的政策评估和我−→表示一个完整的政策改进。政策评估完全按照前一节所述进行。由于近似的动作-值函数渐近地逼近真函数，因而出现了许多次发作。现在，让我们假设我们确实观察了无数的情节，而且，这些情节是在探索开始时产生的。根据这些假设,蒙特卡洛方法将计算每个qπk,任意πk。
策略改进是通过使策略对当前值函数变得贪婪来实现的。在这种情况下，我们有一个动作值函数，因此不需要模型来构造贪婪策略。对于任何行为价值函数q,相应的贪婪的政策是,对于每个∈年代,确定性与最大操作值选择一个动作:

π(年代)。= argmax
一个
问(年代)。 					(5.1)

政策改进然后可以通过构建每个πk + 1对qπk贪婪的政策。政策改进定理(4.2节),那么适用于πk

和πk + 1,因为对于所有∈年代,
qπk(年代,πk + 1(s))= qπk(年代,argmax
一个
qπk(s))

=最大
一个
qπk(年代)
≥qπk(年代,πk(s))≥vπk(年代)。
正如我们在前一章中讨论的,这个定理保证我们每个πk + 1均匀比πk,或πk一样好,在这种情况下,他们都是最优策略。这反过来又保证了整个过程收敛到最优策略和最优值函数。在这种情况下，蒙特卡罗方法可以用来寻找最优的策略，只提供样本集，而不了解环境的动力学知识。
为了方便地得到蒙特卡罗方法的收敛保证，我们做了两个不太可能的假设。一个是，这些事件有探索的开始，另一个是，政策评估可以用无数个事件来完成。为了得到一个实用的算法，我们必须去除这两个假设。我们将第一个假设的考虑推迟到本章后面的部分。
目前，我们关注的是，政策评估的作用是无限次的。这个假设相对容易删除。事实上，即使在经典的DP方法中也会出现同样的问题，比如迭代策略评估，它也只渐进地收敛于真值函数。在DP和蒙特卡罗的情况下，有两种方法可以解决这个问题。一个是持有公司的想法近似qπk在每个政策评估。测量和假设是为了获得估计误差的大小和概率的界限，然后在每次政策评估期间采取充分的步骤，以确保这些界限足够小。这种方法可能完全令人满意，因为它可以保证正确的收敛到某种程度的近似。然而，它也可能需要太多的插曲，在实践中有用的任何问题，除了最小的问题。
第二种方法是避免在名义上需要进行政策评估的无数个阶段，在这些阶段中，我们放弃了在恢复政策改进之前完成政策评估的努力。在每个评估步骤,我们将向qπk价值函数,但实际上我们并不期望接近除了在许多步骤。我们在第4.6节首次介绍GPI的概念时使用了这个概念。该思想的一种极端形式是价值迭代，即在策略改进的每个步骤之间只执行一次迭代策略评估。值迭代的就地版本甚至更为极端;在那里，我们在单个状态的改进和评估步骤之间交替进行。
对于蒙特卡罗的政策评估来说，很自然地要在评估和改进之间交替进行。在每一集结束后，观察到的收益被用于政策评估，然后在每一集访问的所有州都对政策进行改进。一个完整的简单算法沿着这些线，我们称为蒙特卡罗ES，为蒙特卡罗探索开始，给出了伪代码在盒子的下一页。

蒙特卡洛ES的伪代码效率很低，因为对于每个状态-动作对，它维护一个所有返回的列表并反复计算它们的平均值。使用类似于2.4节中解释的技术来维护仅仅是平均值和计数(对于每个状态操作对)，并且增量地更新它们将会更有效。
描述如何修改伪代码以实现这一点。 					?
在蒙特卡洛ES中，每个状态-行为对的所有回报都是累积和平均的，不管观察到什么政策是有效的。很容易看出蒙特卡罗不可能收敛于任何次优策略。如果它这样做了，那么值函数将最终收敛到该策略的值函数，这反过来将导致策略的更改。只有当政策和价值函数都是最优时，才能实现稳定性。当动作-值函数的变化随着时间的推移而减小时，收敛到这个最优不动点似乎是不可避免的，但是还没有得到正式的证明。在我们看来，这是强化学习中最基本的开放理论问题之一(参见tsiklis, 2002)。例子5.3:解决21点的问题将蒙特卡洛ES应用到21点很简单。因为每一集都是模拟游戏，所以很容易安排包括所有可能性的探索开始。在这种情况下，你只需选择发牌人的牌，玩家的总数，以及玩家是否有可用的ace，所有这些都是随机的，概率相等。作为初始策略，我们使用前一个21点示例中评估的策略，该策略只适用于20或21。对于所有状态-动作对，初始动作-值函数可以为零。图5.2显示了蒙特卡洛发现的21点的最优策略。该策略与Thorp(1966)的“基本”策略相同，唯一的例外是可用ace策略中最左的一个等级，而Thorp的策略中没有这一项。我们不确定产生这种差异的原因，但是我们相信这里显示的确实是我们所描述的21点版本的最佳策略。

One hundred.


V *

1 1

1 1

图5.2蒙特卡洛ES发现的21点的最优策略和状态值函数。所示的状态值函数是从蒙特卡罗ES发现的动作值函数中计算出来的。

\subsection{无探测启动的蒙特卡罗控制}

我们怎样才能避免不太可能的探险开始的假设呢?确保无限次地选择所有操作的惟一通用方法是代理继续选择它们。有两种方法可以确保这一点，这导致了我们所称的“政策上的方法”和“政策外的方法”。策略方法试图评估或改进用于决策的策略，而策略方法则评估或改进与用于生成数据的策略不同的策略。上面开发的蒙特卡罗ES方法是一个政策方法的例子。在本节中，我们将展示如何设计一种不使用不切实际的探索开始假设的策略蒙特卡罗控制方法。下一节将考虑策略外方法。
在政策控制方法的政策通常是柔软,这意味着π(|)> 0∈年代和所有∈(s),但渐渐地转移越来越接近确定性的最优政策。第2章中讨论的许多方法提供了实现这一点的机制。我们目前的政策方法这一节使用ε-greedy政策,这意味着大多数时候他们选择一个行动,最大估计行动的价值,但概率ε他们而不是随机选择一个行动。即所有nongreedy行动给出的最小概率选择,ε(s)| |,剩下的大部分的概率,1−ε+ε(s)| |,贪婪的行动。ε-soft政策的例子,ε-greedy策略定义为政策的π(|)≥ε|一个(s)|对于所有国家和动作,对于一些ε> 0。在ε-soft政策,ε-greedy政策在某种意义上

这是最接近贪婪的。
在政策上蒙特卡罗控制的总体思路仍然是GPI。就像在蒙特卡罗ES中一样，我们使用第一次访问MC方法来估计当前策略的行动价值函数。但是，如果没有开始探索的假设，我们就不能简单地通过对当前值函数的贪婪来改进策略，因为这样可以防止对非贪婪行为的进一步探索。幸运的是，GPI并不要求将该策略一直应用到贪婪的策略上，而只是要求将其转向贪婪的策略。在我们的政策方法我们将只有一个ε-greedy政策。对于任何ε-soft政策,π,任何关于qπε-greedy政策是保证优于或等于π。完整的算法在下面的框中给出。
 

任何ε-greedy政策对qππ是一个比任何ε-soft政策是保证政策的改进公式。让π吗?ε-greedy政策。政策改进定理的条件,因为申请任何∈年代:

qπ(年代,π?(s))=

一个
π?(|)qπ(,)

=
ε
| |(s)

一个
qπ(s)+(1−ε)马克斯
一个
qπ(年代) 					(5.2)

≥ε(s)| |

一个
qπ(s)+(1−ε)

一个
π(|)−ε(s)| |
1−ε
qπ(年代)

(和是一个非负权和为1的加权平均数，因此

必须小于或等于最大的平均数)

=
ε
| |(s)

一个
qπ(s)−ε(s)| |

一个
qπ(s)+

一个
π(|)qπ(,)

= vπ(s)。
因此,通过政策改善定理,π吗?≥π(即。,vπ?(s)≥vπ(s),对所有∈年代)。我们现在证明平等可以容纳只有当两个π吗?和πε-soft之间的最优政策,也就是说,当它们优于或等于其它所有ε-soft政策。
考虑一个新的环境,就像原始的环境中,除了要求政策是ε-soft“内部”环境。新环境具有与原始环境相同的动作和状态集，其行为如下。如果在状态和采取行动,那么概率1−ε新环境的行为就像旧的环境。随机概率ε重新选择英雄的行动,以同样的概率,然后像旧的与新的环境,随机行动。最好可以做在这个新的环境与一般的政策是一样的最好的一个可以在原始环境ε-soft政策。让   v∗

问∗表示最优值函数的新环境。
v∗。
v∗我们知道这是唯一的解决方案

v∗(s)=(1−ε)马克斯
一个

问∗(s)+ε(s)| |
一个

问∗(年代)
=(1−ε)马克斯一个

r s ?,  p(s ?


r +γ
v∗(s ?)
+

ε

| |(s)
一个r s ?,

p(s ?  r | s,)

r +γ

当拥有平等和πε-soft政策不再是改进,然后我们也知道,从(5.2)
vπ(s)=(1−ε)马克斯

一个
qπ(s)+ε(s)| |

一个
qπ(年代)

=(1−ε)马克斯
一个r s ?,

p(s ?


r +γvπ(?)
+
ε

| |(s)

一个
r s ?,p(s ?

r +γvπ(?)

然而,这个方程是与前一个相同,除了vπ的替换v∗。因为
v∗是独特的解决方案,它必须vπ=v∗。

大致与前一节相同。现在我们只有ε-soft政策实现最好的政策,但另一方面,我们已经消除了假设的探索的开始。


\subsection{通过重要性抽样进行政策外预测}

所有的学习控制方法都面临着一个两难的境地:它们寻求在后续最优行为条件下学习行为值，但为了探索所有的行为(寻找最优行为)，它们需要非最优行为。他们如何在执行探索性策略时了解最优策略?上一节中的on-policy方法实际上是一种折衷方法——它学习的行为值不是针对最优策略，而是仍然在探索的接近最优策略。一个更直接的方法是使用两个策略，一个是学习到的，一个是最优策略，一个是探索性的，用于生成行为。正在学习的策略称为目标策略，用于生成行为的策略称为行为策略。在这种情况下，我们说学习是从目标策略的“off”数据中学习，整个过程被称为“off -policy learning”。
在本书的其余部分中，我们同时考虑政策上的方法和政策外的方法。On-policy方法通常更简单，首先考虑。Off-policy方法需要额外的概念和符号，而且由于数据是由不同的策略导致的，所以Off-policy方法的差异通常更大，收敛速度也更慢。另一方面，政策之外的方法更为强大和普遍。它们包括策略方法，作为目标和行为策略相同的特殊情况。策略外方法在应用程序中也有各种附加用途。例如，它们通常可以应用于从传统的非学习控制器生成的数据或从人类专家那里学习。政策之外的学习也被一些人视为学习世界动态多步骤预测模型的关键(见第17.2节;萨顿,2009;萨顿et al .,2011)。
在本节中，我们将从预测问题开始研究非策略方法，其中目标策略和行为策略都是固定的。即,假设我们希望估计vπ或qπ,但我们是集后另一项政策b,b在哪里? =π。在这种情况下,π是目标政策,b是行为的政策,这两项政策被认为是固定的,。
为了使用集从b为π估算值,我们要求每一个行动在π也是,至少偶尔,在b。也就是说,我们要求π(|)> 0意味着b(|)> 0。这叫做承保范围假设。报道可以看出,b必须随机州π是不相同的。π目标政策,另一方面,可能是确定的,事实上,这是一个特别感兴趣的控制应用程序。在控制中，目标策略通常是与当前操作值函数估计值相关的确定性贪婪策略。这一政策成为一个确定性的最优政策政策仍然是随机和更多的探索性行为,例如,一个ε-greedy政策。然而,在这一节中,我们考虑的预测问题,在π是不变的。
几乎所有的非策略方法都使用重要性抽样，这是一种通用的方法

估计一个分布下的期望值，给出另一个分布下的样本值。我们将重要性抽样应用到非政策学习中，根据目标和行为政策下所发生的轨迹的相对概率来加权收益，称之为进口-抽样比率。给定一个初始状态St，下一个状态运动轨迹的概率，在St+1处，在+1处…圣,发生在任何政策π

Pr{At, St+1, At+1，…,圣|圣:T−1∼π}
在| =π(St)p(St + 1 |圣,在π(+ 1 |圣+ 1)···p(St |圣−1−1)=
T−1 ?k = t
π(Ak | Sk)p(Sk + 1 | Sk,Ak),


其中p为(3.4)定义的状态跃迁概率函数。因此，目标和行为策略下轨迹的相对概率(重要性-采样比)为


ρt:T−1
。=
? T−1
k = tπ(Ak | Sk)p(Sk + 1 | Sk,Ak)? t−1 k = t b(Ak | Sk)p(Sk + 1 | Sk,Ak)
=
T−1 ?k = t
π(Ak | Sk)
b(Ak | Sk)。(5.3)
虽然轨迹概率依赖于一般未知的MDP的跃迁概率，但它们在分子和分母上都是相同的，因此抵消了。重要性抽样比率最终只取决于两个策略和序列，而不是MDP。
回想一下，我们希望估计目标策略下的预期回报(值)，但由于行为策略，我们所有的都是返回Gt。这些返回错误的期望E[Gt |圣= s]= vb(s)所以不能平均获得vπ。这就是重要性抽样的原因。比ρt:T−1转换返回正确的期望值:

E[ρt:T−1 gt |圣= s]= vπ(年代)。 					(5.4)
现在我们已经准备好给蒙特卡罗算法的平均回报一批观察发作后的政策b估计vπ(年代)。在这里，以一种跨事件边界增加的方式对时间步进行编号是很方便的。也就是说，如果第一个片段在时间100结束时处于终端状态，那么下一个片段在时间t = 101开始。这使我们能够使用时间步数来引用特定事件中的特定步骤。特别是，我们可以定义访问状态s的所有时间步骤的集合，表示为T(s)。这是为每一次访问的方法;对于第一次访问的方法，T(s)将只包含第一次访问s的时间步骤。同时,让T(T)表示的终止时间T后,第一次和Gt表示返回后T通过T(T)然后{ Gt } T∈(s)是属于国家的回报,和ρt:T(T)−1 ? T∈(s)是相应的重要性抽样比率。估计vπ(s),我们只是规模收益的比率和平均结果:

当重要抽样以这种方式进行时，它被称为普通重要性抽样。
一个重要的替代方法是加权重要性抽样，它使用加权平均数，定义为
 

或者分母为零。理解这两个品种的重要性抽样,考虑他们的第一次访问方法的估计后观察一个返回的状态。在加权平均估计,比ρt:T(T)−1单返回取消的分子和分母,所以估计等于观察恢复独立的比率比非零(假设)。考虑到这回来是唯一一个注意到,这是一个合理的估计,但其预期是vb(s),而不是vπ(s),并在统计意义上是有偏见的。相比之下,第一次访问普通版本的重要性抽样估计量(5.5)总是vπ(s)的期望(公正),但它可以极端。假设比率为10，说明在目标策略下观察到的轨迹是行为策略下的10倍。在这种情况下，通常的重要性抽样估计是观测到的收益的10倍。也就是说，即使这段插曲的轨迹被认为是非常具有代表性的目标政策，它也远未达到预期的回报。
在形式上，两种重要性抽样的第一次抽样方法的差异表现在它们的偏差和方差上。普通的重要性抽样是无偏的，而加权的重要性抽样是有偏的(尽管偏差渐近收敛到零)。另一方面，普通重要性抽样的方差一般是无界的，因为比率的方差可以是无界的，而在加权估计值中，任何单一收益的最大权重是1。事实上，假设有界回报，即使比率本身的方差为无穷大(Precup, Sutton, Dasgupta 2001)，加权重要抽样估计值的方差也收敛到零。在实践中，加权估计值通常具有较低的方差，并且是较强的首选。然而，我们不会完全放弃普通的重要性抽样，因为它更容易扩展到使用函数逼近的近似方法，我们将在本书的第二部分中探索。
普通抽样和加权重要性抽样的每一次抽样方法都是有偏差的，但是，随着样本数量的增加，偏差会逐渐趋于零。在实践中，每次访问方法通常是首选的，因为它们消除了跟踪访问了哪些州的需要，而且它们更容易扩展到近似。在110页的下一节中，给出了一种完整的每一次访问MC算法，用于使用加权重要性抽样进行离策略评估。

练习5.5考虑一个MDP和一个非终结符状态和一个行动转换回非终结符状态概率p和转换到终端状态的概率1−p。在所有转换让奖励+ 1,并让γ= 1。假设你观察到一个情节持续10步，然后返回10步。非终端状态的价值的第一次访问和每一次访问估计是什么??

示例5.4:对Blackjack状态值的离策略估计，我们使用了普通的和加权的输入-抽样方法，从离策略数据中估计单个Blackjack状态的值(示例5.1)。回想一下蒙特卡罗方法的优点之一是，它们可以用来评估单个状态，而不需要对其他任何状态进行估计。在这个例子中，我们评估了发牌人展示一张牌的状态，玩家牌的总数是13张，玩家有一张可用的ace(即玩家持有一张ace和一张deuce，或相当于3张ace)。数据是从这个状态开始生成的，然后选择按相同概率(行为策略)随机命中或随机选择。目标策略是只坚持20或21，如示例5.1所示。这种状态下的价值目标政策大约是−0.27726(这是由分别生成一百集使用目标政策和平均回报)。这两种离策略方法在使用随机策略的1000次离策略事件后都接近这个值。为了确保他们能可靠地做到这一点，我们进行了100次独立的实验，每一次实验都从零开始，学习了10000集。图5.3显示了合成的学习曲线——每个方法的估计的平方误差作为周期数的函数，在100次运行中平均。两种算法的误差接近于零，但加权输入-抽样方法在开始时误差小得多，这在实践中是很典型的。
 
图5.3:加权重要性抽样产生的错误估计值较低，即从策略外的事件中得到的单个21点状态的值。


例5.5:无限大方差一般重要性抽样的估计值具有无限大的方差，因此，当规模收益有无限大的方差时，其收敛性是不令人满意的。图5.4中显示了一个简单的示例。只有一个非终结态s和两个动作，右和左。右边的动作导致确定性的过渡到终止，而左边的动作，概率是0.9，回到s，概率是0.1，到终止。后一种转变的回报是+1，否则为0。考虑总是选择左边的目标策略。在此策略下的所有阶段都包含一定数量的转换(可能为零)

到s后终止，奖励和回报+1。因此年代的价值目标政策下是1(γ= 1)。假设我们估计这个值从off-policy数据使用的行为策略,选择正确的,剩下的概率相等。
 
10万1千1百万1千1百万1万

集(对数尺度)

图5.4:普通重要性抽样在inset显示的单状态MDP上产生了惊人的不稳定估计(示例5.5)。正确估计这里是1(γ= 1),而且,即使这是一个示例返回的期望值(重要性抽样后),样本的方差是无限的,估计不收敛于这个值。这些结果是针对非政策的首次访问MC。

图5.4的下半部分显示了使用普通重要性抽样的第一次访问MC算法的10次独立运行。即使在经历了数百万次的事件之后，估计也没有达到1的正确值。相比之下，加权重要采样算法会在第一集以左动作结束后给出准确的1的估计值。所有收益不等于1(即以正确的行动)是不一致的与目标政策,因此将会有一个ρt:T(T)−1的零,既有助于(5.6)的分子和分母。加权重要性-抽样算法只产生与目标策略一致的回报率的加权平均数，所有这些都是1。
我们可以通过一个简单的计算来验证在这个例子中，输入- samplingscale返回的方差是无穷大的。方差的随机变量X的期望值是偏离其平均X̄,可以写

Var[X]。= E

−X̄
2吗?
= E

X2−2 xx̄+ X̄2

= E

X2

−X̄2。

因此，如果均值是有限的，在我们的例子中，当且仅当

随机变量的平方的期望是无限的。因此，我们只需要证明，重要性抽样规模收益的期望平方是无限的:



海尔哥哥
⎡
⎣吗?T−1 ?t = 0
π(圣)|
在| b(St)G0
2⎤⎦

为了计算这个期望，我们将它分解为基于事件长度和终止的情况。首先要注意，对于任何以正确动作结尾的情节，重要性抽样比率为零，因为目标策略永远不会采取这种行动;因此，这些事件对期望没有任何贡献(括号中的数量将为零)，可以忽略。我们只需要考虑包含一些向左动作(可能为零)的事件，这些左动作会转换回非终结状态，然后是向左动作转换到终止状态。所有这些事件的返回值都是1，因此可以忽略G0因子。为了得到期望的平方，我们只需要考虑每一集的长度，将每一集发生的概率乘以重要抽样比率的平方，然后相加:
 
 

= 0.1
∞吗?k = 0
0.9k·2k·2 = 0.2
∞吗?k = 0
1.8 k =∞。

练习5.6对于作用值Q(s, a)而不是
状态值V (s)，再次给出使用b生成的返回值? 					?

在学习曲线中，如图5.3中所示，练习5.7通常会随着训练而减少，就像普通的重要性抽样方法一样。但对于加权输入-抽样方法误差先增大后减小。为什么
你认为这是真的吗? 					?

练习5.8结果为例5.5，如图5.4所示，使用了第一次访问MC方法。假设在相同的问题上使用了一个全访问MC方法。估计量的方差仍然是无穷大吗?为什么或为什么不呢??

\section{增量实现}

蒙特卡罗预测方法可以逐步实现，每一集都可以使用第2章(2.4节)所述技术的扩展。而在第二章中我们平均回报，在蒙特卡罗方法中我们平均回报。在所有其他方面，与第2章中使用的方法完全相同，可以用于on-policy Monte Carlo方法。对于非策略蒙特卡罗方法，我们需要分别考虑使用普通重要性抽样的方法和使用加权重要性抽样的方法。
在普通的重要性抽样,返回相应的重要性抽样比率ρt:T(T)−1(5.3),然后简单平均,如(5.5)。对于这些方法，我们可以再次使用第2章的增量方法，但是使用按比例计算的回报来代替那一章的回报。这就剩下了使用加权重要性抽样的非策略方法。这里，我们必须形成一个加权平均的回报，并且需要一个稍微不同的增量算法。
假设我们有一个返回序列G1 G2…,Gn−1,开始在同一个国家和每一个都有相应的随机Wi重量(例如,Wi =ρti:T(ti)−1)。我们希望作出估计
 

并保持它的最新，因为我们获得一个额外的回报Gn。除了跟踪Vn之外，我们还必须为每个状态维护给定的前n个返回的权重的累积和Cn。Vn的更新规则是
和

n + 1。= Cn + Wn + 1,

C0的地方。= 0 (V1是任意的，因此不需要指定)。下一页的框包含一个完整的逐集递增算法，用于蒙特卡罗政策评估。off-policy算法是名义上的情况下,使用加权重要性抽样,但也适用于对政策的情况下,通过选择目标和行为的政策是相同的(在这种情况下(π= b),W总是1)。近似Q收敛于qπ(所有遇到政府行动对),而根据一个潜在的不同的政策选择行为,b。
练习5.9修改第一次访问MC策略评估的算法(第5.1节)，以使用第2.4节中描述的样本平均值的增量实现。?
练习5.10从(5.7)推导出加权平均更新规则(5.8)。遵循
无加权规则的推导模式(2.3)。 					?

\section{场外蒙特卡罗控制}

我们现在准备展示我们在本书中考虑的第二种学习控制方法的例子:非策略方法。回想一下，on-policy方法的显著特征是，它们在使用策略进行控制时估计策略的价值。在非策略方法中，这两个函数是分开的。用于生成行为的策略(称为行为策略)实际上可能与被评估和改进的策略(称为目标策略)无关。这种分离的一个优点是目标策略可能是确定性的(例如，贪心)，而行为策略可以继续对所有可能的操作进行示例。
非策略蒙特卡罗控制方法使用前两部分中介绍的技术之一。他们在学习和改进目标政策的同时遵循行为政策。这些技术要求行为策略选择目标策略(覆盖率)可能选择的所有操作的概率是非零的。为了探究所有的可能性，我们要求行为策略是软的(即:，选择所有非零概率状态下的所有动作)。
盒子在下一页显示了off-policy蒙特卡罗控制方法,基于谷歌价格指数和加权重要性抽样估计π∗和q∗。目标政策π≈π∗贪婪的政策就问,这是一个估计的qπ。政策的行为可以是任何东西,但为了保证收敛的π的最优政策,无限的回报必须获得每一对状态和行动。这可以保证通过选择bε-soft。政策π收敛于最优状态即使遇到行动是选择根据不同的软政策b,这可能改变之间,甚至在事件。

一个潜在的问题是，该方法只从事件的尾部学习，而所有事件中剩余的动作都是贪婪的。如果不贪心的行为是普遍的，那么学习将是缓慢的，特别是对于那些出现在长时期早期的国家。这可能会大大降低学习速度。使用非政策蒙特卡罗方法来评估这个问题的严重程度的经验是不够的。如果它是严肃的，最重要的解决方法可能是合并时间差异学习，算法思想在下一章中发展。或者,如果γ小于1,那么下一节中所开发的想法也可以显著帮助。

练习5.11盒装off-policy MC算法控制,你可能期望W更新涉及重要性抽样比率π(| St)b(| St),但是
相反，它包含1个b(在|St)。为什么这是正确的呢? 					?
练习5.12:Racetrack(编程)考虑像图5.5所示的那样，驾驶一辆赛车在一个拐弯处行驶。你想走得越快越好，但不要跑得太快，以免偏离轨道。在我们简化的赛道上，汽车是在一个离散的网格位置，图中的单元格。速度也是离散的，许多网格单元格水平移动和垂直移动每一步。动作是速度分量的增量。每一个可能改变了+ 1,−1或0在每个步骤中,总共9(3×3)行动。两个速度分量都被限制为非负的和小于5的，它们不能都是零，除非在起跑线上。每一集都是从一个随机选择的开始状态开始，两个速度分量都为零，当赛车越过终点线时结束。奖励是−1每一步直到车穿过终点线。如果汽车撞到轨道边界，它被移动到起跑线上的一个随机位置，两个速度分量被减少到零，并且

起跑线 					起跑线


图5.5:赛道任务的右转数。


事件仍在继续。在每一步更新汽车的位置之前，检查汽车的预定路径是否与轨道边界相交。如果它与终点线相交，那一集就结束了;如果它与其他地方相交，那么汽车被认为已经到达了轨道边界并被送回起跑线。为了使任务更具挑战性，每一步的概率为0.1，速度增量都为0，与预期增量无关。将蒙特卡罗控制方法应用于该任务，从每个起始状态计算最优策略。沿着
最优政策(但关闭这些轨迹的噪声)。 					?


\section{Discounting-aware重要性抽样}

到目前为止，我们所考虑的场外政策方法是基于对被认为是单一整体的回报形成重要的抽样权重，而不考虑回报的内部结构作为折现回报的总和。我们现在简要地考虑使用这种结构来显著降低非策略估计量的方差的前沿研究思想。
例如,考虑一下这种情况:集长,γ明显小于1。具体性,说去年100步和γ= 0。时间0的回报将是G0 = R1，但它的重要性抽样比率将是
100年因素,π(A0 | S0)b(A0 | S0)
π(A1 | S1)
b(A1 | S1)···
π(A99 | S99)
b(A99 | S99)。在普通重要性抽样中，返回
将被缩放整个产品,但它只需要规模第一因子,通过π(A0 | S0)b(A0 | S0)。其他99个因素
π(A1 | S1)
b(A1 | S1)···
π(A99 | S99)
b(A99 | S99)因为无关
在第一次奖励之后，回报已经确定。这些后面的因素都与回报和期望值1无关;它们不会改变预期的更新，但会极大地增加其方差。在某些情况下，他们甚至可以

方差无限。现在让我们考虑一个避免这种巨大的额外差异的想法。这个概念的本质是把折现看成是确定概率
终止或者，等价地，部分终止的程度。对于任何γ∈(0,1),我们可以把返回G0部分终止在一个步骤中,程度1−γ,产生一个返回的第一个奖励,R1,部分终止后两个步骤,程度(1−γ)γ,生产R1 + R2的回报,等等。后者程度对应终止第二步,1−γ,而不是已经终止的第一步,γ。终止的程度在第三步是这样(1−γ)γ2,γ2反映的终止不会发生在前两个步骤。这里的部分收益称为平部分收益:
Ḡt:h。= Rt + 1 + Rt + 2 +···+ Rh,0≤t < h≤t,
其中，“flat”表示没有贴现，“partial”表示这些返回不会一直延伸到终止，而是在h处停止，称为horizon (T是该集的终止时间)。传统的全收益Gt可以看作是平的部分收益之和，如下所示:
Gt。= Rt + 1 +γRt + 2 +γ2Rt + 3 +···+γT−−1 t Rt
=(1−γ)Rt + 1
+(1−γ)γ(Rt + 1 + Rt + 2)
+(1−γ)γ2(Rt + 1 + Rt + 2 + Rt + 3)。


t +(1−γ)γT−−2(Rt + 1 + Rt + 2 +···+ Rt−1)+γT t−−1(Rt + 1 + Rt + 2 +···+ Rt)

=(1−γ)
T−1 ?h = t + 1
tγh−−1 ḡt:h +γT t−−1 ḡt:t。

现在我们需要用同样被截断的重要性抽样比率来衡量平坦的部分回报。Ḡt:h只涉及奖励地平线h,我们只需要概率之比h。我们定义一个普通的重要性抽样估计量,类似于(5.5),

V(s)。=	
t∈(s)

(1−γ)?T(T)−1 h = T + 1γh T−−1ρt:h−1 ḡT:h +γT T(T)−−1ρt:T(T)−1 ḡT:T(T)

| | T(年代) 					,	(5.9)
与(5.6)相似的加权输入-抽样估计量为


V(s)。=

t∈(s)

(1−γ)?T(T)−1 h = T + 1γh T−−1ρt:h−1 ḡT:h +γT T(T)−−1ρt:T(T)−1 ḡT:T(T)


t∈(s)

(1−γ)?T(T)−1 h = T + 1γh T−−1ρt:h−1 +γT T(T)−−1ρt:T(T)−1
吗?。(5.10)
我们把这两个估计量称为感知重要性的抽样估计量。他们考虑贴现率,但没有影响(是一样的off-policy估计从5.5节)如果γ= 1。

\section{每个决策重要性抽样}
还有一个方法返回的结构作为一笔奖励可以考虑在off-policy重要性抽样,这样可以减少方差即使没有折扣(即即使γ= 1)。在off-policy估计量(5.5)和(5.6),每一项的总和分子本身就是一笔:
ρt:T−1 gt =ρt:T−1

Rt + 1 +γRt + 2 +···+γT−−1 t Rt

=ρt:T rt + 1 +γρt−1:T−1 rt + 2 +···+γT T−−1ρt:T−1 rt。(5.11)
非策略估计依赖于这些术语的预期值，可以用更简单的方法编写。注意，每个子项(5.11)是随机奖励和随机输入-抽样比的乘积。例如，可以使用(5.3)as编写第一个子项

ρt:T rt + 1 =−1
π(圣)| b(圣)|
π(+ 1 |圣+ 1)b(+ 1 |圣+ 1)
π(+ 2 | St + 2)
b(+ 2 | St + 2)···
π(圣−−1 | 1)
b(圣−−1 | 1)Rt + 1。(5.12)
在所有这些因素中，人们可能会怀疑只有第一个和最后一个(奖励)是相关的;所有其他的都是在奖励后发生的事件。此外，所有这些其他因素的期望值是1:
子邮件

π(Ak | Sk)b(Ak | Sk)

。=

一个
b(| Sk)π(| Sk)b(一个| Sk)=

一个
π(| Sk)= 1。 					(5.13)

只要再多走几步，我们就会发现，正如人们所怀疑的那样，所有这些其他因素对期望都没有影响，换句话说，就是

E[ρt:T rt−1 + 1]= E[ρt:泰爱泰党+ 1]。 					(5.14)
如果我们对(5.11)的第k项重复这个过程，我们得到

E(ρt:T−1 rt + k)= E[ρt:T + k−1 rt + k)。
它遵循那我们最初的期望项(5.11)可以写E[ρt T−1 gt):= E

G̃t

在哪里

G̃t =ρt:泰爱泰党+ 1 +γρt:t rt + 2 + 1 +γ2ρt:t rt + 3 + 2 +···+γT t−−1ρt:t−1 rt。
我们把这种观点称为“按决策重要性抽样”。它紧跟着,有另一种重要性抽样估计,无偏相同的期望(在第一次访问的情况下)作为ordinary-importance-sampling估计量(5.5),使用G̃t:

我们可能期望方差更低。
加权重要性抽样是否有每个决策版本?这是不太清楚。到目前为止，我们所知道的所有估计量都是不一致的(也就是说，它们不收敛于具有无限数据的真实值)。
∗5.13运动中显示的步骤(5.14)(5.12)。 					?
∗练习5.14修改off-policy蒙特卡罗算法控制(第111页)使用截断加权平均估计量的概念(5.10)。注意，您首先需要
将此方程转换为动作值。 					?


\section{总结}

本章提出的蒙特卡罗方法通过样本集的形式从经验中学习价值函数和最优策略。与DP方法相比，这至少给了他们三种优势。首先，它们可以直接从与环境的交互中学习最优行为，没有环境动力学模型。第二，它们可以与仿真或样本模型一起使用。对于许多应用程序来说，即使很难构造出DP方法所要求的转换概率的显式模型，也很容易模拟样本集。第三，将蒙特卡罗方法集中在状态的一小部分上是容易和有效的。一个特别感兴趣的区域可以被精确地评估，而不需要精确地评估状态集的其他部分(我们将在第8章中进一步探讨这个问题)。
蒙特卡罗方法的第四个优点，我们在后面的书中讨论过，是它们可能较少受到违反马尔可夫财产的伤害。这是因为它们不根据继承国的价值估计来更新其价值估计。换句话说，这是因为它们没有引导。
在设计蒙特卡罗控制方法时，我们遵循了第4章中介绍的广义策略迭代(GPI)的总体模式。GPI涉及政策评价和政策改进的相互作用过程。蒙特卡罗方法提供了一个可供选择的政策评估过程。它们并不使用模型来计算每个状态的值，而是平均从状态开始的许多返回值。因为一个状态的值是期望回报，这个平均值可以成为这个值的一个很好的近似值。在控制方法中，我们对近似动作-值函数特别感兴趣，因为这些函数可以用来改进策略，而不需要环境的转换动力学模型。蒙特卡罗方法将政策评估和政策改进步骤按章节进行混合，并可按章节进行递增。
保持足够的勘探是蒙特卡罗控制方法中的一个问题。仅仅选择当前估计为最佳的操作是不够的，因为这样就不会获得替代操作的回报，而且可能永远也不会知道它们实际上更好。一种方法是忽略这个问题，假设这些事件从随机选择的状态-动作对开始，以涵盖所有可能性。这种探索的开始有时可以安排在模拟事件的应用程序中，但不太可能从真实经验中学习。在策略方法中，代理总是提交。

探索并尝试找到仍在探索的最佳策略。在off-policy方法中，代理也进行了探索，但是学习了可能与所遵循的策略无关的确定性最优策略。
策略外预测是指从不同行为策略生成的数据中学习目标策略的值函数。这种学习方法是基于某种形式的重要性抽样，即以两种策略下观察到的行为的概率之比来加权收益，从而将它们的期望从行为策略转换为目标策略。普通重要性抽样使用加权回报率的简单平均数，而加权重要性抽样使用加权平均数。普通重要性抽样产生的是无偏估计，但有较大的，可能是无限的，方差，而加权重要性抽样总是有有限的方差，在实践中是首选的。尽管它们的概念很简单，但用于预测和控制的非策略蒙特卡罗方法仍然不稳定，并且是一个正在进行的研究课题。
本章所处理的蒙特卡罗方法与前一章中处理的DP方法有两大不同。首先，它们基于样本经验进行操作，因此可以在没有模型的情况下进行直接学习。其次，它们不引导。也就是说，他们不根据其他价值估计来更新他们的价值估计。这两个差异并不是紧密相连的，而且可以分开。在下一章中，我们将考虑从经验中学习的方法，如蒙特卡罗方法，也将考虑引导方法，如DP方法。


书目的和历史的言论

“蒙特卡洛”这个词可以追溯到20世纪40年代，当时洛斯阿拉莫斯的物理学家们发明了一种可能性游戏，他们可以通过研究来帮助理解与原子弹有关的复杂物理现象。在这种意义上对蒙特卡罗方法的报道可以在几本教科书中找到(例如，Kalos和Whitlock, 1986;鲁宾斯坦,1981)。

5.1-2 Singh和Sutton(1996)区分了每次访问和第一次访问的MC
方法和证明了这些方法与强化学习算法的关系。blackjack示例基于Widrow、Gupta和Maitra(1973)使用的示例。肥皂泡的例子是一个经典的狄利克雷问题，它的蒙特卡罗解最早由Kakutani(1945)提出;参见赫什和格里戈，1969;柯南道尔和斯奈尔,1984)。
Barto和Duff(1994)讨论了在求解线性方程组的经典蒙特卡罗算法的背景下的政策评估。他们利用Curtiss(1954)的分析指出了蒙特卡罗政策评价对大问题的计算优势。

5.3-4蒙特卡洛在1998年出版的这本书中被介绍。可能
是蒙特卡罗估计和基于策略迭代的控制方法之间的第一个明确的联系。早期使用蒙特卡罗方法来估计增强学习环境中的动作值是由Michie和
