\chapter{第十章 在政策控制近似}
\begin{summary}
	在这一章里,我们回到控制问题,现在行为价值的参数近似函数q̂(年代,w)≈问∗(,),其中w∈Rd是一个有限维度权重向量。我们继续限制对政策上的案例的关注，把政策上的方法留给了第11章。本章主要介绍半梯度Sarsa算法，半梯度TD(0)在动作值和策略控制上的自然扩展。在情景性的情况下，扩展是简单的，但是在连续的情况下，我们必须后退几步，重新检查我们如何使用折现来定义最优策略。令人惊讶的是，一旦我们有了真正的函数逼近，我们就不得不放弃折现，转而使用新的“平均奖励”来制定控制问题，新的“微分”值函数。
	首先从情景性的例子开始，我们将上一章提出的函数逼近思想从状态值扩展到行为值。然后我们扩展他们控制政策谷歌价格指数的一般模式,使用ε-greedy行动选择。我们在山车问题上给出了n阶线性萨尔萨的结果。然后我们转向继续的情况，重复这些想法的发展为平均回报情况下的差异值。
\end{summary}

\section{章节Semi-gradient控制}
将第9章的半梯度预测方法扩展到作用值是很简单的。在这种情况下它是近似行为价值函数,问̂≈qπ,表示为参数化函数形式与权向量w。而在我们考虑随机培训形式的例子圣?→Ut,现在我们考虑的例子形成圣,?→Ut。更新目标Ut可以是任何近似qπ(圣,),包括常见的备份等完整的蒙特卡罗返回值(Gt)或任何n-step撒尔沙返回(7.4)。动作值的通用梯度下降更新。

预测是


wt + 1
.
= wt +α

Ut−问̂(圣,wt)

wt∇问̂(St)。 					(10.1)

例如，一步Sarsa方法的更新是


wt + 1
.
= wt +α

Rt + 1 +γq̂(圣+ 1 + 1,wt)−问̂(圣,在wt)

wt∇问̂(St)。(10.2)

我们称这种方法为间歇性半梯度一步Sarsa。对于常数策略，该方法收敛的方式与TD(0)相同，具有相同的误差范围(9.14)。
为了形成控制方法，我们需要将这种行为价值预测方法与政策改进和行动选择技术相结合。适用于连续操作或大型离散集合的操作的合适技术，是目前尚没有明确解决方案的研究课题。另一方面，如果操作集是离散的，而且不太大，那么我们可以使用前面几章中已经开发的技术。为每个可能的行动,在圣当前状态,我们可以计算q̂(圣,wt),然后找到贪婪的行动∗t = argmaxa问̂(圣,wt)。政策改进然后做(在本章在政策的情况下治疗)通过改变估计政策的软近似贪婪ε-greedy政策等政策。操作是根据相同的策略选择的。在方框中给出了完整算法的伪代码。
 

例如，在图10.1左上方的图表所示，山地车任务考虑的任务是在陡峭的山路上驾驶一辆动力不足的汽车。问题在于，重力比汽车的引擎更强，即使在全速行驶时，汽车也无法加速爬坡。唯一的解决办法是先离开目标，沿着左边的相反方向往上走。然后，通过完全油门汽车
 

图10.1:山车任务(上左面板)和cost-to-go函数(−maxa问̂(年代,w))学会在一次运行。



它可以积累足够的惯性使它沿着陡峭的斜坡向上移动，即使整个过程都在减速。这是一个连续控制任务的简单例子，在某种意义上，事情必须变得更糟(离目标更远)，才能变得更好。许多控制方法在处理此类任务时都有很大的困难，除非有人类设计者的明确帮助。
这个问题的奖励是−1在所有时间步骤直到车移动过去它的目标位置在山顶,结束这一事件。有三种可能的行为:加足马力向前(+ 1),加足马力反向(−1),和零油门(0)。汽车根据一个简化的物理移动。它的位置、xt和速度,ẋt,更新


xt + 1
。=绑定

xt + ẋt + 1


ẋt + 1
。=绑定

ẋt + 0.001−0.0025 cos(3 xt),
在绑定操作执行−1.2≤xt + 1≤0.5−0.07≤ẋt + 1≤0.07。此外,当xt + 1到达左绑定,ẋt + 1是重置为零。当它到达正确的界限时，目标就实现了，这一情节就结束了。每一集开始从一个随机位置xt∈(−0.6−0.4)和零速度。为了将两个连续状态变量转换为二进制特征，我们使用了如图9.9所示的网格倾斜。我们使用了8个倾斜，每一个平铺都覆盖了每个维度上1/8的有界距离，
第9.5.4.1节描述的不对称偏移量，然后将由tile coding生成的特征向量x(s, a)与参数向量线性组合，近似出动作-值函数:


问̂(s w)。= w ?x(,)=
d ?i = 1
wi·xi(,), 					(10.3)

对于每一对状态s和动作a。
图10.1显示了在学习用这种形式的函数逼近来解决此任务时通常发生的情况。显示的是在一次运行中学习到的值函数(成本-go函数)的负值。最初的行动值都为零,这是乐观的(所有真值是-在这个任务中),导致广泛的探索发生即使勘探参数,ε0。这可以在图的中上面板中看到，标记为“第428步”。在这个时候，甚至连一集都没有完成，但是汽车在山谷里来回摆动，沿着州空间的圆形轨迹运动。所有访问过的州都比未探索过的州的价值要低，因为实际的回报比(不切实际的)预期要差。这不断地将代理从它所在的任何地方赶走，以探索新的状态，直到找到一个解决方案。
图10.2显示了在这个问题上的半梯度Sarsa的几个学习曲线，有不同的步长。
 
200年
400山车
每集的步数
平均超过100分
 


图10.2:半梯度Sarsa算法的山地车学习曲线
函数逼近和ε-greedy行动选择。




特别地，我们使用了在http://incompleteideas.net/tiles/上提供的tile编码软件。
tiles3。使用iht= iht(4096)和tiles(iht,8，[8*x/(0.5+1.2)，8*xdot/(0.07+0.07))， A)获取状态(x, xdot)和动作A的特征向量中的索引。
2这些数据实际上是“semi-gradient撒尔沙(λ)”算法,我们不会满足,直到
第12章，但是半梯度萨尔萨的表现是相似的。


\section{Semi-gradient n-step撒尔沙}

在半梯度Sarsa更新方程(10.1)中，我们可以使用n步返回作为更新目标，得到情景半梯度Sarsa的n步版本。n阶返回立即从它的表格形式(7.4)推广到函数逼近形式:
Gt:t + n。= Rt + 1 +γRt + 2 +···+γn−1 Rt + n +γnq̂(圣+ n + n,wt + n−1),t + n < t,Gt(10.4):t + n。= Gt如果t + n≥t,像往常一样。n阶更新方程是

wt + n
。=ωt + n−1 +α(Gt:t + n−问̂(圣,在wt + n−1)]∇问̂(圣,在wt + n−1),0≤t < t。
(10.5)

完整的伪代码在下面的框中给出。
 
200年
400山车
每集的步数
平均超过100分
 

图10.3:单步与8步半梯度萨尔萨在山地车任务中的性能。
好大小使用步:α= 0.5/8 n = 1和α= 0.3/8 n = 8。
 
280年

山的车


240年
每集260步
平均值
前50集和100集
220年

0 0.5
1 1.5

α×瓷砖的数量(8)

图10.4:α和n对早期性能的影响n-step semi-gradient撒尔沙
在山地车任务中，代码函数近似。和往常一样，中级水平
引导(n = 4)表现最好。这些结果对于选择α值,在对数尺度,然后用直线连接。n = 1的标准误差范围为0.5(小于线宽)，n = 16的标准误差范围为4，所以主要影响都具有统计学意义。

练习10.1我们还没有明确地考虑或给出任何蒙特卡罗方法的伪代码或在本章中。他们会是什么样子?为什么不为它们提供伪代码呢?他们将如何完成山地车任务??

练习10.2给出半梯度单步预期Sarsa的伪代码进行控制。吗?练习10.3为什么图10.4所示的结果具有更高的标准错误
大n比小n大? 					?

\section{平均奖励:持续任务的新问题设置}

我们现在介绍了第三种经典的设置——与情景性和折扣的设置——用于在马尔可夫决策问题(MDPs)中制定目标。和折扣设置一样，平均奖励设置也适用于持续的问题，即代理和环境之间的交互一直持续下去，没有终止或启动状态。然而，与这种设定不同的是，没有折扣——代理关心的是延迟奖励，而不是即时奖励。平均奖励设置是动态规划经典理论中普遍考虑的主要设置之一，在强化学习中不太常见。正如我们在下一节中所讨论的，贴现设置在函数逼近中存在问题，因此需要使用平均回报设置来替换它。
平均报酬的设置,政策π的质量被定义为奖励的平均利率,或简单的平均回报,而这一政策后,我们表示r(π):
 
= lim
t→∞
E(t Rt | S0 A0:−1∼π), 					(10.7)

=

s
μπ(s)

一个
π(|)?
r s ?,
p(s ?)r,r | s,


期望以初始状态S0和后续动作为条件，A0 A1…据π−1,被。μπ稳态分布,μπ(年代)。圣= = limt→∞公关{ | A0:t−1∼π},这是假定存在任何π和S0的独立。这种关于MDP的假设被称为遍历性。这意味着，在MDP开始的地方或由代理做出的任何早期决定都只能产生暂时的影响;从长远来看，对处于一个国家的期望只取决于政策和MDP的转移概率。遍历性足以保证上述方程极限的存在。
在未贴现连续情况下，不同的最优性之间可以有微妙的区别。然而,对于大多数实用目的可能是足够简单的秩序政策根据每时间步的平均回报,换句话说,根据他们的r(π)。这个量是平均奖励在π,所显示(10.7)。特别是,我们考虑所有的政策获得的最大价值r(π)是最优的。
注意,稳态分布的特殊分布,如果您选择的行为根据π,你留在相同的分布。这是,

s
μπ(s)

一个
π(|)p(s ?|年代,)=μπ(s ?)。 					(10.8)

在平均奖励设置中，返回的定义是在不同之间。

250年 					第10章:带有近似的政策控制



奖励和平均奖励:

Gt。= Rt + 1 Rt + 2−−r(π)+(π)+ Rt + 3−r(π)+···。(10.9)这称为微分返回，相应的值函数称为微分值函数。他们以同样的方式定义,我们将使用相同的符号一直:vπ(年代)。= Eπ[Gt |圣= s]和qπ(年代)。圣= = Eπ[Gt |年代,在=](类似v∗和q∗)。微分值函数也有Bellman方程，和我们之前看到的稍有不同。我们只是删除所有γs替换所有奖励,奖励和真正的平均回报的区别:

vπ=

一个
π(|)?
(r,s ?
p(s ?r | s,)

−r(π)+ vπ(?)


qπ(,)=

(r,s ?
p(s ?r | s,)

−r(π)+ ?
一个?
π(|年代?)qπ(s ?,一个?)

,




v∗(s)= max
一个

(r,s ?
p(s ?r | s,)

r−马克斯
π
r(π)+ v∗(?)

,


问∗(,)=

(r,s ?
p(s ?r | s,)

r−马克斯
π
r(π)+马克斯
一个?
问∗(s ?,一个?)

(cf.(3.14)，练习3.17(3.19)和(3.20))。
还有两个TD错误的差异形式:

δt。= Rt + 1−R t + 1 + v̄̂(wt)圣+ 1−v̂(St,wt), 					(10.10)
和
δt。= Rt + 1−R̄t + 1 + q̂(圣+ 1 + 1,wt)−问̂(圣,在wt), 					(10.11)

在R̄t估计在时间t的平均回报R(π)。有了这些替代的定义，我们的大多数算法和许多理论结果都可以在不改变的情况下进行平均回报设置。
例如，半梯度Sarsa的平均奖励版本被定义为(10.2)，除了TD错误的差分版本。也就是说,通过

wt + 1
.
q = wt +αδt∇̂(圣,在wt), 					(10.12)

与δt由(10.11)给出。完整算法的伪代码在下一页的框中给出。
练习10.4给出半梯度Q-learning微分版本的伪代码。吗?练习10.5需要什么方程(超过10.10)来指定微分
版本的TD(0)? 					?

练习10.6考虑一个由三个状态组成的环a、B和C组成的马尔可夫奖励过程，状态转换在环周围以确定的方式进行。到达A时，奖励为+1，否则奖励为0。微分的值是多少
三个州? 					?

示例10.2:访问控制队列任务这是一个决策任务，涉及到一组10台服务器的访问控制。四个不同优先级的客户到达一个队列。如果给客户访问服务器，客户将根据自己的优先级向服务器支付1、2、4或8的奖励，优先级更高的客户将支付更高的费用。在每个时间步骤中，位于队列头部的客户要么被接受(分配给一个服务器)，要么被拒绝(从队列中删除，奖励为零)。在这两种情况下，下一个时间步骤将考虑队列中的下一个客户。队列从不清空，队列中客户的优先级也是随机分布的。如果没有免费的服务器，当然不能为客户提供服务;在这种情况下，客户总是被拒绝。每个繁忙的服务器在每个时间步上都有可能是p = 0.06。虽然我们刚才已经明确地描述了它们，但是让我们假设到达和离开的统计数字是未知的。任务是根据他的优先级和免费服务器的数量，决定是否接受或拒绝下一个客户，以便在不打折扣的情况下最大化长期回报。
在这个例子中，我们考虑这个问题的表格式解决方案。虽然状态之间没有泛化，但是我们仍然可以在一般的函数逼近设置中考虑它，因为这个设置泛化了表格设置。因此，我们对每一对状态(空闲服务器数量和位于队列顶端的客户的优先级)和操作(接受或拒绝)都有一个不同的动作值估计。图10.5显示了解决发现的微分semi-gradient撒尔沙与参数α= 0.01,β= 0.01,ε= 0.1。最初的行动值和R̄是零。
5
 
-5




-10年


0 1 2 3 4 5 6 7 8 9 10。

免费的服务器数量

图10.5:2百万步后访问控制队列任务上的微分半梯度单步Sarsa的策略和值函数。图表右边的下降可能是由于数据不足;许多这样的国家从未经历过。学会了R值̄约为2.31。



假设有一个MDP，在任何策略下产生奖励+1、0、+1、0、+1、+1、0的确定性序列…永远。严格地说，这是不允许的，因为它违反了遍历性;没有固定的极限分布μπ和限制(10.7)并不存在。然而，平均奖励(10.6)是很明确的;它是什么?现在考虑MDP中的两个状态。从A开始，奖励序列与上面描述的完全一样，从A +1开始，而从B开始，奖励序列以0开始，然后以+1、0、+1、0继续……微分回报(10.9)在这种情况下没有很好的定义，因为极限不存在。要修复这个问题，可以交替地定义状态的值为


vπ(年代)。= limγ→1 lim h→∞
h ?t = 0
γt

EπRt + 1 | S0 =[s]−r(π)

. 					(10.13)


在这个定义下，状态A和状态B的值是多少? 					?
251页上的练习10.8盒子里的伪代码更新R̄t + 1使用δt作为一个错误而不是简单的Rt + 1−R̄t + 1。这两个错误的工作,但是使用δt更好。要了解原因，请考虑练习10.6中三个状态的环MRP。平均报酬的估计值应趋向于其真实值1 3。假设它已经在那里并被扣留

卡在那里。什么Rt−R̄t错误的顺序是什么?δt错误的顺序是(使用(10.10))?如果允许估计随着误差的变化而变化，那么哪个误差序列会对平均奖励产生更稳定的估计?
为什么? 					?


\section{不赞成折现设置}

连续的、折现的问题公式在表列情况下非常有用，在这种情况下，每个状态的回报可以分别确定和平均。但在近似情况下，人们是否应该使用这个问题公式是值得怀疑的。
要知道为什么，要考虑一个没有开始或结束的无限序列，没有明确的确定状态。这些状态可能只由特征向量表示，而特征向量对于区分状态和其他状态几乎没有作用。作为特殊情况，所有的特征向量可能是相同的。因此，一个人实际上只有奖励序列(和行为)，而绩效只能从这些行为中进行评估。怎么做呢?一种方法是在长时间间隔内对奖励进行平均——这是平均奖励设置的思想。如何使用贴现?每个时间步都可以衡量折现收益。有些回报是小的，有些是大的，所以我们需要在足够大的时间间隔内对它们进行平均。在连续设置中，没有开始和结束，也没有特殊的时间步骤，所以没有其他事情可以做。然而，如果你这样做，结果是折现回报的平均值与平均回报成正比。事实上,对于政策π的平均折扣回报总是r(π)/(1−γ),也就是说,它本质上是平均回报,r(π)。特别地，在平均折扣返回设置中所有策略的顺序与在平均奖励设置中是完全相同的。贴现率γ因此没有影响问题公式化。它实际上可能是零，排名也不会改变。
这个令人惊讶的事实在下一页的盒子里被证明了，但是基本的观点可以通过一个对称的论证来发现。每个时间步都完全相同。有了折现，每个奖励都会在每个位置出现一次。t奖励将尚未完全出现在t−1日返回,一旦在t−2日返回,折扣和折扣999次t−1000返回。重量在t因此奖励1 +γ+γ2 +γ3 +···= 1 /(1−γ)。因为所有的国家都是一样的,他们都是加权,因此回报率的平均值将它乘以平均奖励,或者r(π)/(1−γ)。
这个例子和盒子里更一般的论点表明，如果我们优化折现值，而不是政策上的分配，那么效果将等同于优化未折现平均回报;γ的实际价值没有影响。这强烈地表明，在函数逼近控制问题的定义中，贴现没有作用。尽管如此，我们仍然可以在解决方法中使用贴现。折扣参数γ参数变化从一个问题解决方法参数!然而，不幸的是，在这种情况下，我们不能保证优化平均回报(或在政策上分配的等效折现值)。

折现控制设置困难的根本原因是由于函数逼近，我们失去了策略改进定理(第4.2节)。如果我们改变政策以提高一个国家的贴现价值，那么我们就肯定能在任何有用的意义上改进总体政策。这种保证是我们强化学习控制理论的关键。通过函数逼近，我们失去了它!
事实上，缺乏一个政策改进定理也是一种理论缺陷，这是对总情景和平均报酬设置的一种解释。一旦引入函数逼近，我们就不能保证对任何设置的改进。在第13章中，我们介绍了一种基于参数化策略的可选增强学习算法，在这里我们有一个理论保证，叫做“政策梯度定理”，它与政策改进定理起着类似的作用。但是对于那些学习行动价值的方法，我们目前似乎没有一个当地的改进保证(可能是Perkins和Precup(2003)采取的方法提供了部分答案)。我们知道ε-greedification有时可能会导致劣质政策,政策可能悄悄议论好的政策而不是收敛(戈登,1996)。这是一个具有多个开放理论问题的领域。

\section{差分半梯度n步萨尔萨}

为了推广到n步引导，我们需要一个n步版本的TD错误。我们首先将n阶返回(7.4)推广到它的微分形式，并使用函数近似:

Gt:t + n。= Rt + 1−R̄t Rt + 2−+ 1 + R̄t Rt + n + 2 +···+−R̄t + n + q̂(圣+ n + n,wt + n−1),
(10.14)
̄是一个估计的R(π),n≥1,和t + n < t。如果t + n≥t,那么我们定义Gt:t + n。像往常一样= Gt。然后是n阶TD错误
δt。q = Gt:t + n−̂(圣,在w), 					(10.15)
之后我们可以应用我们通常的半梯度Sarsa更新(10.12)。在方框中给出了完整算法的伪代码。
 

10.9运动微分semi-gradient n-step撒尔沙算法步长参数的平均回报,β,需要非常小所以R̄成为一个良好的长期的平均估计奖励。不幸的是,R̄将偏见的初始值为许多步骤,这可能会使学习效率低下。或者,可以使用一个样本的平均R̄,观察到的报酬。这将在最初迅速适应，但从长远来看也将缓慢适应。随着政策慢慢改变,R̄也会变化;这种长期的非平稳性的可能性使得样本平均方法不适合。实际上，平均奖励上的步长参数是使用练习2.7中无偏恒步长技巧的完美地方。描述用于差分半梯度n阶Sarsa的装箱算法所需的具体变化
技巧。 					?

\section{总结}

在本章中，我们将前一章介绍的参数化函数逼近和半梯度下降的思想扩展到控制。对情节的扩展是直接的，但是对于连续的情况我们必须引入一个全新的问题公式基于最大化每个时间步的平均奖励设置。令人惊讶的是，在近似的情况下，折现公式不能用于控制。在近似情况下，大多数策略不能用一个值函数表示。任意的政策仍然需要排名,和标量平均奖励r(π)提供了一种有效的方法。
平均奖励公式包括新的微分版本的值函数，Bellman方程和TD错误，但所有这些都与旧的相似，概念上的变化很小。对于平均奖励情况，还有一组新的并行微分算法。


\section{书目的和历史的言论}

10.1函数逼近半梯度Sarsa是Rummery首先探索的
和Niranjan(1994)。线性semi-gradient撒尔沙ε-greedy行动选择没有通常意义上的收敛,但进入一个有界区域附近的最佳解决方案(戈登,1996 a,1996)。Precup和Perkins(2003)在可微行为选择的背景下表现出收敛性。另见珀金斯和潘德里斯(2002)、梅洛、米恩和里贝罗(2008)。山地车的例子是基于Moore(1990)研究的一个类似的任务，但是这里使用的确切形式来自Sutton(1996)。
10.2章节n-step semi-gradient撒尔沙是基于向前撒尔沙(λ)算法
的范Seijen(2016)。本文第二版的实证结果是新的。

10.3描述了动态规划的平均报酬公式
(例如，Puterman, 1994)和从强化学习的观点来看(Ma-hadevan, 1996;Tadepalli和好的,1994;Bertsekas Tsitiklis,1996;Tsitsiklis和Van Roy, 1999)。这里描述的算法是Schwartz(1993)引入的“R-learning”算法的策略模拟。R-learning这个名字可能是Q-learning的字母继承，但我们更愿意把它当作学习微分或相对值的参考。的访问控制队列的例子是由工作Carlström和Nordström(1997)。

10.4认识到贴现作为约束的一种形式的局限性
在本文第一版出版后不久，函数逼近的学习问题就显现出来了。Singh、Jaakkola和Jordan(1994)可能是第一个用印刷体来观察它的人。
