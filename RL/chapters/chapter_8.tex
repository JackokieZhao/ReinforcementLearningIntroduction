\chapter{第八章 用表格法进行规划和学习}

\begin{summary}
	在本章中，我们提出了一种统一的强化学习方法，它需要一个环境模型，如动态规划和启发式搜索，以及没有模型的方法，如蒙特卡罗和时间差分方法。这些方法分别称为基于模型和无模型强化学习方法。基于模型的方法依赖于计划作为其主要组件，而无模型的方法主要依赖于学习。虽然这两种方法确实存在差异，但也有很大的相似之处。特别地，这两种方法的核心是价值函数的计算。此外，所有的方法都基于对未来事件的展望，计算一个备份值，然后将其作为一个近似值函数的更新目标。在本书的前面，我们提出了蒙特卡罗和时间差方法作为不同的替代方法，然后展示了如何用n阶方法来统一它们。本章的目标是对基于模型的和无模型的方法进行类似的集成。在前面的章节中，我们已经确定了它们的不同之处，现在我们来探究它们在多大程度上可以混合在一起。	
\end{summary}

\section{模型和规划}

通过环境模型，我们指的是任何代理可以用来预测环境对其行为的反应的东西。给定一个状态和一个动作，一个模型生成下一个状态和下一个奖励的预测。如果模型是随机的，那么接下来会有几个可能的状态和下一个奖励，每一个都有发生的概率。有些模型对所有可能性及其概率进行描述;我们称之为分布模型。其他模型只产生一种可能性，根据概率抽样;我们称之为样本模型。例如，考虑建模一打骰子的和。一个分布模型将产生所有可能的和及其发生的概率，而一个样本模型将产生一个个体

根据这个概率分布得出的和。MDP动态规划估计中假定的一种模型，p(s?， r |s a)是一种分布模型。第5章中的21点示例使用的模型是一个示例模型。分布模型比样本模型更强，因为它们总是可以用来生产样本。然而，在许多应用程序中，获得样本模型比获得分布模型要容易得多。打骰子就是一个简单的例子。编写一个计算机程序来模拟掷骰子的结果并返回和是很容易的，但是要计算所有可能的和及其概率就会越来越困难，也更容易出错。
模型可以用来模拟或模拟体验。给定一个起始状态和动作，一个样本模型将产生一个可能的过渡，一个分布模型将生成所有可能的过渡，并根据它们发生的概率进行加权。给定一个起始状态和策略，示例模型可以生成整个事件，而分布模型可以生成所有可能的事件及其概率。在这两种情况下，我们都说该模型用于模拟环境并产生模拟的经验。
规划这个词在不同的领域有不同的用法。我们用这个词来指代任何以模型作为输入并产生或改进与模型环境交互的策略的计算过程:
规划
模型 					政策

在人工智能中，根据我们的定义，有两种不同的规划方法。国家空间规划，包括我们在本书中采用的方法，主要被看作是在国家空间中寻找最优政策或实现目标的最优路径。动作导致从状态到状态的转换，并且值函数是通过状态来计算的。在我们所说的计划空间规划中，计划是对计划空间的搜索。操作符将一个计划转换为另一个计划，值函数(如果有的话)是在计划空间中定义的。规划空间规划包括进化方法和“部分顺序规划”，这是人工智能中常见的一种规划，在规划的所有阶段中，步骤的顺序并没有完全确定。平面空间方法很难有效地应用于随机序列决策问题，而这些问题是强化学习的重点，我们没有进一步考虑它们(但参见，例如，Russell和Norvig, 2010)。
我们在本章中提出的统一观点是，所有的国家空间规划方法都有一个共同的结构，这一结构也体现在本书的学习方法中。开发这个视图需要本章的其余部分，但是有两个基本思想:(1)所有的状态空间规划方法都将计算值函数作为改进策略的关键中间步骤;(2)通过应用于模拟体验的更新或备份操作来计算值函数。这种共同结构可以如下图所示:

备份模型模拟值
经验 					政策
更新备份


动态编程方法显然适合这种结构:它们使遍历状态空间，为每个状态生成可能的转换的分布。然后使用每个分布来计算备份值(更新目标)并更新
状态的估计价值。在这一章中，我们认为各种其他的国家空间规划方法也适用于这种结构，不同的方法只在更新的种类、更新的顺序以及保存备份信息的时间上有所不同。
以这种方式观看规划方法强调了它们与我们在本书中描述的学习方法的关系。学习和计划方法的核心是通过备份更新操作来估计值函数。不同之处在于，规划使用模型生成的模拟体验，而学习方法使用环境生成的真实体验。当然，这种差异导致了许多其他的差异，例如，如何评估绩效，以及如何灵活地产生经验。但共同的结构意味着许多想法和算法可以在规划和学习之间转换。特别是，在许多情况下，学习算法可以代替计划方法的关键更新步骤。学习方法只需要经验作为输入，在许多情况下，它们既可以用于模拟经验，也可以用于实际经验。下面的框显示了一个简单的规划方法示例，该方法基于一步表q学习和样本模型中的随机样本。这种方法,我们称之为随机样本一步表格Q-planning,收敛于最优政策模型在相同条件下,一步法表格q学习收敛于最优政策的实际环境中(每一对政府行动必须选择在步骤1中无限次的,随着时间的推移和α必须适当减少)。
 

除了规划和学习方法的统一观点之外，本章的第二个主题是在小的、增量的步骤中进行规划的好处。这使得规划在任何时候都可以被中断或重定向，而很少浪费计算，这似乎是有效地将规划与行为和模型的学习混合在一起的关键需求。如果问题太大而无法精确地解决，即使是在纯粹的规划问题上，以非常小的步骤进行规划也可能是最有效的方法。


\section{Dyna:综合规划、表演、学习}

当计划在网上完成时，当与环境交互时，会出现许多有趣的问题。从交互中获得的新信息可能会改变模型，从而与计划交互。可能需要以某种方式将规划过程自定义为当前正在审议或预期的国家或决定

在不久的将来。如果决策和模型学习都是计算密集型的过程，那么可用的计算资源可能需要在它们之间进行划分。为了开始研究这些问题，在本节中，我们将介绍Dyna-Q，一个集成在线规划代理所需的主要功能的简单架构。在Dyna-Q中，每个函数都以简单的形式出现。在后面的小节中，我们将详细介绍实现每个函数的一些替代方法以及它们之间的权衡。现在，我们仅仅是为了阐明这些想法并激发你的直觉。
在规划代理中，实际体验至少有两个角色:可以使用它改进模型(使模型更精确地匹配真实环境)，也可以使用它们直接改进值函数和策略

规划
价值/政策

经验模型

模型的学习
代理
直接RL
我们在前几章已经讨论过的强化学习方法。前者我们称之为模型学习，后者我们称之为直接强化学习(直接RL)。经验、模型、价值和政策之间的可能关系在右边的图表中得到了总结。每个箭头都显示了影响和假定的改进之间的关系。注意经验如何通过模型直接或间接地改进价值功能和策略。后者，有时被称为间接强化学习，涉及到规划。
直接法和间接法各有利弊。间接方法通常会更充分地利用有限的经验，从而在较少的环境交互作用下实现更好的政策。另一方面，直接方法要简单得多，并且不会受到模型设计中的偏差的影响。一些人认为间接方法总是优于直接方法，而另一些人则认为直接方法是人类和动物学习的主要原因。心理学和人工智能的相关辩论涉及认知相对于试错学习的相对重要性，以及商议性规划相对于反应性决策的相对重要性(从心理学的角度讨论这些问题，见第14章)。我们的观点是，在所有这些辩论中，各种选择之间的对比被夸大了，认识到这两方之间的相似之处比反对它们能获得更多的洞察力。例如，在这本书中，我们强调了动态编程和时间性差异方法之间的深层相似性，尽管其中一种方法是为计划而设计的，另一种是为无模型学习而设计的。
Dyna-Q包含了上述图中所示的所有过程，包括计划、行为、模型学习和直接rl——所有这些过程都在不断地发生。规划方法为第161页上的随机样本一步表q规划方法。直接RL方法是一步表q学习。模型学习方法也是基于表格的，并假定环境是确定的。每个转换后圣,→Rt + 1,圣+ 1,模型记录了圣表条目,在Rt + 1的预测,圣+ 1将确定性跟踪。因此，如果用之前经历过的状态-动作对模型进行查询，则只需返回最后观察到的下一个状态和下一个奖励作为预测。

在规划过程中，Q-planning算法只从之前经历过的状态-动作对(步骤1)中随机抽取样本，因此模型不会被没有信息的状态-动作对查询。
Dyna代理的总体架构(其中Dyna- q算法就是一个例子)如图8.1所示。中心柱代表了agent与环境之间的基本交互，产生了真实体验的轨迹。图左边的箭头表示对真实体验的直接强化学习，以改进价值函数和策略。右边是基于模型的流程。该模型从实际经验中学习，并产生模拟经验。我们使用“搜索控制”这个词来指代由模型生成的模拟体验的启动状态和动作的过程。最后，通过将强化学习方法应用到模拟体验中来实现规划，就像它们真的发生了一样。通常，在Dyna-Q中，同样的强化学习方法用于从真实经验中学习，也用于从模拟经验中进行规划。因此，强化学习方法是学习和计划的“最终共同路径”。学习和计划是紧密地结合在一起的，因为它们几乎共享所有相同的机制，只是在经验的来源上有所不同。
 

图8.1:通用Dyna体系结构。真正的经验，来回穿梭。
环境和策略对策略和价值功能的影响与它们的影响非常相似
模拟环境模型产生的经验。

从概念上讲，在Dyna代理中，计划、行为、模型学习和直接RL同时并行地发生。然而，对于串行计算机上的具体和实现，我们完全指定它们在时间步长内发生的顺序。在Dyna-Q中，行为、模型学习和直接RL过程只需要很少的计算，我们假设它们只占用一小部分时间。每个步骤的剩余时间可以用于规划过程，这本质上是计算密集型的。让我们假设在执行、模型学习和指导RL之后，每一步都有时间来完成

Q-planning算法的n次迭代(步骤1-3)。在下面框中Dyna-Q的伪代码算法中，模型(s, a)表示状态-动作对(s, a)的内容(预测下一个状态和奖励)。直接强化学习，模型学习，和计划分别由步骤(d)、(e)和(f)来实现。如果省略(e)和(f)，剩下的算法将是一步式的列表Q-learning。

示例8.1:Dyna Maze考虑一下图8.2中显示的简单迷宫。在每个47个状态中，有4个动作，向上、向下、右和左，这些动作对对应的相邻状态有决定性的作用，除非当移动被障碍物或迷宫的边缘阻挡时，在这种情况下，agent保持在原来的位置。所有转变的奖励都是零，除了那些进入目标状态的，在这个状态下奖励是+1。在达到目标状态(G)之后，代理返回到开始状态，开始新的一集。这是一个折扣,情景任务与γ= 0.95。
图8.2的主要部分显示了将Dyna-Q agent应用于迷宫任务的实验的平均学习曲线。最初的行动值是零,步长参数α= 0.1,勘探参数ε= 0.1。在动作间的贪婪选择中，关系被随机打破。代理人在计划步骤的数量上是不同的，n，他们按实际步骤执行。对于每个n，曲线显示了agent在每一集中达到目标所采取的步骤的数量，平均超过30次的实验重复。在每次重复中，随机数生成器的初始种子在算法之间保持不变。正因为如此，对于所有n的值来说，第一个插曲是完全相同的(大约1700步)，它的数据没有显示在图中。在第一集之后，对所有n值的性能都有了改进，但是对于更大的值来说则更快了。回想一下，n = 0代理是一个非规划代理，只使用直接强化学习(一步表格q学习)。这是迄今为止最慢的代理在这个问题上,尽管参数值(α和ε)进行了优化。无计划代理了约25集(ε)最优性能,而n = 5代理花了五集,和n = 50代理只花了三集。

		G
800年		
	0规划步骤


5计划步骤50	
	(direct RL only)	
600年




400年
每一个步骤
集


200年




14

2 20 10 30


集
165年
 

行动




40 50


图8.2:一个简单的迷宫(inset)和Dyna-Q的平均学习曲线变化。
在他们的计划步骤数量(n)每一个真正的步骤。任务是从S到G
越好。



图8.3显示了为什么计划代理比非计划代理更快地找到解决方案。如图所示，在第二集中，n = 0和n = 50个代理的策略。在没有计划(n = 0)的情况下，每一集都只向策略添加了一个额外的步骤，因此到目前为止，只学习了一个(最后一个)步骤。通过计划，在第一集中只学习了一步，但是在第二集中，已经制定了一项广泛的政策，即在这一集结束时，几乎要回到起始状态。此策略是由计划过程构建的，而代理仍然徘徊在开始状态附近。到第三集结束时，将找到一个完整的最优策略，并实现完美的性能。

没有计划(=0)和计划(=50)n。 					N

								G										G
																		
																		
年代										年代								

图8.3:计划和非计划的Dyna-Q代理在第二集中中途发现的策略。箭头表示每个州的贪婪行为;如果没有为一个状态显示箭头，那么它的所有动作值都是相等的。黑色方块表示代理的位置。

在Dyna-Q中，学习和计划是通过完全相同的算法完成的，在学习的实际经验和计划的模拟经验上操作。因为计划是渐进的，所以把计划和行动结合起来是很简单的。两者都尽可能快地进行。代理总是反应性的，总是深思熟虑的，对最新的感觉信息做出即时反应，但总是在后台进行规划。在后台进行的还有模型学习过程。随着新信息的获取，模型被更新以更好地匹配现实。随着模型的变化，正在进行的规划过程将逐渐计算出与新模型匹配的不同的行为方式。

练习8.1非计划方法在图8.3中看起来特别糟糕，因为它是一步法;采用多步引导的方法可以做得更好。你认为第七章中的一种多步骤引导方法也能做得一样好吗
强啡肽的方法吗?解释为什么或为什么不。 					?


\section{模型错误时}

在前一节中给出的迷宫示例中，模型的变化相对较小。模型一开始是空的，然后只填充正确的信息。总的来说，我们不能指望如此幸运。模型可能是不正确的，因为环境是随机的，并且只有有限的样本被观察到，或者因为模型是用不完美的广义函数逼近来学习的，或者仅仅是因为环境发生了变化，并且它的新行为还没有被观察到。当模型不正确时，规划过程可能会计算一个次优策略。
在某些情况下，通过规划计算的次优策略会导致建模误差的发现和修正。当模型乐观地预测比实际可能的更大的回报或更好的状态转变时，就会出现这种情况。计划中的政策试图利用这些机会，并在此过程中发现它们并不存在。
示例8.2:阻塞迷宫一个迷宫示例演示了这种相对较小的建模错误并从中恢复，如图8.4所示。最初，从开始到目标有一条很短的路径，到屏障的右边，如图左上方所示。经过1000步后，短路径被“阻塞”，较长的路径沿着屏障的左侧被打开，如图右上角所示。图中显示了Dyna-Q代理和增强的Dyna-Q+代理的平均累积报酬。图的第一部分显示了两个Dyna代理在1000步内找到了短路径。当环境发生变化时，图形变平，表明在这段时间内，这些特工因为在障碍物后面游荡而得不到任何报酬。然而，过了一段时间，他们找到了新的突破口和新的最优行为。
当环境变得比以前更好时，就会出现更大的困难，而以前正确的政策并没有显示出改善。在这些情况下，建模错误可能不会被检测很长时间，如果有的话。
	
累积奖励				2000年
年代		
					Dyna-Q	
						
	0	1000年			
时间步长
图8.4:Dyna代理在阻塞任务上的平均性能。前1000步使用的是左环境，其余的是右环境。Dyna-Q+是Dyna-Q，具有鼓励探索的探索奖金。

累积奖励
年代
G 					G

年代

0 3000 3000

时间步长
400年




0
Dyna-Q +
Dyna-Q

图8.5:Dyna试剂的平均性能
一个快捷键的任务。左环境用于
前3000步，剩下的环境。
例8.3:快捷迷宫
图8.5所示的迷宫示例说明了由这种环境变化引起的问题。最开始，最优路径是绕过障碍物的左边(左上)。然而，在3000步之后，沿着右边打开一条较短的路径，而不会干扰较长的路径(右上角)。图表显示，常规的Dyna-Q代理从未切换到快捷方式。事实上，它从未意识到它的存在。它的模型说没有捷径可走，所以计划得越多，越不可能走到右边去发现它。即使ε-greedy政策,很可能一个代理需要很多探索性行为,发现捷径。

这里的一般问题是探索和开发之间的冲突的另一个版本。在规划环境中，探索意味着尝试改进模型的行动，而开发意味着在当前模型下以最佳方式运行。

我们希望代理能够探索环境中的变化，但不能因此而大大降低性能。就像早期的勘探/开采冲突一样，可能没有既完美又实用的解决方案，但简单的启发式往往是有效的。
解决捷径迷宫的Dyna-Q+代理使用了这样的启发式。该代理跟踪每个状态操作对，跟踪自上次在与环境的实际交互中尝试这两个步骤以来的时间间隔。时间过得越久，就越有可能(我们可能会认为)这一对的动力学发生了变化，并且模型是错误的。为了鼓励对长期未尝试过的行为进行测试的行为，在涉及这些行为的模拟体验中会给予一个特殊的“奖励”。特别是,如果过渡的建模奖励是r,和过渡尚未在τ时间步骤,然后做计划更新好像转变生产r +κ的奖励
√
一些小κτ。这鼓励代理保留
测试所有可访问的状态转换，甚至查找长序列动作，以便进行此类测试。当然，所有这些测试都是有代价的，但在许多情况下，就像在快捷迷宫一样，这种计算上的好奇心是值得进行额外探索的。练习8.2为什么有探索奖金的Dyna agent, Dyna- q +在第一阶段以及阻塞和快捷的第二阶段表现得更好
实验吗? 					?
练习8.3仔细检查图8.5，发现Dyna-Q+和Dyna-Q的差异在第一部分实验中略有缩小。的原因是什么
对于这个吗? 					?
练习8.4(编程)上面描述的探索奖金实际上改变了状态和动作的估计值。这是必要的吗?使用假设奖金κ√τ不更新,但是只在行动的选择。也就是说,假设的行动选择的总是问(圣,)+κ

τ(St)是最大的。执行一个
gridworld实验测试并演示了这种方法的优缺点
替代的方法。 					?
练习8.5第164页所示的表格式Dyna-Q算法如何修改以处理随机环境?这个修改如何在不断变化的环境(如本节中所考虑的)上执行得很差?如何修改算法来处理随机环境和变化的环境??

\section{优先考虑全面}

在前几节介绍的Dyna agent中，模拟跃迁是在状态-动作对中启动的，这些状态-动作对是从所有先前经历过的对中随机选择的。但统一的选择通常不是最好的;如果模拟的转换和更新集中在特定的状态-动作对上，那么计划可以更有效。例如,考虑

Dyna-Q+试剂也有另外两种变化。第一，从未有过的行动
在上面框中的表格式Dyna-Q算法的规划步骤(f)中，允许从一个状态进行尝试。第二，这类行为的初始模型是，它们会以零的奖励回到相同的状态。

在第一个迷宫任务的第二集中发生了什么(图8.3)。在第二集中开始时，只有直接进入目标的状态-动作对具有正向价值;所有其他对的值仍然为零。这意味着在几乎所有的转换上执行更新是没有意义的，因为它们将代理从一个零值状态转换为另一个，因此更新没有任何效果。只有在转换到目标之前或之后的状态时进行更新，才会更改任何值。如果模拟的转换是一致生成的，那么在遇到这些有用的转换之前，将进行许多浪费的更新。随着计划的进展，有用的更新区域不断增加，但是如果将重点放在最有用的地方，那么计划的效率仍然远远低于计划。在我们真正的目标——大得多的问题中，状态的数量是如此之多，以至于不集中的搜索将会是极其低效的。
这个例子表明，通过从目标状态向后工作，搜索可以有效地集中精力。当然，我们并不是真的想要使用任何特定于“目标状态”的方法。“我们需要为一般的奖励功能而工作的方法。目标状态只是一种特殊情况，便于激发直觉。一般来说，我们不仅希望从目标状态，还希望从任何价值发生变化的状态返回。假设在给定模型的情况下，这些值最初是正确的，就像在发现目标之前在迷宫示例中所做的那样。现在假设代理发现了环境中的变化并更改了一个状态的估计值(向上或向下)。通常，这意味着许多其他状态的值也应该更改，但是惟一有用的一步更新是那些直接导致值已更改的一个状态的操作。如果这些操作的值被更新，那么前一个状态的值可能会依次变化。如果是这样，那么引入它们的操作需要更新，然后它们的前任状态可能已经改变。通过这种方式，可以从值发生变化的任意状态向后工作，或者执行有用的更新，或者终止传播。这种一般的思想可以称为规划计算的向后聚焦。
随着有用更新的前沿向后传播，它通常会迅速增长，产生许多可以有效更新的状态-动作对。但并非所有这些都同样有用。有些州的价值观可能已经发生了很大的变化，而另一些州则几乎没有变化。在随机环境中，估计的过渡概率的变化也会导致变化大小的变化以及需要更新的紧急程度的变化。根据更新的紧迫性对更新进行优先级排序是很自然的，并且按照优先级顺序执行更新。这就是优先级清理背后的思想。每个状态操作对都维护一个队列，如果更新的话，每个状态操作对的估计值都将不会随更改的大小而变化。当队列中的顶部对被更新时，将计算对其每个前任对的影响。如果效果大于某个小阈值，则将这对组合插入到具有新优先级的队列中(如果队列中有一对之前的条目，那么插入结果只会在队列中保持较高的优先级)。这样，变化的影响被有效地向后传播，直到静止。在下一页的方框中给出了确定性环境的完整算法。

101010101010101010101010106 107

102年

04794 186 376 752 1504 3008 6016。

Dyna-Q

优先考虑全面
更新到
最优解
示例8.4:在迷宫中按优先级排序的清扫被发现可以极大地提高迷宫任务中找到最优解决方案的速度，通常是5到10倍。右边是一个典型的例子。这些数据是用于一系列与图8.2中所示的结构完全相同的迷宫任务，只不过它们在网格分辨率上有所不同。优先级的扫扫保持了相对于没有优先级的Dyna-Q的决定性优势。这两个系统每次环境交互最多更新n = 5次。改编自彭和威廉姆斯(1993)。

将优先级范围扩展到随机环境是很简单的。该模型是通过对每一对国家行动对所经历的次数和下一个州的数量进行统计来维持的。然后，很自然地不使用示例更新来更新每一对，正如我们到目前为止所使用的，而是使用预期的更新，并考虑到所有可能的下一个状态及其发生的可能性。
优先级清理只是一种分配计算以提高规划效率的方法，而且可能不是最好的方法。优先级扫描的局限性之一是它使用预期更新，这在随机环境中可能会在低概率转换上浪费大量的计算。正如我们在下一节中所展示的，示例更新


在许多情况下，尽管抽样引入了方差，但计算量较少，能够更接近真值函数。样本更新可以获胜，因为它们将整体的备份计算分解为更小的部分——对应于单个的转换——从而使它更精确地集中在将产生最大影响的部分上。这一想法在van Seijen和Sutton(2013)引入的“小备份”中达到了逻辑极限。这些是沿着单个转换进行的更新，如示例更新，但是基于没有采样的转换的可能性，如预期的更新。通过选择完成小更新的顺序，可以极大地提高计划效率。
我们在本章中建议，所有类型的状态空间规划都可以看作是价值更新的序列，仅在更新的类型、预期或示例、大或小以及更新的顺序上有所不同。在本节中，我们强调了反向聚焦，但这只是一种策略。例如，另一种办法是根据在现行政策下经常访问的国家能够多容易地联系到这些国家，把重点放在这些国家上，这种政策可以称为前向集中。彭和威廉姆斯(1993)、巴托(Barto)、布莱特克(Bradtke)和辛格(Singh)(1995)研究了前向聚焦的不同版本，接下来几节介绍的方法将其发挥到了极致。

172年 					第8章:用表格法进行规划和学习


\section{预期更新与示例更新}

前几节中的示例给出了结合学习和计划方法的可能性范围的一些概念。在本章的其余部分中，我们将从预期更新和示例更新的相对优势开始分析所涉及的一些组件思想。
这本书的大部分内容都是关于不同类型的价值功能更新，我们已经考虑了很多种类。目前专注于一步更新，它们主要沿着三个二进制维度变化。前两个维度是更新状态值或动作值，还是估计最优策略的值或任意给定策略的值。这两个维度产生的四类更新近似的四个价值函数,问∗∗,qπ,vπ。的

价值估计
预计更新(DP)
示例更新(一步TD)

π
s


s?
π

r p
一个

qπ(年代)

问∗(s)vπ(s)

v∗(年代)
s

年代?r
马克斯
一个

p
政策评估

值迭代

r
s?
年代,

一个?
π
p

q-policy评价

r
s?
年代,

一个?
马克斯
p

核反应能量迭代
s


一个


年代?R




R
年代?年代,




一个吗?




R
年代?年代,



马克斯
TD(0)




撒尔沙




q学习的
一个?


图8.6:所有步骤的备份图
本书考虑的更新。
另一个二进制维度是，考虑到可能发生的所有可能的事件，更新是预期的，还是考虑到可能发生的单个示例的示例更新。这三个二进制维度产生了8种情况，其中7种对应于特定的算法，如图右侧所示。(第八例似乎没有任何有用的更新。)任何这些一步更新都可以用于规划方法。前面讨论的Dyna-Q代理使用q∗样本更新,但他们也用q∗预计更新,或预期或样本qπ更新。Dyna-AC系统一起使用vπ样本更新学习政策结构(如第13章)。对于随机问题，优先级清理总是使用预期的更新之一完成。
当我们在第6章中介绍一步样例更新时，我们将它们作为预期更新的替代品。在没有分发模型的情况下，不可能进行预期的更新，但是可以使用来自环境的示例转换或示例模型来完成示例更新。在这个观点中隐含的是，如果可能的话，预期的更新比样本更新更可取。但是真的是这样吗?预期

\section{预期与样本更新}



更新肯定会产生更好的估计，因为它们不会被采样错误破坏，但是它们也需要更多的计算，并且计算常常是规划中的限制资源。为了正确评估预期更新和样本更新的相对优点，我们必须控制它们的不同计算需求。
具体性,考虑预期和样本更新近似问∗,和离散状态和行为的特殊情况,近似的一览表表示值函数,q,和一个模型估计的形式动态,p̂(s ?， r |s, a).状态-动作对s, a的预期更新为:

问(,)←?
r s ?,
p̂(s ?r | s,)

r +γmax
一个?
Q(s ?,一个?)

。 					(8.1)


对应的s, a的样本更新，给定一个样本下一个状态和奖励，s ?R(来自模型)，为q -learning更新:

Q(,)←Q(s)+α

R +γmax
一个?
Q(S ??)−Q(,)

, 					(8.2)

其中α是平时积极的步长参数。
这些预期更新和样本更新之间的差异非常重要，因为环境是随机的，特别是，在给定状态和操作的情况下，许多可能的下一个状态可能会以不同的概率出现。如果只有一个下一个状态是可能的,然后上面给出的预期和样本更新是相同的(α= 1)。如果有许多可能的下一个状态,然后可能有显著差异。支持预期更新的是，这是一个精确的计算，导致一个新的Q(s, a)，其正确性仅受Q(s)的正确性所限制。在继承国。样本更新还受到抽样误差的影响。另一方面，示例更新在计算上更便宜，因为它只考虑下一个状态，而不是所有可能的下一个状态。在实践中，更新操作所需的计算通常由计算Q的状态-动作对的数量决定。对于一个特定的起始对，s a，设b为分支因子。，可能的下一个状态数，s?,p̂(s ?|s, a) > 0)，然后预期的更新将需要大约b倍的计算量作为样本更新。
如果有足够的时间完成预期的更新，那么由于没有采样错误，结果估计通常比b样例更新要好。但是，如果没有足够的时间来完成预期的更新，那么示例更新总是比较可取的，因为它们至少在少于b的更新的情况下对价值估计做了一些改进。在许多国家行动对的大问题中，我们往往处于后者。由于有如此多的状态-动作对，所有的期望更新都需要很长时间。在此之前，在许多状态操作对中使用一些示例更新可能比在一些状态操作对中使用预期更新要好得多。给定一个计算工作单元，它是用于一些预期的更新，还是用于b倍的示例更新?
图8.7显示了一个分析的结果，它给出了这个问题的答案。它将估计误差作为计算时间的函数，用于各种分支因素的预测和样本更新。

计算最大数量
一个?
Q(s ?,一个?)

图8.7:预期更新和样本更新效率的比较。



b继承国的可能性相等，初始估计的误差为1。假设下一个状态的值是正确的，因此预期的更新在完成时将错误减少到零。在这种情况下，样例更新会根据以下内容减少错误

b−1
其中t是已经执行的示例更新的数量
(假设样本平均值,也就是说。α= 1 / t)。关键的观察是，对于中等大小的b，错误会随着b更新的一小部分而急剧下降。对于这些情况，许多状态操作对的值可以显著提高，达到预期更新效果的几个百分点，与此同时，单个状态操作对可以进行预期更新。
图8.7所示的示例更新的优点可能低估了实际效果。在真正的问题中，继承国的价值将是自己更新的估计数。通过使估计更早地准确，样本更新将具有第二个优势，即继承国所支持的值将更准确。这些结果表明，样本更新很可能优于对具有较大随机分支因子和太多状态无法精确解决的问题的预期更新。
以上的分析假设所有b可能的下一个状态发生的概率是相等的。相反，假设分布是高度倾斜的，一些b状态比大多数更容易发生。比起预期的更新，这是否会加强或削弱示例更新的理由?支持你的答案。?


\section{采样轨迹}

在本节中，我们将比较分发更新的两种方式。来自动态编程的经典方法是对整个状态(或状态操作)空间执行扫描，每次扫描更新每个状态(或状态操作对)。这是有问题的

在大任务上，因为可能没有时间去完成一次扫描。在许多任务中，绝大多数国家都是无关紧要的，因为它们只在非常差的政策或极低的概率下访问。详尽的扫描隐含地将同样的时间用于状态空间的所有部分，而不是集中在需要的地方。正如我们在第4章中所讨论的，详尽的扫掠和它们所暗示的所有状态的平等处理并不是动态规划的必要属性。原则上，更新可以以任何你喜欢的方式发布(为了保证聚合，所有状态或状态-动作对必须以无限次的限制访问;虽然在下面的第8.7节中讨论了这个异常，但是实际上经常使用彻底的扫描。
第二种方法是根据一定的分布从状态或状态作用空间中抽取样本。就像Dyna-Q试剂一样，我们可以均匀地取样，但这也会遇到一些与彻底扫瞄相同的问题。更吸引人的是根据策略分发分发分发更新，即根据跟踪当前策略时观察到的分发。这种分布的一个优点是很容易生成;您只需按照当前策略与模型进行交互。在情景性任务中，一个人从开始状态(或根据开始状态分布)开始并模拟到结束状态。在一个持续的任务中，你可以从任何地方开始，并不断地模拟。无论哪种情况，模型都会给出示例状态转换和奖励，而当前策略给出示例操作。换句话说，一个人模拟显式的个人轨迹，并在沿途遇到的状态或状态对上执行更新。我们称之为产生经验和更新轨迹抽样的方式。
很难想象有什么有效的方式来分配更新，根据政策的分配，除了通过轨道抽样。如果有一个显式的策略分布表示，那么就可以遍历所有状态，根据策略分布对每个状态的更新进行加权，但是这又给我们留下了所有彻底清除的计算成本。也许可以从分布中取样和更新单个状态-动作对，但即使这可以有效地完成，这对模拟轨迹有什么好处呢?甚至不太可能知道显式的政策分配。每当策略发生变化时，分布就会发生变化，计算分布需要与完整的策略评估相当的计算。考虑到其他的可能性，使得轨迹采样看起来既高效又优雅。
更新的策略分发是一个好的吗?直觉上，这似乎是个不错的选择，至少比均匀分布要好。例如，如果你正在学习下象棋，你要学习的是在真正的游戏中可能出现的位置，而不是棋子的随机位置。后者可能是有效的状态，但能够准确地评估它们是一种不同的技能，从评估真实游戏中的位置。我们还将在第二部分中看到，当使用函数逼近时，策略上的分布具有显著的优势。无论是否使用函数逼近，我们都可以期望专注于策略以显著提高计划的速度。
专注于策略分配可能是有益的，因为它会导致大量、无趣的空间部分被忽略，或者可能是有害的，因为它会导致空间中相同的旧部分不断更新。我们进行了一个小

通过实验对效果进行实证评估。为了隔离更新分发的影响，我们使用了完全一步的预期表格更新，如(8.1)所定义。在统一的情况下,我们骑车穿过所有的政府行动对,更新每个到位,我们模拟集在政策的情况下,所有从相同的状态,更新每一对政府行动发生在当前ε-greedy政策(ε= 0.1)。这些任务是不间断的任务，随机生成如下。从|S|的每个状态中，都可能有两个动作，每个动作都会导致b的下一个状态，所有的可能性都是相等的，每个状态-动作对的b状态的随机选择都不一样。分支因子b，对于所有的状态-行为对都是一样的。此外，在所有的转变中，有0.1个转变到最终状态的概率，结束了这个插曲。每个过渡的期望回报是从均值0和方差1的高斯分布中选择的。




b = 10 b = 3
b = 1

b = 1
我on-pol cy

我on-pol cy
统一的

统一的
0 1 2 3


开始状态的值
贪婪的政策

0 5000 10000 15000 20000

计算时间，完全备份。

0 1 2 3

开始状态的值
贪婪的政策

50万，10万，15万，20万

计算时间，完全备份
统一的

统一的
在政策

在政策

预计更新

预计更新

10000个国家


图8.8:在状态空间上均匀分布的相对效率，而不是集中在模拟的on-policy轨迹上。
从相同的状态开始。结果用于随机生成的两个大小和不同分支的任务
因素,b。
在规划过程可以在任何时候停止并详尽计算vπ̃(s0),一开始状态的真正价值在贪婪的政策下,π̃,鉴于当前行为价值函数Q,视为代理会做如何的新一集它是贪婪地(同时假设模型是正确的)。
右图的上部显示了200多个样本任务的平均结果，这些任务具有1000个状态和1、3和10个分支因子。发现的策略的质量被绘制成完成的预期更新数量的函数。在所有情况下，根据政策分布进行抽样，会使最初的规划速度更快，而长期的规划速度较慢。在较小的分支因子下，效果更强，更快规划的初始阶段更长。在其他实验中，我们发现随着状态数的增加，这些效应也变得更强。例如，图的下半部分显示了具有10,000个状态的任务的分支因子1的结果。在这种情况下，专注于政策的优势是巨大而持久的。
所有这些结果都是有道理的。在短期内，根据政策分布进行抽样有助于集中注意与政策分布相近的州


一开始的状态。如果有许多状态和一个小的分支因子，这种影响将是巨大和持久的。从长远来看，关注政策上的分布可能会造成伤害，因为通常出现的状态都已经有了正确的值。对它们进行采样是无用的，而对其他状态进行采样实际上可能执行一些有用的工作。这大概就是为什么详尽、不集中的方法从长期来看效果更好，至少在小问题上是这样。这些结果并不是决定性的,因为他们只在一个特定的生成问题,随机的方式,但他们认为,根据政策分布抽样可以为大问题是一个很大的优势,尤其是对问题的一小部分在政策下的政府行动空间访问分布。
练习8.7图8.8中的一些图形在它们的早期部分似乎是扇形的，特别是b = 1的上图和均匀分布。你认为这是为什么?数据的哪些方面支持你的假设?吗?练习8.8(编程)复制实验结果如图8.8中所示，然后尝试相同的实验，但b = 3。讨论了
你的结果的意义。 					?


\section{实时动态规划}

实时动态规划(RTDP)是动态规划的价值迭代算法(DP)的一种基于策略的轨迹采样算法。因为它与传统的基于缓存的策略迭代密切相关，RTDP以一种特别清晰的方式说明了在策略轨迹抽样中所能提供的一些优势。RTDP通过按(4.10)定义的期望表格值迭代更新来更新实际或模拟轨迹中访问的状态的值。它基本上是生成图8.8所示的策略结果的算法。
RTDP与传统DP的密切联系使得利用已有的理论可以得到一些理论结果。RTDP是第4.5节中描述的异步DP算法的一个示例。异步DP算法不是按照对状态集的系统扫描来组织的;它们以任何顺序更新状态值，使用其他状态的值。在RTDP中，更新顺序由访问真实轨迹或模拟轨迹的顺序状态决定。
开始状态
无关紧要的国家:
在任何最优策略下，从任何起始状态都无法到达

相关的国家
在一些最优策略下从某个起始状态可达
如果轨迹可以只从一组指定的开始,如果你有兴趣在预测问题对于一个给定的政策,然后在政策轨迹采样算法可以完全跳过,不能从任何给定的政策达成的开始状态:这些国家预测问题是无关紧要的。对于控制问题，如果目标是找到最优策略而不是评估给定的策略，那么很可能存在不可能的状态

由任何起始状态的最优策略达成，不需要为这些不相关的状态指定最优操作。我们需要的是一个最优部分策略，即对相关国家最优的策略，但可以为不相关国家指定任意操作，甚至是未定义的策略。
但是要找到这样一个具有on-policy轨迹抽样控制方法的最优部分策略(如Sarsa(第6.4节))，通常需要访问所有的状态-动作- - -即使是那些最终会被证明是无关的- - - -的组合，次数是无限的。这可以通过使用explore begin(第5.3节)来实现。对于RTDP也是如此:对于开始探索的情景任务，RTDP是一种异步的值迭代算法，它收敛于具有折扣的有限MDPs(以及在特定条件下的未折扣情况)的最优策略。与预测问题的情况不同，通常不可能停止更新任何状态或状态-动作对，如果收敛到最优策略是重要的。
RTDP最有趣的结果是，对于满足合理条件的某些类型的问题，RTDP保证在不无限频繁地访问每个国家，甚至根本不访问某些国家的情况下，找到对相关国家最优的政策。事实上，在一些问题中，只有一小部分国家需要访问。对于状态集很大的问题，这是一个很大的优势，在这种情况下，即使是一次扫描也不可行。
这一结果所包含的任务是多边开发银行的不间断任务，目标状态吸引人，产生零回报，如第3.4节所述。在真实轨迹或模拟轨迹的每一步，RTDP都会选择一个贪婪的动作(随机断线)，并将预期值迭代更新操作应用到当前状态。它还可以在每个步骤中更新任意集合的其他状态的值;例如，它可以更新从当前状态开始的有限视界前视搜索中访问的状态的值。
对于这些问题，每一集从开始状态的集合中随机抽取，并以目标状态结束，RTDP与概率为1的策略，对于所有相关的状态都是最优的:1)每个目标状态的初始值是0,2)存在至少一个政策,保证将达到一个目标状态的概率从任何一个开始状态,3)所有奖励过渡件州严格负,和4)所有的初始值都等于或大于其最优值(可满足通过简单设置所有状态的初始值为零)。
具有这些性质的任务是随机最优路径问题的例子，这些问题通常用成本最小化来表示，而不是像我们这里所做的那样，用报酬最大化来表示。在我们的版本中，最大限度地增加负回报等于最小化从起始状态到目标状态的路径成本。这种任务是最短时间控制任务的例子,在每个时间步需要达成的目标产生−1的奖励,或高尔夫3.5节中的示例问题,其目标是用最少的打洞中风。

例8.6:跑道上的RTDP练习的跑道问题5.12(第111页)是一个随机最优路径问题。将RTDP和传统的DP值迭代算法对一个跑道问题进行了比较，说明了基于策略的轨迹抽样的一些优点。
从练习中回想一下，一个代理人必须学会如何像图5.5中所示的那样在转弯时驾驶一辆车，并在赛道上尽快越过终点线。起始状态是起始线上所有的零速度状态;目标状态是所有可以在一个时间内到达的状态，通过在赛道内穿越终点线。与练习5.12不同，这里没有对汽车速度的限制，因此状态集可能是无限的。然而，可以通过任何策略从起始状态集获得的状态集是有限的，可以被认为是问题的状态集。每一集都以随机选择的开始状态开始，当赛车越过终点线时结束。奖励是−1每一步直到车穿过终点线。如果汽车撞到赛道边界，它会被移回一个随机的开始状态，然后情节继续。
类似于图5.5左边的小型赛马场的赛马场，任何策略都可以从起始状态到达9,115个州，其中只有599个州是相关的，这意味着它们可以通过一些最优策略从起始状态到达。(有关国家的数目是通过计算访问的国家，同时执行107集的最佳行动来估计的。)
下表比较了用常规DP和RTDP解决这个问题。这些结果是超过25次的平均值，每一次都以不同的随机数开始。传统DP在本例中是值迭代使用状态集的详尽的清洁工,值更新的一个州,这意味着每个状态的更新使用其他州的最新值(这是高斯-赛德尔迭代版本的价值,这是发现大约两倍雅可比版本在这个问题上。参见4.8节)。没有特别注意更新的顺序;其他排序可能会产生更快的收敛速度。两个方法每次运行的初始值都为0。DP是判断聚合时最大变化状态值扫描不到10−4,和RTDP被判断聚合时的平均时间跨越终点线超过20集似乎在一个渐近稳定的步骤。这个版本的RTDP只更新每个步骤上当前状态的值。


	DP	RTDP
平均计算收敛的平均数量更新收敛平均数量的更新每集%的状态更新≤% 100倍的状态更新≤0 * %的状态更新的10倍	28 252784年清洁工

- - - - - -	4000集127600 31.9 98.45 80.51 3.18
	- - - - - -

这两种方法生成的策略平均需要14到15步才能跨越终点线，但RTDP只需要DP所做的更新的一半。这是RTDP政策轨迹抽样的结果。而每个状态的值都被更新

在DP的每一次扫描中，RTDP都关注较少的状态更新。在平均运行中，RTDP更新了98.45%的州不超过100次，80.51%的州不超过10次;大约290个州的值在平均运行中根本没有更新。
RTDP的另一个优点是,随着价值函数趋于最优值函数v∗,代理生成轨迹方法所使用的政策最优政策,因为它总是贪婪的当前值函数。这与传统价值迭代的情况相反。在实践中，当值函数在一次扫描中仅发生很小的变化时，值迭代就终止了，这就是我们终止值迭代以获得上面表格中的结果的方式。在这一点上,价值函数密切接近v∗,贪婪的政策接近最优的政策。但是，在价值迭代终止之前，对最新值函数贪婪的策略可能是最优的，或者几乎是最优的。(从第四章回忆,最优策略可以贪婪对许多不同的价值函数,不仅v∗)。在值迭代收敛之前检查最优策略的出现并不是传统的DP算法的一部分，需要大量的额外计算。
在赛马场的例子中,通过运行许多测试集每次DP扫描后,用行动选择贪婪地根据扫描的结果,可以估计最早的点的DP计算近似最优评价函数是足够好,这样相应的贪婪的政策几乎是最优的。对于这个racetrack，在15次值迭代或者136,725次值迭代更新之后，会出现一个接近最优策略。这是大大低于252784 DP的更新需要收敛于v∗,但窗台上超过127600年的更新RTDP必需的。
虽然这些仿真当然不是RTDP与传统的基于扫描的值迭代的最终比较，但它们说明了基于策略的轨迹抽样的一些优点。常规的值迭代持续更新所有状态的值，而RTDP则强烈关注与问题目标相关的状态子集。随着学习的继续，这种关注变得越来越狭隘。因为RTDP的收敛定理适用于模拟，我们知道RTDP最终只关注相关的状态，即。，在制定最佳路径的州。RTDP几乎实现了最优控制，其计算量约为基于扫描的值迭代所需计算量的50%。


\section{在决策时进行规划}

计划至少可以用两种方式。我们在这一章中所考虑的，以动态规划和Dyna为代表的一种方法是，利用规划，在从模型(样本或分布模型)获得的模拟经验的基础上，逐步改进策略或价值函数。然后，选择动作就是比较当前状态从表中获得的动作值，我们已经考虑过这种情况，或者用下面第二部分中考虑的近似方法计算数学表达式。在为任何当前的状态St选择一个动作之前，计划在改进表条目或

数学表达式，需要为许多状态选择动作，包括st使用这种方式，规划不关注当前状态。我们称这种方式的规划为背景规划。
另一种使用规划的方法是在遇到每个新的状态St后开始并完成它，作为一个计算，其输出是在;在下一个步骤中，规划从St+1重新开始，生成At+1，以此类推。最简单,几乎退化,这样的例子使用计划是当状态值,并选择一个行动通过比较模型预测未来状态的值为每个行动(或通过比较的值afterstates如井字的例子在第1章),更普遍的是,规划以这种方式使用可以看远比领先一步,评估行动的选择导致不同的预测状态和轨迹的奖励。与规划的第一次使用不同，这里的规划侧重于特定的状态。我们称之为决策时间规划。
这两种思考方式借助模拟经验,逐步完善政策或价值函数,或者使用模拟的经验对当前国家可以选择一个行动自然和有趣的方式混合在一起,但是他们往往是单独研究,这是一个好方法首先了解他们。现在让我们来仔细看看决策时间规划。
即使计划只在决策时完成，我们仍然可以将其视为从模拟体验到更新和值，最终到策略。只是现在的值和策略是特定于当前状态和可用的操作选择，以至于规划过程创建的值和策略通常在用于选择当前操作之后被丢弃。在许多应用程序中，这并不是很大的损失，因为有很多州，而且我们不太可能在很长一段时间内回到同一个州。一般来说，人们可能想要同时做两件事:把计划的重点放在当前的状态上，并将计划的结果存储起来，以便在以后返回到相同的状态时能够走得更远。决策时间规划在不需要快速响应的应用程序中最有用。例如，在下棋程序中，一个棋子可能被允许进行几秒或几分钟的计算，而强程序可能计划在这段时间内进行几十次棋步。另一方面，如果低延迟动作选择是优先级，那么通常情况下，最好在后台进行规划，以计算可以快速应用于每个新遇到的状态的策略。


\section{启发式搜索}

人工智能中经典的状态-空间规划方法是决策-时间规划方法，统称为启发式搜索。在启发式搜索中，对于遇到的每个状态，都考虑一个可能延续的大树。将近似值函数应用于叶节点，然后备份到根的当前状态。搜索树中的备份是在预期的更新一样的高峰(v∗和q∗)在本书中讨论。备份停止在当前状态的状态操作节点处。一旦计算出这些节点的备份值，最好选择它们作为当前操作，然后全部。
备份值被丢弃。
在传统的启发式搜索中，不需要通过改变近似值函数来保存备份值。事实上，价值函数一般是由人设计的，不会因为搜索而改变。然而，很自然地，我们会考虑允许值函数随时间而改进，使用启发式搜索中计算的备份值或本书中介绍的任何其他方法。从某种意义上说，我们一直采用这种方法。我们的贪婪、ε-greedy UCB(2.7节)行为选择方法并不是与启发式搜索不同,尽管规模较小。例如，要计算给定一个模型和一个状态值函数的贪心行为，我们必须从每个可能的行为到每个可能的下一个状态，考虑到奖励和估计值，然后选择最佳的行为。就像传统的启发式搜索一样，这个过程计算可能操作的备份值，但不尝试保存它们。因此，启发式搜索可以被看作是贪婪策略思想的延伸，超越了单个步骤。
比一步更深入的搜索是为了获得更好的操作选择。如果一个人有一个完美的模型和一个不完美的行为价值函数，那么实际上深入的搜索通常会产生更好的策略。2 .当然，如果搜索一直持续到事件结束，那么不完全值函数的影响就被消除了，以这种方式确定的行为必须是最优的。如果足够的搜索深度kγk很小,然后将相应的行动接近最优。另一方面，搜索越深入，需要的计算就越多，通常会导致响应时间变慢。Tesauro的特级西洋双陆棋玩家TD-Gammon(第16.1节)提供了一个很好的例子。本系统利用TD学习通过多款自玩游戏学习后态值函数，采用启发式搜索的形式进行运动。作为一个模型，TD-Gammon使用事先知道掷骰子的概率，并假设对手总是选择TD-Gammon认为最适合的动作。Tesauro发现，启发式搜索越深入，TD-Gammon的移动越好，但每次移动都需要更长的时间。Backgammon有一个很大的分支因素，但必须在几秒钟内移动。只有在前面有选择地搜索几个步骤是可行的，但即使这样，搜索结果也显著地改善了操作选择。
我们不应该忽视启发式搜索关注更新的最明显的方式:当前状态。启发式搜索之所以有效，很大程度上是因为它的搜索树密切关注可能立即跟随当前状态的状态和操作。也许你一生中下棋的时间比西洋跳棋要多，但是当你下西洋跳棋时，考虑你的特定的西洋跳棋位置、你可能的下一个棋以及下一个棋子的位置是值得的。无论您如何选择操作，这些状态和操作是更新的最高优先级，并且您最迫切地希望您的近似值函数是准确的。您的计算不仅应该优先用于即将发生的事件，还应该优先用于有限的内存资源。例如，在国际象棋中，有太多可能的位置来存储每个位置的不同价值估计，但是基于启发式搜索的国际象棋程序可以很容易地存储它们遇到的数百万个位置的不同估计

有一些有趣的例外(见，例如，Pearl, 1984)。

从一个单一的位置向前看。这种对内存和计算资源的极大关注可能是启发式搜索如此有效的原因。
更新的发布可以以类似的方式进行更改，以关注当前状态及其可能的继任者。作为一个极限情况，我们可能会使用启发式搜索的方法来构建一个搜索树，然后执行自底向上的单步更新，如图8.9所示。如果以这种方式对更新进行排序并使用表格表示，那么将实现与深度优先启发式搜索完全相同的整体更新。任何状态空间搜索都可以被看作是将大量单个单步更新拼凑在一起。因此，在深度搜索中观察到的性能改进并不是因为使用了多步更新。相反，这是由于更新的焦点和集中在当前状态下的状态和操作上。通过投入大量与候选动作相关的计算，决策时间规划可以产生比依赖于不集中的更新更好的决策。
 

图8.9:启发式搜索可以实现为一个单步更新序列(在这里用蓝色表示)，从叶子节点备份到根节点的值。这里显示的顺序是用于选择深度优先搜索。




\section{Rollout算法}

Rollout算法是一种基于蒙特卡罗控制的决策时间规划算法，用于模拟轨迹，这些轨迹都是从当前环境状态开始的。他们通过对许多模拟轨迹的平均来估计给定政策的行动值，这些轨迹从每个可能的行动开始，然后遵循给定的政策。当动作价值估计值被认为足够准确时，动作(或其中一个动作)

具有最高估计值的操作将被执行，然后流程将从产生的下一个状态中重新执行。正如Tesauro和Galperin(1997)所解释的，“rollout”一词来自于估计一个backgammon位置的值，即“rollout”。“滚出”指的是在游戏结束前，玩家通过随机生成的掷骰子序列多次移动的位置。
与第五章中描述的蒙特卡罗控制算法,rollout算法的目标是估计一个完整的优化行为价值函数,问∗,或一个完整的行为价值函数,πqπ对于给定的政策。相反，它们只对每个当前状态和一个通常称为rollout策略的给定策略生成行为值的蒙特卡罗估计。作为决策时间规划算法，推出算法立即利用这些行动价值估计，然后丢弃它们。这使得rollout算法实现起来相对简单，因为不需要为每个状态-动作对抽样结果，也不需要在状态空间或状态-动作空间中估计一个函数。
那么，推出算法实现了什么呢?4.2节中描述的政策改进定理告诉我们,给定的任意两个政策ππ?是相同的除了π?(s)= a ? =π(s)对某些国家,如果qπ(,)≥vπ(s),那么政策π?是一样好,或者更好,比π。此外,如果不严格,那么π吗?实际上是比π。这适用于rollout算法是当前状态和π推出政策。模拟轨迹产生估计的平均回报qπ(年代,?)为每个行动?∈(s)。然后选择一个行动的政策年代最大化这些估计,此后π是一个很好的候选人的政策改善了π。结果类似于第4.3节中讨论的动态编程策略迭代算法的一个步骤(尽管它更像是第4.5节中描述的异步值迭代的一个步骤，因为它只改变当前状态的操作)。
换句话说，推出算法的目的是改进推出策略;不是为了找到最优策略。经验表明，推出算法可以非常有效。例如，Tesauro和Galperin(1997)对rollout方法所产生的西洋双陆棋游戏能力的显著提高感到惊讶。在某些应用程序中，即使rollout策略完全是随机的，也可以使用rollout算法获得良好的性能。但是改进策略的性能取决于推出策略的属性和蒙特卡罗值估计产生的操作的排序。直觉告诉我们，推出策略越好，价值估计越准确，由推出算法生成的策略就可能越好(但是参见Gelly和Silver, 2007)。
这涉及到重要的权衡，因为更好的推出策略通常意味着需要更多的时间来模拟足够多的轨迹，以获得良好的价值评估。作为决策时间规划方法，rollout算法通常必须满足严格的时间约束。rollout算法所需的计算时间取决于数量的行动必须被评估为每一个决定,时间步的数量所需要的模拟轨迹获得有用的示例返回,所花费的时间推出政策决策和模拟轨迹的数量需要获得良好的蒙特卡罗行为价值的估计。

在任何rollout方法的应用中，平衡这些因素都很重要，不过有几种方法可以减轻这种挑战。因为蒙特卡罗试验是相互独立的，所以可以在单独的处理器上并行运行许多试验。另一种方法是截断短于完整集的模拟轨迹，通过存储的评估函数纠正截断的返回(这使我们在前面几章中关于截断的返回和更新的所有内容发挥作用)。也有可能,因为Tesauro和加尔佩林(1997)表明,监控蒙特卡罗模拟和修剪掉的候选人的行动不太可能是最好的,或其值接近,当前最好的选择他们将没有真正的区别(尽管Tesauro和加尔佩林指出,这将使一个并行实现)。
我们通常不认为推出算法是学习算法，因为它们不维护对价值或策略的长期记忆。然而，这些算法利用了我们在这本书中强调的强化学习的一些特点。作为蒙特卡罗控制的实例，它们通过对一组样本轨迹的返回进行平均来估计动作值，在这种情况下，模拟与环境样本模型的相互作用轨迹。这样，它们就像增强学习算法一样，避免了通过轨迹抽样对动态规划的彻底扫荡，避免了依赖于样本(而非预期)更新的分布模型的需要。最后，推出算法利用策略改进特性，对估计的操作值执行贪婪操作。


\section{特卡罗树搜索}

蒙特卡罗树搜索(MCTS)是最近一个非常成功的决策时间规划的例子。在它的基础上，MCTS是如上所述的一种推出算法，但是增加了一种方法来积累蒙特卡罗模拟得到的值估计，以便连续地将模拟引向更有价值的轨迹。MCTS在很大程度上促进了计算机的发展，从2005年的业余水平下降到2015年的大师水平(6 dan或更多)。基本算法的许多变体已经被开发出来，包括我们在第16.6节中讨论的一个变体，它对于程序AlphaGo在2016年击败一名18届世界围棋冠军至关重要。未经中华人民共和国交通部已经证明是有效的在各种各样的竞争环境中,包括通用游戏(例如,看到Finnsson Björnsson,2008;Genesereth和Thielscher, 2014)，但不限于游戏;如果环境模型足够简单，可以进行快速的多步仿真，那么它对单代理的连续决策问题是有效的。
MCTS在遇到每个新状态后执行，以选择该状态的代理动作;它再次执行，为下一个状态选择操作，以此类推。与rollout算法一样，每次执行都是一个迭代过程，它模拟许多轨迹，从当前状态开始，运行到终端状态(或者直到折扣使任何进一步的奖励作为回报的贡献变得微不足道)。MCTS的核心思想是连续地将多个模拟从当前状态开始

扩展从早期模拟得到高评价的轨迹的初始部分。MCTS不必保留从一个操作选择到下一个操作选择的近似值函数或策略，但在许多实现中，它保留了可能对下一次执行有用的选定操作值。
在大多数情况下，模拟轨迹中的操作是使用一个简单的策略生成的，通常称为rollout策略，因为它用于更简单的rollout算法。当rollout策略和模型都不需要大量计算时，可以在很短的时间内生成许多模拟轨迹。与任何表列蒙特卡罗方法一样，状态-动作对的值被估计为这对(模拟)返回值的平均值。蒙特卡罗值估计只针对最可能在几个步骤中达到的状态-动作对的子集，它形成了根在当前状态的树，如图8.10所示。基于模拟轨迹的结果，MCTS通过添加表示状态的节点来逐步扩展树。任何模拟的轨迹都会经过这棵树，然后在某个叶子节点上退出。在树和叶子节点上，rollout策略用于操作选择，但是在树内的状态中，更好的情况是可能的。对于这些州，我们至少有一些行动的价值估计，因此我们可以在他们之间使用一种被称为“树政策”的明智政策来平衡探索。

选择 					模拟扩张 					备份
重复而时间仍
 

图8.10:蒙特卡罗树搜索。当环境改变到一个新的状态时，MCTS
在需要增量地选择操作之前，执行尽可能多的迭代
构建一个根节点表示当前状态的树。每个迭代由四个元素组成
操作选择，扩展(虽然可能在某些迭代中跳过)，模拟，
和备份，就像文中解释的那样，用树中的粗体箭头来说明。改编
来自Chaslot, Bakkes, Szita和Spronck(2008)。

与剥削。例如,树策略可以选择操作使用ε-greedy或联合选择规则(第二章)。
更详细地说，MCTS的基本版本的每个迭代包括以下四个步骤，如图8.10所示:

1。选择。从根节点开始，基于附加到树边缘的操作值的树策略将遍历树以选择叶子节点。

2。扩张。在一些迭代中(取决于应用程序的细节)，通过添加一个或多个通过未探索的操作从选定节点到达的子节点，树从选定的叶子节点扩展。

3所示。模拟。从所选的节点，或者从其新增的子节点(如果有的话)中，将运行一个完整的事件的模拟，并使用rollout策略所选择的操作。结果是一个蒙特卡罗试验，首先由树策略选择动作，然后再由树推出策略选择树之外的动作。

4所示。备份。模拟事件生成的返回被备份，以更新或初始化在此MCTS迭代中由树策略遍历的树的边缘附加的操作值。不为树之外的rollout策略访问的状态和操作保存任何值。图8.10显示了从模拟轨迹的终端状态直接到开始执行rollout策略的树中的状态操作节点的备份(尽管通常情况下，整个模拟轨迹的返回都备份到这个状态操作节点)。

MCTS继续执行这四个步骤，每次从树的根节点开始，直到没有剩余的时间，或者其他计算资源耗尽。然后，最后，根据依赖于树中累积的统计信息的某种机制选择根节点(仍然表示环境的当前状态)的操作;例如，它可能是一个操作，其操作值可能是根状态中所有可用操作中最大的，或者可能是访问计数最大的操作，以避免选择离群值。这是MCTS实际选择的动作。在环境转换到新状态之后，MCTS将再次运行，有时从表示新状态的单个根节点的树开始，但通常从包含该节点的任何后代的树开始，这些后代来自于以前执行MCTS时所构建的树;所有剩余的节点以及与它们相关的操作值都将被丢弃。
MCTS最初被提议在播放双人竞技游戏(如围棋)的程序中选择动作。在游戏中，每个模拟的场景都是游戏的一个完整的游戏，在游戏中，两个玩家都选择树的动作和推出的策略。第16.6节描述了AlphaGo程序中使用的MCTS的一个扩展，该扩展将MCTS的蒙特卡罗评估与深度人工神经网络通过自玩强化学习获得的动作值结合起来。
将MCTS与我们在本书中描述的强化学习原理相结合，可以让我们了解它是如何获得如此令人印象深刻的结果的。MCTS是一种基于蒙特卡罗控制的决策时间规划算法


从根状态开始;也就是说，它是前一节描述的一种rollout算法。因此，它受益于在线的、增量的、基于样本的价值评估和策略改进。除此之外，它还保存附加到树边缘的动作值估计，并使用强化学习的示例更新对它们进行更新。它的作用是将蒙特卡罗试验集中在轨迹上，其初始段与先前模拟的高回报轨迹相同。此外，通过逐步扩展树，MCTS有效地扩展查找表来存储部分动作值函数，并将内存分配给在高收益样本轨迹的初始段中访问的状态操作对的估计值。因此，MCTS避免了全局近似行为价值函数的问题，同时保留了利用过去经验指导探索的好处。
MCTS的决策时间规划取得了惊人的成功，对人工智能产生了深远的影响，许多研究人员正在研究对基本程序的修改和扩展，以便在游戏和单智能体应用中使用。


\section{本章小结}

规划需要一个环境模型。分布模型由下一个状态的概率和可能的行为的奖励组成;一个样本模型产生单个转换和根据这些概率产生的奖励。动态编程需要一个分布模型，因为它使用预期更新，这涉及到对所有可能的下一个状态和奖励的计算期望。另一方面，示例模型是模拟与环境交互所需要的，在此过程中，可以使用示例更新，如许多增强学习算法所使用的更新。样本模型通常比分布模型更容易获得。
我们提出了一种观点，强调规划最优行为和学习最优行为之间惊人的密切关系。两者都涉及估计相同的值函数，在这两种情况下，都很自然地在一系列小型备份操作中逐步更新估计。这使得集成学习和规划过程变得非常简单，只需允许两者更新相同的估计值函数。此外，任何学习方法都可以通过将它们应用于模拟(模型生成的)体验而不是实际体验而转化为规划方法。在这种情况下，学习和计划变得更加相似;它们可能是相同的算法，在两个不同的经验来源上运行。
将增量规划方法与行为和模型学习相结合是很简单的。计划、表演和模型学习以循环的方式进行交互(如第162页的图表)，每一个都产生了其他需要改进的东西;它们之间没有其他的相互作用是必需的或禁止的。最自然的方法是让所有进程异步并行地进行。如果进程必须共享计算资源，那么可以几乎任意地处理分割——由任何对当前任务最方便和有效的组织来处理。
在本章中，我们讨论了国家空间规划方法之间的一些变化维度。一个维度是更新大小的变化。的


更新越小，计划方法就可以越增量。在最小的更新中有一步样本更新，就像在Dyna中一样。另一个重要的方面是更新的分布，也就是搜索的重点。优先级的全面扫荡集中在最近改变其价值观的国家的前身。策略上的轨迹抽样集中在代理控制环境时可能遇到的状态或状态对上。这允许计算跳过与预测或控制问题无关的部分状态空间。实时动态编程是一种基于策略的价值迭代采样版本，它展示了该策略相对于传统的基于扫描的策略迭代的一些优势。
规划还可以从相关的状态开始，例如在代理-环境交互过程中实际遇到的状态。最重要的形式是在决策时进行规划，即作为行动选择过程的一部分。在人工智能中研究的经典启发式搜索就是一个例子。其他的例子还有推出算法和蒙特卡罗树搜索，它们得益于在线的、增量的、基于样本的价值估计和策略改进。


\section{第一部分概述:尺寸}

这一章结束了这本书的第一部分。在这篇文章中，我们试图把强化学习描述成一套贯穿方法的连贯的思想。每个想法都可以看作是方法变化的一个维度。这些维度的集合跨越了大量可能的方法。通过在维度层面上探索这个空间，我们希望获得最广泛、最持久的理解。在这一节中，我们使用方法空间中的维数概念来概括本书迄今为止发展起来的强化学习的观点。
到目前为止，我们在本书中探索的所有方法都有三个共同点:第一，它们都寻求估算价值函数;第二，它们都通过沿着实际或可能的状态轨迹备份值来操作;第三，它们都遵循广义策略迭代(GPI)的一般策略，这意味着它们保持了一个近似的值函数和近似的策略，并且它们不断地试图以另一个为基础来改进它们。这三个观点是本书主题的核心。我们认为，价值函数、支持价值更新和GPI是强大的组织原则，可能与任何人工或自然的智能模型相关。
图8.11显示了方法的两个最重要的维度。这些维度与用于改进值函数的更新有关。水平维度是它们是样本更新(基于样本轨迹)还是预期更新(基于可能轨迹的分布)。预期的更新需要一个分布模型，而示例更新只需要一个示例模型，或者可以从没有模型的实际经验中完成(另一个变化维度)。图8.11的垂直维度对应于更新的深度，即自举程度。在这个空间的四个角中有三个是估计值的主要方法:动态规划、TD和蒙特卡罗。沿空间的左边缘是样本更新方法，

 
 

图8.11:通过增强学习方法的空间，突出显示了本书第一部分中探索的两个最重要的维度:更新的深度和宽度。



从单步TD更新到全返回蒙特卡罗更新。这些是一个谱系之间包括方法基于n-step更新(在第12章我们将扩展这个混合物n-step更新如λ-updates实施资格痕迹)。
动态编程方法显示在空间的右上角，因为它们涉及一步预期更新。右下角是预期更新非常深入的极端情况，它们会一直运行到终端状态(或者，在一个持续的任务中，直到折扣将任何进一步的奖励降低到可以忽略的程度)。这是彻底搜索的情况。沿着这个维度的中间方法包括启发式搜索和相关方法，这些方法可以搜索和更新到一个有限的深度，可能是有选择性的。也有一些方法是沿着水平维度的中间的。这些包括混合预期更新和示例更新的方法，以及在单个更新中混合示例和分布的方法的可能性。正方形的内部填充了所有这些中间方法的空间。
我们在本书中强调的第三个方面是政策方法和政策方法之间的二元区别。在前一种情况下，代理将学习它当前所遵循的策略的值函数，而在后一种情况下，它会学习该策略。

书目的和历史的言论 					191



针对不同策略的策略的值函数，通常是代理当前认为最好的策略。由于需要进行探索，策略生成行为通常与当前认为的最佳行为不同。第三个维度可以可视化为垂直于图8.11中页面的平面。
除了刚才讨论的三个维度之外，我们在整本书中还发现了其他一些维度:

回报的定义是任务是间歇性的还是持续的，折现的还是不折现的?动作值、状态值、后状态值应该是什么类型的值
估计?如果只估计状态值，那么行为选择需要一个模型或一个单独的策略(如actor - critics方法)。
行动选择/探索如何选择行动以确保在探索和开发之间进行适当的权衡?我们只考虑最简单的方法:ε-greedy,乐观的初始化值,soft-max,上层的信心。

同步与异步是同时执行的所有状态的更新，还是按某种顺序逐个执行的更新?

一个更新是基于真实体验还是模拟体验?如果两者都有，每一个多少?
更新的位置应该更新哪些状态或状态对?无模型方法只能在实际遇到的状态和状态操作对之间进行选择，但是基于模型的方法可以任意选择。这里有很多可能性。

更新的时间应该作为选择动作的一部分进行更新，还是只在之后进行更新?

更新的内存应该保留多长时间?它们是应该永久保留，还是只在计算操作选择时保留，就像在启发式搜索中那样?

当然，这些维度既不是详尽的，也不是相互排斥的。个别算法在许多其他方面也不同，许多算法在几个维度上的几个位置。例如，Dyna方法使用真实的和模拟的经验来影响相同的值函数。维护以不同方式或不同状态和操作表示方式计算的多个值函数也是完全合理的。然而，这些维度确实构成了一套连贯的概念，用来描述和探索广泛的可能方法。
这里没有提到的最重要的维度，也没有在本书的第一部分中介绍，是函数逼近的维度。函数逼近可以看作是一系列可能性的正交谱，从一个极端的列表方法到状态聚合、各种线性方法，再到一组不同的非线性方法。这个维度在第二部分中进行了探讨。


书目的和历史的言论

8.1在这里提出的规划和学习的总体观点是逐渐发展起来的。
多年来，部分由作者(Sutton, 1990, 1991a, 1991b;Barto, Bradtke, Singh, 1991, 1995;萨顿和Pinette,1985;萨顿和Barto,1981 b);它受到Agre和Chapman(1990)的强烈影响;Agre 1988)， Bertsekas和tsiklis (1989)， Singh(1993)等人。作者还受到潜在学习的心理学研究(Tolman, 1932)和对思想本质的心理学观点(如Galanter和Gerstenhaber, 1956)的强烈影响;Craik,1943;坎贝尔,1960;丹尼特,1978)。在本书的第三部分，第14.6节将基于模型的和无模型的方法与学习和行为的心理学理论联系起来，第15.11节讨论了大脑如何实施这些方法的想法。

8.2直接和间接的术语，我们用来描述不同种类的
强化学习，来自自适应控制文献(例如，Goodwin和Sin, 1984)，他们被用来做同样的区分。“系统识别”一词用于自适应控制中，我们称之为模型学习(例如，Goodwin和Sin, 1984;Ljung所以̈derstrom,1983;年轻,1984)。Dyna体系结构源自Sutton(1990)，本节和下一节的结果基于报告的结果。Barto和Singh(1990)在比较直接和间接强化学习方法时考虑了一些问题。早期的工作扩展强啡肽线性函数近似(第9章)是由Sutton Szepesvári,Geramifard,和保龄球(2008)和帕尔,李泰勒,李曼荣Painter-Wakefield,(2008)。

8.3基于模型的强化学习已经有了一些成果。
探索加值和乐观初始化的思想达到了逻辑的极限，其中所有未完全探索的选择都假定为最大回报，并计算出最优路径来测试它们。Kearns和Singh(2002)的E3算法和Brafman和Tennenholtz(2003)的R-max算法保证在状态数和动作数的时间多项式中找到一个接近最优的解。对于实际的算法来说，这通常太慢，但在最坏的情况下，这可能是最好的。

8.4优先级清扫是由Moore同时独立开发的
还有阿特克森(1993)，彭和威廉姆斯(1993)。第170页的方框中的结果是由Peng和Williams(1993)提出的。第171页方框中的结果是由摩尔和阿特克森得出的。这一领域的主要后续工作包括McMahan和Gordon(2005)以及van Seijen和Sutton(2013)。

8.5这一节受到Singh(1993)实验的强烈影响。

8.6-7轨迹采样是增强学习的一部分
Barto、Bradtke和Singh(1995)在介绍RTDP时，最明确地强调了这一点。他们认识到Korf(1990)的学习

书目的和历史的言论 					193年



实时A* (LRTA*)算法是一种适用于随机问题以及Korf关注的确定性问题的异步DP算法。除了LRTA*之外，RTDP还包括在执行操作之间的时间间隔内更新许多状态值的选项。Barto等人(1995)通过结合Korf(1990)收敛证明LRTA*与Bertsekas (1982) (Bertsekas和Tsitsiklis, 1989)的结果，证明了这里所描述的收敛结果，确保了异步DP在未折现情况下的随机最短路径问题的收敛性。将模型学习与RTDP结合称为适应性RTDP, Barto等人(1995)也提出了这一观点，Barto(2011)也对此进行了讨论。

8.9为了进一步阅读启发式搜索，鼓励读者查阅课文。
罗素和诺维格(2009)和科尔夫(1988)的调查。Peng和Williams(1993)研究了一个前沿的更新焦点，就像本节所建议的那样。

8.10 Abramson(1990)提出的期望结果模型是一种应用于2 -的推出算法
人的游戏，两个模拟玩家的游戏都是随机的。他认为，即使是随机游戏，它也是一种“强大的启发式”，“精确、准确、易于估计、有效计算和领域无关”。Tesauro和Galperin(1997)通过使用不同随机生成的掷骰序列来计算西洋双陆棋的位置，采用了“滚滚出”这个术语，证明了滚出算法在改进西洋双陆棋程序的表现上的有效性。Bertsekas、tsiklis和Wu(1997)研究了应用于组合优化问题的推出算法，Bertsekas(2013)调查了它们在离散确定性优化问题中的应用，并指出它们“通常非常有效”。

8.11 Coulom(2006)和Kocsis介绍了MCTS的核心思想
和Szepesvári(2006)。他们在之前的研究中建立了蒙特卡罗规划算法。Browne, Powley, Whitehouse, Lucas, Cowling, Rohlfshagen, Tavener, Perez, Samothrakis和Colton(2012)是对MCTS方法及其应用的极好的调查。大卫·西尔弗对这一节的观点和介绍做出了贡献。
