\chapter{第十一章  Off-policy方法近似}
\begin{summary}
	
	本书从第5章开始，对政策和非政策的学习方法进行了处理，主要是两种不同的方法来处理在学习形式的广义政策迭代中所固有的开发与探索之间的冲突。前两章用函数逼近的方法来处理保单案例，本章用函数逼近的方法来处理保单案例。对函数逼近的扩展结果表明，与对政策学习的扩展相比，偏离政策学习的扩展结果是显著不同的，也更加困难。在第6章和第7章中开发的表外策略方法很容易扩展到半梯度算法，但是这些算法不像在策略训练中那样强收敛。在本章中，我们探讨了收敛问题，深入研究了线性函数逼近理论，引入了可学习性的概念，然后讨论了具有更强收敛保证的新算法。最后，我们将改进方法，但理论结果不会像政策学习那样强，实证结果也不会像政策学习那样令人满意。在此过程中，我们将进一步加深对策略学习和非政策学习的强化学习的理解。
	回想一下,在off-policy学习我们寻求学习目标政策π值函数,给定数据由于不同行为策略b。在预测的情况下,政策都是静态的,因为,我们寻求学习状态值v̂≈vπ或行动值q̂≈qπ。在控制的情况下,动作值,和这两项政策通常改变learning-π期间被贪婪的政策对q̂,和b被更探索性如ε-greedy政策对q̂。
	非政策学习的挑战可以分为两部分，一个是在表格案例中出现的，另一个是在函数逼近的情况下产生的。挑战的第一部分涉及更新的目标(不要与目标策略混淆)，第二部分涉及更新的分发。第5章和第7章中关于重要性抽样的技术涉及第一部分;这些可能会增加方差，但在所有成功的算法中都需要，
	表格和近似。将这些技术扩展到函数逼近，将在本章的第一节中快速讨论。
	由于在非政策情况下的更新的分布不符合政策的分配，所以在功能近似下的非政策学习的第二个部分需要更多的东西。政策上的分布对半梯度法的稳定性有着重要的影响。已经探讨了两种一般方法来处理这一问题。一是再次使用重要的抽样方法，这一次将更新分布重新分配到on-policy分布，从而保证半梯度方法收敛(在线性情况下)。另一种方法是开发真正的梯度方法，不依赖任何特殊的分布来获得稳定性。我们提出了基于这两种方法的方法。这是一个前沿的研究领域，目前还不清楚哪种方法在实践中最有效。
	
		
\end{summary}

\section{Semi-gradient方法}

我们首先描述在较早的章节中为非策略案例所开发的方法如何容易地扩展为半梯度方法。这些方法解决了脱机策略学习的第一部分(更改更新目标)，而不是第二部分(更改更新分布)。因此，这些方法在某些情况下可能会发散，在这种意义上不是声音，但它们仍然经常被成功地使用。记住，这些方法对于表格情形是稳定且渐进无偏的，它对应于函数逼近的一个特殊情形。因此，仍有可能将它们与特征选择方法结合在一起，从而确保组合系统的稳定性。无论如何，这些方法都很简单，因此是一个很好的起点。
在第7章中，我们描述了各种表外策略算法。将其转换成semi-gradient形式,我们只是替换更新数组(V或Q)更新权重向量(w),使用近似值函数(V̂或Q̂)及其梯度。这些算法中有许多采用了每步重要性抽样比率:
 
例如,一步,州值算法semi-gradient off-policy TD(0),就像相应的政策算法(第203页)除了ρt的添加:

wt + 1
.
= wt +αρtδt∇v̂(St,wt), 					(11.2)
δt在哪里定义适当取决于问题是情景和折扣,或者继续和尚未完全使用平均奖励:

δt。= Rt + 1 +γv̂(wt)圣+ 1−v̂(St,wt),或 					(11.3)


δt。= Rt + 1−R̄t + v̂(wt)圣+ 1−v̂(St,wt)。 					(11.4)

11.1。Semi-gradient方法 					259年



对于动作值，一步算法为半梯度期望Sarsa:

wt + 1
.
q = wt +αδt∇̂(圣,在wt) 					(11.5)

δt。= Rt + 1 +γ

一个
π(|圣+ 1)问̂(圣+ 1,wt)−问̂(圣,在wt)或(情景)


δt。= Rt + 1−R̄t +

一个
π(|圣+ 1)问̂(圣+ 1,wt)−问̂(圣,在wt)。(继续)

注意，该算法不使用重要抽样。在列表的情况下，很明显，这是适当的，因为唯一的示例操作是At，并且在学习它的值时，我们不需要考虑任何其他操作。对于函数逼近，情况就不那么清楚了，因为我们可能想要对不同的状态-动作对施加不同的权重，一旦它们都对相同的整体逼近做出贡献。这个问题的正确解决需要在强化学习中更深入地理解函数逼近理论。
在这些算法的多步推广中，状态值算法和动作值算法都涉及到重要性抽样。例如，半梯度的n阶版本期望Sarsa是。

wt + n
.
=ωt + n−1 +αρt + 1···ρt + n−1(Gt:t + n−问̂(圣,在wt + n−1)]∇问̂(圣,在wt + n−1)
(11.6)
与

Gt:t + n。= Rt + 1 +···+γn−1 Rt + n +γnq̂(圣+ n + n,wt + n−1),或(情景)Gt:t + n。= Rt + 1−R̄t Rt + n +···+−R̄t + n−1 + q̂(圣+ n + n,wt + n−1),(继续)
这里我们对情节结尾的处理有点不正式。在第一个方程中,ρks k≥T(T)是最后一集的时间步)应采取是1,n和Gt:应采取Gt如果T + n≥T。
回想一下，我们也在第七章中提出了一种不涉及重要抽样的非策略算法:n步树备份算法。这里是它的半梯度版本:

wt + n
。=ωt + n−1 +α(Gt:t + n−问̂(圣,在wt + n−1)]∇问̂(圣,在wt + n−1),(11.7)

Gt:t + n。=问̂(圣,在wt−1)+
t + n−1 ?k = t
δk
k ?我= t + 1
γπ(Ai | Si), 					(11.8)

与δt定义为预期的撒尔沙这个页面的顶部。我们也在第七章中定义一个算法,结合所有行为价值算法:n-step Q(σ)。我们将该算法的半梯度形式，以及n步状态值算法，作为读者的练习。
练习11.1将n步偏离策略TD(7.9)的方程转化为半梯度形式。给出情景和持续案例的回报的相关定义。吗?∗11.2运动的方程转换n-step Q(σ)(7.11和7.17)semi-gradient形式。给出涵盖情景和连续情况的定义。?

\section{非政策分歧的例子。}

在这一节中，我们将从函数逼近的角度来讨论离线学习的挑战的第二部分——更新的分布与策略上的分布不匹配。我们描述了一些针对非策略学习的有指导意义的反例——半梯度和其他简单算法是不稳定和发散的。
要建立直觉，最好先考虑一个非常简单的例子。想,也许作为一个更大的MDP的一部分,有两种状态的估算值的函数形式w和2 w,那里只有一个组件的参数向量w由w。这发生在线性函数近似如果两种状态的特征向量都是简单的数字(单组分向量),在这种情况下1和2。在第一个状态中，只有一个动作可用，并且它在向第二个状态的转换中确定的结果为0:
2 w 0 2 w
两个圆圈内的表达式表示两个状态的值。
假设初始w = 10。然后，这个过渡将从估计价值10的状态变为估计价值20的状态。它看起来是一个很好的过渡，w将被增加以提高第一个状态的估计值。如果γ几乎是1,那么TD错误将是近10 w,如果α= 0.1,那么将增加到近11试图减少TD的错误。然而，第二个州的估计价值也将增加到将近22个。如果再次发生转变,那么它将从一个状态估计价值≈≈22日11状态的估计价值的TD错误≈11-larger,不是小,比以前。看起来更像第一个状态被低估,其价值将再次增加,这时间≈12.1。这看起来很糟糕，实际上随着进一步的更新，w将会发散到无穷大。
要明确地看到这一点，我们必须更仔细地观察更新的顺序。在两个状态之间的转换上的TD错误是

δt = Rt + 1 +γv̂(wt)圣+ 1−v̂(St,wt)= 0 +γ2wt−wt =(2γ−1)wt,和off-policy semi-gradient TD(0)更新(从(11.2))
wt + 1 = wt +αρtδt∇v̂(St,wt)= wt +α·1·(2γ−1)wt·1 = ? 1 +α(2γ−1)wt。
注意,重要性抽样比率,ρt,这种转变是1,因为只有一个动作可以从第一个状态,所以它的概率下的被目标和行为的政策必须是1。在最后的更新,新的参数是旧的参数乘以一个标量常数,1 +α(2γ−1)。如果这个常数大于1,则系统不稳定和w将积极或消极的无穷根据其初始值。这个常数大于1时γ> 0.5。注意,稳定性不依赖于具体的步长,只要α> 0。更小或更大的步长会影响w趋于无限大的速率，但不会影响它是否达到无限大的速率。
这个示例的关键是，一个转换重复地发生，而没有在其他转换上更新w。这在政策外的培训中是可能的，因为

行为策略可以选择目标策略永远不会选择的其他转换上的操作。对于这些转换,ρt将是零,没有更新。在政策培训,然而,ρt总是一个。每次有一个从w状态到2w状态的转变，增加w，也会有一个从2w状态的转变。过渡会降低w,除非它是一个国家,其价值高(因为γ< 1)比2 w,然后国家必须紧随其后的更高的价值,否则再次w将会降低。每个状态只能通过创建更高的期望来支持一个状态。终有一天，风笛手必须得到报酬。在按政策执行的情况下，必须遵守未来奖励的承诺，并控制系统。但在非政策的情况下，可以做出承诺，然后在采取目标政策永远不会采取的行动之后，就会忘记和原谅。
这个简单的例子说明了为什么非政策培训会导致分歧，但它并不能完全令人信服，因为它还没有完成——它只是完整的MDP的一部分。真的有一个不稳定的完整系统吗?一个简单而完整的散度例子是Baird的反例。考虑图11.1所示的情景七态双作用MDP。虚线运动使系统有相等的概率到达六种上状态之一，而实线运动使系统达到七种状态。行为策略b选择概率为6 7和的虚线和实线动作
1
因此它下的下一个状态分布是均匀的
对于所有非终端状态也是一样，这也是每一集的开始分布。目标政策π总是需要坚实的行动,所以在政策分布(π)集中在第七状态。所有转变的回报都是零。贴现率是γ= 0.99。
考虑在每个状态圆中表示的线性参数化下估计状态值。例如，最左侧状态的估计值是2w1+w8，其中下标对应于
 

图11.1:Baird的反例。这个马尔可夫的近似状态值函数
过程是每个状态中的线性表达式所显示的形式。坚实的行动通常
结果是第七种状态，而虚线运动通常会导致另外六种状态中的一种，每种状态的概率都是相等的。报酬总是零。

整体权向量w∈R8;这对应于第一个状态为x(1) =(2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,1)的特征向量。奖励在所有转换为零,所以真正价值函数是vπ(s)= 0,所有的年代,可以完全近似如果w = 0。事实上,有许多解决方案,作为权向量有多个组件(8)比非终结符(7)。此外,特征向量的集合,{ x(s)的模式:s∈年代},在所有这些方面都是一个线性无关组。这个任务似乎是一个良好的线性函数近似。
如果我们对这个问题(11.2)应用半梯度TD(0)，那么权值会发散到无穷大，如图11.2(左)所示。不稳定性发生在任何正的步长，无论多小。事实上，如果像dynamic programming (DP)那样进行预期的更新，甚至会发生这种情况，如图11.2所示(右)。也就是说，如果权重向量wk同时以半梯度的方式对所有状态进行更新，使用DP(基于期望)目标:

周+ 1
.
=周+
α|的|

s

Eπ(Rt + 1 +γv̂(圣+ 1周)|圣= s]−v̂(年代,周)

∇v̂(年代,周)。(11.9)

在这种情况下，不存在随机性和异步性，就像经典的DP更新一样。除了使用半梯度函数近似外，该方法是常规的。然而，这个体系仍然不稳定。
如果我们在Baird的反例中仅仅改变DP更新的分布，从均匀分布到策略内分布(通常需要异步更新)，那么可以保证收敛到一个误差为(9.14)的解决方案。这个例子引人注目，因为所使用的TD和DP方法可以说是最简单的
将
One hundred.




10 1

1000 0的步骤
 
将




1000 0扫

图11.2:Baird反例的不稳定性证明。展示了两种半梯度算法参数向量w的分量的演化。步长
α= 0.01,和初始重量w =(1,1,1,1,1,1,1)吗?。

而最容易理解的自举法，以及所使用的线性、半下降法，可以说是最简单、最容易理解的一类函数逼近。这个例子表明，即使是自举和函数逼近的最简单的组合也可能不稳定，如果更新不是按照on-policy的分布进行的。
也有类似Baird的反例显示出Q-learning的发散。这是令人担忧的原因，否则Q-learning具有所有控制方法的最佳收敛性保证。为了找到解决这一问题的办法或得到一些较弱但仍然可行的保证，已经作出了相当大的努力。例如,它可能会保证收敛的q学习只要行为政策是足够接近目标政策,例如,当它是ε-greedy政策。就我们所知，在这种情况下，Q-learning从未被发现有分歧，但还没有理论分析。在本节的其余部分中，我们将介绍一些已经探讨过的其他想法。
假设我们不是像Baird的反例那样，在每次迭代中向预期的单步返回迈出一步，而是将值函数一直更改为最佳的最小二乘逼近。这会解决不稳定问题吗?当然如果特征向量,{ x(s)的模式:s∈年代},形成一个线性无关组,在Baird的反例,因为精确的近似是可能的在每个迭代和表格DP的方法降低标准。但这里的重点当然是考虑当不可能得到精确解的情况。在这种情况下，即使在每次迭代中形成最佳逼近时，稳定性也得不到保证，如示例中所示。
示例11.1:tsiklis和Van Roy的反例本例表明，即使最小二乘，线性函数逼近也不能用于DP


1−ε



ε
w 2 w
每一步都有解决方案。反例是通过扩展wto -2w示例(来自本节前面的部分)来形成的，该示例具有末端状态，如右边所示。如前所述，第一个状态的估计值为w，第二个状态的估计值为2w。所有转变的回报都是零，所以两种状态的真实值都是零，这在w = 0时是完全可以表示的。如果我们在每一步都设置wk+1，以最小化估计值和期望的单步返回之间的VE，那么我们就得到了


周+ 1 = argmin
w∈R

∈年代

v̂(s,w)−Eπ吗?Rt + 1 +γv̂(圣+ 1周)?圣=年代
？2
= argmin
w∈R

w−γ2wk
2 + 2 w ?−1−ε)γ2wk 2

=
6−4ε
5γwk。 					(11.10)
序列{周}发散当γ> 5 6−4ε,w0 = 0。

另一种防止不稳定性的方法是使用函数逼近的特殊方法。特别是，函数逼近方法不从观测目标外推的稳定性得到了保证。这些方法，被称为平均值，包括最近的邻居方法和局部加权回归，但不流行的方法，如tile编码和人工神经网络(ANNs)。
练习11.3(编程)应用一步半梯度q学习到Baird的coun-。
举个例子来说明它的权重是不同的。 					?


\section{致命的三合会}

我们到目前为止的讨论可以总结为，当我们把以下三个要素结合在一起时，就会产生不稳定和分歧的危险，这就是我们所说的致命三位一体:

函数逼近是一种强大的、可扩展的方法，可以从比内存和计算资源(例如，线性函数逼近或ANNs)大得多的状态空间进行归纳。

引导更新目标，包括现有的估计(如动态规划或TD方法)，而不是仅仅依赖于实际的奖励和完整的回报(如MC方法)。

政策外的培训，关于转变的分配，而不是目标政策产生的。遍历状态空间并统一更新所有状态，就像在动态规划中一样，不尊重目标策略，这是场外政策培训的一个例子。

特别要注意的是，危险不是由于控制或泛化策略迭代造成的。这些情况分析起来比较复杂，但在更简单的预测情况中，只要包含致命三元组的所有三个元素，就会出现不稳定性。危险也不是由于学习或环境的不确定性，因为它在规划方法(例如动态规划)中同样发生，在动态规划中，环境是完全已知的。
如果死亡三位一体中的任何两个元素存在，但不是全部，那么就可以避免不稳定。因此，很自然地，通过这三个，看看是否有任何一个可以被放弃。
在这三种方法中，函数逼近显然是不能放弃的。我们需要的方法可以扩展到大问题，并具有强大的表达能力。我们至少需要具有许多特征和参数的线性函数近似。状态聚合或非参数方法，其复杂性随着数据的增长而增长，这些方法要么太弱，要么太昂贵。像LSTD这样的最小二乘方法具有二次复杂度，因此对于大型问题来说代价太高。
以计算和数据效率为代价，不进行引导是可能的。也许最重要的是计算效率的损失。蒙特卡罗(非bootstrapping)方法需要内存来保存生成过程中发生的所有事情。

每一个预测和得到最终的回报，所有的计算都是在得到最终的回报之后完成的。这些计算问题的成本在冯·诺依曼系列计算机上并不明显，但将在专用硬件上体现出来。使用bootstrapping和资格跟踪(第12章)，数据可以在何时、何地生成，然后再也不需要使用。通过bootstrapping实现的通信和内存节省是很大的。
放弃自举操作在数据效率上的损失也很严重。我们在第七章(图7.2)和第九章(图9.2)中多次看到过这种情况，在这些章节中，在随机行走预测任务中，某种程度的引导比蒙特卡罗方法表现得更好，在第十章中，在登山车控制任务中也看到了这种情况(图10.4)。许多其他的问题显示了自举学习的速度快得多(例如，参见图12.14)。自举通常会导致更快的学习，因为它允许学习利用状态属性，即返回状态时识别状态的能力。另一方面,引导可以削弱学习问题的状态表示很差,导致可怜的泛化(例如,这似乎是在俄罗斯方块,看到̧imşek,藻类́等,和Kothiyal,2016)。糟糕的状态表示也会导致偏见;这就是自举法渐近逼近质量较差的原因(公式9.14)。总的来说，引导的能力必须被认为是极有价值的。一个有时可能会选择不使用它通过选择长n-step更新(或大型引导参数,λ≈1;参见第12章)，但是引导通常会极大地提高效率。这是我们非常希望保留在我们的工具箱中的一种能力。
最后，还有政策外的学习;我们能放弃吗?政策上的方法通常是足够的。对于无模型强化学习，可以简单地使用Sarsa而不是Q-learning。策略外方法将行为从目标策略中释放出来。这可以被认为是一种吸引人的便利，但不是必要的。但是，脱机学习对于其他预期的用例是必不可少的，这些用例我们在本书中还没有提到，但是对于创建强大的智能代理的更大目标可能是重要的。
在这些用例中，代理不仅学习一个值函数和一个策略，还同时学习大量的值函数和策略。有大量的心理学证据表明人类和动物学会预测许多不同的感觉事件，而不仅仅是奖励。我们会对不寻常的事件感到惊讶，并纠正我们对它们的预测，即使它们是中性价(既不好也不坏)。这种预测可能是世界的预测模型的基础，如在规划中使用的模型。我们预测我们的眼睛运动后会看到什么，走路回家需要多长时间，篮球跳投的可能性，以及接受一个新项目的满足感。在所有这些情况下，我们想要预测的事件取决于我们的行为方式。要学习它们，就需要从经验中学习。有许多目标策略，因此一个行为策略不能等同于所有的策略。然而，并行学习在概念上是可能的，因为行为策略可能部分地与许多目标策略重叠。要充分利用这一点，需要学习政策以外的知识。

\section{线性值函数几何}

为了更好地理解非策略学习的稳定性挑战，更抽象地、独立地思考价值函数逼近，而不是学习是如何完成的，这是很有帮助的。我们可以想象所有可能的状态值的空间从各州实数v函数功能:S→r .这些价值函数并不对应任何政策。对于我们的目的来说，更重要的是，大多数参数不能用函数逼近器来表示，而函数逼近器的参数要比有状态的参数少得多。
给定状态空间S = {s1, s2，…， s| s|}，任何值函数v都对应一个向量，该向量按照v(s1)、v(s2)、…v(s | |)]?。这个值函数的向量表示具有状态的分量。在大多数情况下，我们想要使用函数逼近，这将是太多的分量来显式地表示向量。然而，这个向量的概念在概念上是有用的。在下面，我们将一个值函数和它的向量表示互换。
为了发展直觉，考虑三个状态的情况，分别为:{s1, s2, s3}和两个参数w = (w1, w2)。然后我们可以把所有的值函数/向量看作三维空间中的点。这些参数在二维子空间上提供了一个可选的坐标系。有权向量w = (w1, w2)吗?是二维子空间中的一个点，因此也是一个完整的值函数vw，它将值赋给所有三个状态。利用一般函数逼近，可表示函数的全空间与子空间之间的关系可以是复杂的，但在线性值函数逼近的情况下，子空间是一个简单的平面，如图11.3所示。
现在考虑一个固定政策π。我们假设它真正的价值函数,vπ,太复杂就像一个近似表示。因此vπ不是子空间;在图中，它被描述为在可表示函数的平面子空间之上。
如果vπ不能代表完全,可表示的值函数是最接近吗?这是一个有多个答案的微妙问题。首先，我们需要测量两个值函数之间的距离。给定两个v1和v2价值函数,我们可以讨论向量之间的区别,v = v1−v2。如果v很小，那么这两个值函数是很接近的。但是我们如何测量这个差向量的大小呢?传统的欧几里得准则是不合适的，因为正如第9.2节所讨论的，有些国家比其他国家更重要，因为它们发生得更频繁，或者因为我们对它们更感兴趣(第9.11节)。在9.2节中,让我们使用μ分布:S→[0,1]来指定我们关心的程度不同的州是准确值(通常采取政策分布)。然后我们可以用范数定义值函数之间的距离

v ?2μ。=

∈年代
μv(s)2。 					(11.11)

注意,已经从9.2节可以写简单地使用这个标准已经(w)= ?大众−vπ吗?2μ。对于任何值函数v，在可表示值函数的子空间中寻找最接近的值函数的操作是一个投影运算。我们定义了一个

图11.3:线性值函数逼近的几何形式。如图所示是三
所有值函数在三种状态下的维数空间，而平面表示的是所有值函数的子空间，由参数w = (w1, w2)的线性函数逼近器表示。真正价值函数vπ是在更大的空间,可以预计(子空间,
利用投影算符Π)的最佳逼近值错误(VE)意义。在Bellman错误(BE)、投射Bellman错误(PBE)和时态中最好的近似器
差分误差(TDE)的感觉都可能是不同的，显示在右下角。(VE, BE，和PBE都被视为这个图中对应的向量。)更夫操作符
取平面内的值函数到平面外的值函数，然后将其投影回来。如果迭代地将Bellman运算符应用到空间之外(如上面的gray所示)，就会达到真正的值函数，就像传统的动态编程那样。如果你保持
在每个步骤中向后投影到子空间中，就像在灰色显示的较低的步骤中，然后是固定的
点是向量零PBE的点。



投影算符Π,需要一个任意值函数是最规范的可表示的函数:

Πv。= vw = argmin
w∈Rd
v−大众?2μ。 					(11.12)
能上演的价值函数,因此是最接近真值函数vπ投影,Πvπ,显示如图11.3所示。这是蒙特卡罗方法渐近发现的解，虽然通常很慢。投影操作将在下一页的框中更详细地讨论。
TD方法找到不同的解决方案。理解他们的理由,回想一下,贝尔曼方程函数vπ价值

vπ=

一个
π(|)?
r s ?,
p(s ?,r | s)[r +γvπ(?)],所有年代∈s(11.13)

真正价值函数vπ是唯一的价值函数,解决了(11.13)。如果一个近似值函数代替vπ大众,左右两侧的区别修改方程可以作为衡量如何从vπ远离大众。我们称之为s州的Bellman错误:


δ̄w(s)。=
⎛⎝吗?
一个
π(|)?
r s ?,
p(s ?,r | s)[r +γvw(s ?)
⎞⎠
−大众(s)(11.17)

= Eπ

Rt + 1 +γvw(圣+ 1)−大众(St)
吗?圣= s,∼π

(11.18)

它清楚地显示了Bellman错误与TD错误(11.3)的关系。Bellman错误是TD错误的期望。
向量的更夫错误,在所有国家,δ̄w∈R | |,叫做传达员误差向量(如图所示,在图11.3)。这个向量的总体大小，在范数中，是值函数误差的总体度量，称为均值平方Bellman误差:

(w)= ??δ̄w ?2μ?。 					(11.19)
一般是不可能减少是零(此时大众= vπ),但对于线性函数近似是一个独特的有价值的w是最小化。这一点在子空间可表示的函数(图11.3)中的标记minBE不同一般,这样可以最大限度减少Πvπ(如图所示)。在接下来的两部分中，我们将讨论一些方法来最小化这些问题。
更夫误差矢量图11.3所示的结果应用传达员运营商Bπ:R S | |→R S | |近似值函数。更夫操作符是

定义为

(Bπv)(s)。=

一个
π(|)?
r s ?,
p(s ?,r | s)[r +γv(?)], 					(11.20)

所有∈年代和v:s→r . v的更夫误差向量可以写δ̄w = Bπvw−大众。如果Bellman算子作用于可表示子空间中的值函数，
然后，一般来说，它将产生一个新的值函数，该函数位于子空间之外，如图所示。在动态规划(没有函数逼近)中，这个算子被反复地应用到可表示空间之外的点上，如图11.3顶部的灰色箭头所示。最终这一过程收敛于真值函数vπ,唯一不动点的更夫运营商唯一值函数

vπ= Bπvπ, 					(11.21)
这是另一种写法是π的贝尔曼方程(11.13)。
然而，对于函数逼近，位于子空间之外的中间值函数是无法表示的。图11.3上部的灰色箭头不能跟随，因为在第一次更新(黑线)之后，值函数必须被投影到可表示的东西上。下一个迭代从子空间开始;值函数再次被Bellman算子提取到子空间之外，然后被投影算子映射回来，如下面的灰色箭头和直线所示。跟随这些箭头的是一个具有近似的dp过程。
在这种情况下，我们感兴趣的是Bellman错误向量的投影回到可表示空间。这是预计传达员误差向量Πδ̄大众,PBE如图11.3所示。这个向量的大小，在范数中，是近似值函数误差的另一个度量。对于任何近似值函数v，我们定义均方投影贝尔曼误差，记为PBE

PBE(w)= ?w ?Πδ̄
吗?？2
μ。 					(11.22)
对于线性函数逼近，总是存在一个具有零PBE的近似值函数(在子空间内);这是TD固定点，wTD，在第9.4节介绍。正如我们所看到的，在半梯度TD方法和非政策训练下，这一点并不总是稳定的。如图所示，这个值函数通常与最小化VE或BE的值函数不同。保证收敛的方法将在第11.7和11.8节中讨论。


在Bellman错误中有11.5梯度下降。

在更好地理解价值函数逼近及其各种目标的基础上，我们现在回到了离线学习中稳定性的挑战。我们想应用随机梯度下降的方法(SGD，第9.3节)，其中更新了期望等于目标的负梯度


函数。这些方法在目标中总是会下降(在期望中)，因为它具有典型的稳定性，具有很好的收敛性。在本书所研究的算法中，只有蒙特卡罗方法是真正的SGD方法。这些方法在政策和非政策训练以及一般的非线性(可微)函数逼近器下都是鲁棒收敛的，尽管它们通常比自举的半梯度方法慢，而非SGD方法。半梯度方法在偏离策略的训练下可能会有分歧，正如我们在本章前面看到的，在设计的非线性函数逼近的情况下也会有分歧(tsiklis和Van Roy, 1997)。用真正的SGD方法，这种发散是不可能的。
SGD的吸引力是如此之强，以至于巨大的努力已经开始寻找一种实用的方法来利用它来加强学习。所有这些努力的出发点都是选择一个要优化的错误或目标函数。在本节和下一节中，我们将探讨基于前一节中引入的Bellman错误的最流行的目标函数的起源和限制。尽管这是一种流行的、有影响力的方法，但我们得出的结论是，这是一个错误的步骤，没有好的学习算法。另一方面，这种方法以一种有趣的方式失败了，它提供了对什么可能构成一个好的方法的洞察。
首先，让我们不要考虑Bellman错误，而是考虑更直接、更天真的问题。时间差异学习是由TD误差驱动的。为什么不把TD误差的期望平方的最小化作为目标呢?在一般的函数逼近情况下，带有折扣的单步TD误差为

δt = Rt + 1 +γv̂(wt)圣+ 1−v̂(St,wt)。

那么，一个可能的目标函数就是所谓的平均平方TD误差:TDE(w) =?
∈年代
μ(s)E ?δ2 t ?圣= s,∼π=

∈年代
μ(s)E ?ρtδ2 t ?圣= s,∼b = Eb

ρtδ2 t

。(如果μb)下遇到的分布

最后一个方程是SGD所需的形式;它将目标作为可以从经验中抽取的期望(请记住，经验是由于行为策略b)。因此，遵循标准的SGD方法，可以根据该期望值的一个样本得出每步更新:


wt + 1 =−那样
1 2
α∇(ρtδ2 t)
=ω−αρtδt∇δt = wt +αρtδt

∇v̂(圣wt)−γ∇v̂(wt)圣+ 1, 					(11.23)
除了附加的最后一项之外，你会发现它和半梯度TD算法(11.2)是一样的。这一项完成了梯度，使它成为一个真正的SGD算法，具有极好的收敛保证。我们把这个算法叫做朴素算法

残差梯度算法(Baird之后，1995)。虽然原始的残差梯度算法是鲁棒收敛的，但它不一定收敛到理想的位置。
 

在A-split示例中使用表格表示，因此可以精确地表示真实的状态值，但是原始的residual-gradient算法会找到不同的值，并且这些值的TDE低于真实值。最小化TDE是幼稚的;通过惩罚所有TD错误，它实现了时间平滑而不是准确的预测。
更好的办法似乎是最小化Bellman错误。如果知道了确切的值，那么Bellman错误在任何地方都是零。因此，bellman -error最小化算法对于a -split示例应该没有问题。我们不能期望得到一般的零Bellman错误，因为它涉及到找到真值函数，我们假定它在可表示值函数的空间之外。但接近这个理想是一个看似自然的目标。正如我们所看到的，Bellman错误也与TD错误密切相关。一个状态的Bellman错误是该状态的预期TD错误。让我们重复前面的推导，得到期望的TD误差(这里所有的期望都隐式地以St为条件):


wt + 1 =−那样
1 2
α∇(Eπ(δt)2)

=ω−
1 2
α∇(Eb(ρtδt)2)
=ω−αEb[ρtδt]∇Eb[ρtδt]=ω−αEb

ρt(Rt + 1 +γv̂(w)圣+ 1−v̂(St,w))eb(ρt∇δt]
= wt +α

海尔哥哥

ρt(Rt + 1 +γv̂(w)圣+ 1)−v̂(St,w)

∇v̂(圣,w)−γEb ?ρt∇v̂(圣+ 1 w)


这种更新和各种采样方法被称为残差梯度算法。如果您只是在所有期望中使用了样本值，那么上面的方程几乎完全可以简化为(11.23)，即原始的residual-gradient算法。但这很天真，因为上面的方程包含了下一个状态St+1，出现在两个期望中，然后相乘。要获得产品的无偏样本，需要两个下一个状态的独立样本，但在与外部环境的正常交互过程中，只获得一个。一个期望或另一个期望可以被抽样，但不能同时抽样。
有两种方法可以使residual-gradient算法工作。一个是确定性环境。如果到下一个状态的转换是确定的，那么两个样本必然是相同的，并且朴素算法是有效的。另一种方法是从St中获得下一个状态的两个独立样本St+1，一个是第一个期望，另一个是第二个期望。在与环境的实际交互中，这似乎是不可能的，但是当与模拟环境交互时，这是可能的。一个简单地回滚到以前的状态，然后在从下一个状态继续前进之前获得另一个状态。在这两种情况下，都保证了在通常的步长参数条件下，残差梯度算法收敛到最小值。作为一种真正的SGD方法，这种收敛是

对于状态值，在处理重要性抽样比时，仍然存在很小的差异。
ρt。在analagous操作-值情况(这是控制算法最重要的情况)中
残差梯度算法可以精确地简化为原始算法。


鲁棒性，适用于线性和非线性函数逼近器。在线性情况下，收敛总是对唯一的w最小。
但是，残差梯度法的收敛至少有三种方法是不能令人满意的。第一个是，从经验上讲，它比半梯度法慢得多。实际上，这种方法的支持者们已经提出通过将它与更快的半梯度方法相结合来提高它的速度，然后逐渐切换到残差梯度以保证收敛(Baird和Moore, 1999)。第二种不满意残差梯度算法的方法是它似乎仍然收敛于错误的值。它在所有的表格情况下都得到了正确的值，例如A-split示例，对于这些Bellman的精确解来说也是如此


方程是可能的。但是如果我们用真实的函数逼近来检验例子，那么残差-梯度算法，实际上是客观的，似乎找到了错误的值函数。最能说明问题的例子之一是在A-split示例上的变化，该示例称为A-presplit示例，如前一页所示，其中residual-gradient算法找到与原始版本相同的糟糕解决方案。这个例子直观地表明最小化BE (residual-gradient算法肯定会这么做)可能不是一个理想的目标。
第三部分解释了残差梯度算法收敛性不佳的原因。就像第二种方法一样，第三种方法也是一种客观的问题，而不是用任何特定的算法来实现它。


\section{Bellman错误是不可学的}

我们在这一节介绍的学习能力的概念与机器学习中常用的概念不同。在这里，假设是“可学的”，如果它是有效的可学的，这意味着它可以在一个多项式中学习，而不是一个指数的例子。在这里，我们用一种更基本的方式来使用这个术语，指的是在任何情况下都可以学习。事实证明，强化学习中许多明显的兴趣是无法从大量的经验数据中习得的。这些量定义得很好，可以根据环境内部结构进行计算，但不能根据所观察到的特征向量、行为和奖励序列进行计算或估计。我们说他们是学不会的。最后两节介绍的Bellman错误目标(BE)在这种意义上是无法学习的。从可观测数据中无法得知Bellman错误目标，这可能是不去寻找它的最大原因。
为了让学习的概念变得清晰，让我们从一些简单的例子开始。考虑下面的两个Markov奖励过程3 (MRPs):
 
当两条边离开一个状态时，假设两个跃迁发生的概率都是相等的，这些数字表示所收到的奖励。所有的状态看起来都一样;它们都产生相同的单分量特征向量x = 1，并且都近似于w。因此，数据轨迹中唯一变化的部分就是奖励序列。左边的MRP保持相同的状态，随机释放出无穷无尽的0和2，每一个都有0.5的概率。正确的MRP，在每一步上，要么停留在它的当前状态，要么。

如果观察到状态序列，而不是只观察到状态序列，它们当然会被估计
相应的特征向量。
所有mrp都可以被认为是MDPs，在所有的状态下都有一个动作;我们对MRPs的结论同样适用于MDPs。

切换到另一个，概率相等。奖励是确定性的MRP,总是从一个状态0和2的,但是因为每个州同样可能在每一步,可观测的数据又是无穷无尽的0和2 s随机,与由左MRP。(我们可以假设正确的MRP以两种状态之一随机启动，概率相等。)因此，即使给定无限的数据，也不可能知道这两个mrp中是哪一个生成的。特别是，我们无法判断MRP是否有一两个状态，是随机的还是确定性的。这些东西是学不会的。
这一对mrp还说明VE目标(9.1)是不可学习的。如果γ= 0,那么的真实值三个州(在两个可机读护照),从左到右,是1 0,2。假设w = 1。那么左MRP为0，右MRP为1。由于VE在这两个问题中是不同的，但是生成的数据具有相同的分布，因此无法了解VE。VE不是数据分布的唯一函数。如果它是学不到的，那么“VE”怎么可能成为学习的目标呢?
如果一个目标不能被学习，它确实会把它的效用变成问题。然而，在VE的情况下，有一条出路。注意,相同的解决方案,w = 1,都是最优的可机读护照上面(假设μ是相同的两个州中区分正确的MRP)。这是巧合吗?还是说所有数据分布相同的MDPs都有相同的最优参数向量?如果这是真的——我们接下来将证明它是真的——那么VE仍然是一个可用的目标。VE不是可学的，但是优化它的参数是!
要理解这一点，引入另一个自然目标函数是很有用的，这一次是一个明显可以学习的函数。一个总是可以观察到的错误是，在每个时间的值估计值和那个时间的收益之间。均方返回错误,表示再保险公司的期望,在μ,广场上的错误。在on-policy的情况下，可以写入RE
 
因此，除了不依赖于参数向量的方差项之外，这两个目标是相同的。两个目标必须具有相同的最优参数值w∗。总体关系在图11.4的左侧进行了总结。
∗锻炼11.4证明(11.24)。提示:写再保险作为期望的可能状态年代期望平方误差的考虑到圣= s。然后加减的真正价值的年代错误(平方之前),分组的真实价值减去回来估计价值增加的真正价值。然后，如果你展开平方，最复杂的项最后会是0，剩下(11.24)?
现在让我们回到现实。BE就像VE，它可以从MDP的知识中计算出来，但不能从数据中学习。但它不像VE，它的最小解是不可学的。下一页的方框给出了一个反例——两个mrp生成相同的数据分布，但其最小参数向量不同，证明了最优参数向量不是

图11.4:数据分布、MDPs和各种目标之间的因果关系。左，蒙特卡罗目标:两个不同的MDPs可以产生相同的数据分布
然而，也产生了不同的VEs，证明VE的目标无法从数据中确定
并不是学得来的。然而,所有这些类型必须具有相同的最优参数向量,w∗!
此外,同样的w∗可以从另一个目标,确定再保险,是独一无二的
由数据分布确定。因此w∗类型和再保险可学的虽然不是。正确，引导目标:两个不同的MDPs可以生成相同的数据
分布也产生不同的微分方程，且具有不同的最小参数向量;这些在数据分布中是无法学习的。PBE和TDE目标及其(不同的)最小值可以直接从数据中确定，因此是可以学习的。




因此不能从数据中学习。我们考虑的另一个引导目标，PBE和TDE，可以从数据(是可学习的)中确定，并确定通常不同的最优解和最小值。一般情况总结在图11.4的右边。
因此，BE是不可学习的;它不能从特征向量和其他可观测数据来估计。这将BE限制为基于模型的设置。没有一种算法可以在不访问特征向量之外的底层MDP状态的情况下最小化be。residual-gradient算法只能最小化BE，因为它允许从相同状态重复采样——不是具有相同特征向量的状态，而是保证为相同底层状态的状态。我们现在可以看到，这是不可能的。最小化BE需要访问名义上的、底层的MDP。这是在第273页上的A-presplit示例中所标识的范围之外的一个重要限制。所有这些都将更多的注意力转向PBE。

\section{Gradient-TD方法}

我们现在考虑SGD方法来最小化PBE。作为真正的SGD方法，这些梯度- td方法在非策略训练和非线性函数逼近下具有鲁棒收敛性。记住，在线性情况下总会有一个精确的解，TD不动点wTD，在这个点PBE为0。这个解可以通过最小二乘方法(第9.8节)找到，但只能通过参数数的二次复杂度O(d2)的方法找到。我们转而寻找一种SGD方法，它应该是O(d)并且具有鲁棒收敛性。梯度- td方法接近于实现这些目标，代价是计算复杂度的粗略增加一倍。
为了得到PBE的SGD方法(假设线性函数逼近)，我们首先将目标(11.22)展开并重写为矩阵形式:
 
=δ̄吗?wΠ吗?DΠδ̄w =δ̄吗?wDX

X ? DX

−1 x ? Dδ̄w 					(11.25)
(使用(11.14)和身份Π?X DΠ= DX ? ?DX−1 X ? D)
=

X ? Dδ̄w
X X ? ? ? DX−1 ? ? Dδ̄w。 					(11.26)
关于w的梯度是。

∇PBE(w)= 2∇? X ?Dδ̄w X ? DX−1 ? X ? Dδ̄w。
要将它转化为SGD方法，我们必须在每一个时间步上对具有这个量作为期望值的东西进行采样。让我们以μ的分布状态下访问行为的政策。以上三个因素都可以用这个分布下的期望来表示。例如，最后一个因素可以写出来

X ?Dδ̄w =

s
μ(s)x(s)δ̄w(s)= E[ρtδtxt],

也就是半梯度TD(0)更新(11.2)的期望。第一个因素是这个更新的梯度的转置:

∇E(ρtδtxt)?= E ?ρt∇δ?t x ?t
= E

ρt∇(Rt + 1 +γw吗?xt + 1−w ? xt)? x ?t
(使用情景δt)
= E

xρt(γxt + 1−xt)?t

.




最后，中间因子是特征向量的期望外积矩阵的逆:

X ?DX =

s
μ(s)用于?s = E

xtx

t


把这些期望代入我们表达式中的三个因子，得到PBE的梯度
∇PBE(w)= 2 e ?xρt(γxt + 1−xt)?t E xtx ?t−1 E[ρtδtxt]。(11.27)用这种形式来写渐变，可能并没有明显的进展。它是三个表达式的乘积，第一个和最后一个不是独立的。它们都依赖于下一个特征向量xt+1;我们不能简单地对这两个期望进行抽样，然后将样本相乘。这将给我们一个有偏见的梯度估计，就像在原始的残差梯度算法。
另一种方法是分别估计这三个期望，然后把它们结合起来，得出对梯度的无偏估计。这将工作,但需要大量的计算资源,特别是存储前两个预期,d×d矩阵,并计算的倒数第二。这个想法可以改进。如果对三个期望中的两个进行估计和存储，那么第三个期望就可以与两个存储的量一起采样和使用。例如，您可以存储第二两个量的估计值(使用9.8节中的增量反更新技术)，然后对第一个表达式进行示例。不幸的是，总体算法仍然是二次复杂度(O(d2))。
将一些估计值单独存储，然后将它们与示例组合在一起的想法是一个很好的想法，并且也用于Gradient-TD方法。Gradient-TD方法在(11.27)中估计和存储第二个因子的乘积。这些因素是d×d矩阵和一维矢量,所以他们的产品只是一个维向量,像w本身。我们用v表示第二个学习向量:

E v≈

xtx

t

−1 E[ρtδtxt]。 					(11.28)
这种形式对线性监督学习的学生很熟悉。线性最小二乘问题的解决方案,试图近似ρtδt的特性。标准的SGD方法，用于增量地寻找向量v，从而最小化预期的平方。
错误

v ?xt−ρtδt
2被称为最小均方(LMS)规则(此处增广)
具有重要抽样比率):

vt + 1
。= vt +βρt

δt−v ?t xt

xt,

其中β> 0是另一个步长参数。我们可以使用这种方法，通过O(d)存储和每步计算有效地实现(11.28)。
给定一个已存储的估计vt近似(11.28)，我们可以使用基于(11.27)的SGD方法更新我们的主要参数向量wt。最简单的规则是

wt + 1 =−那样
1 2
α∇PBE(wt) 					(一般SGD规则)

=ω−
1 2
α2E

xρt(γxt + 1−xt)?t

和

xtx

t

−1 E[ρtδtxt](从(11.27))
= wt +αE

xρt(xt−γxt + 1)?t

和

xtx

t

−1 E(ρtδtxt)(11.29)
≈wt +αE

xρt(xt−γxt + 1)?t

vt 					(基于(11.28))
≈wt +αρt(xt−γxt + 1)x ?t vt。 					(抽样)

280年 					第11章:带有近似的政策外方法


这个算法叫做GTD2。注意，如果最终的内积(x?首先完成t (vt)，然后整个算法的复杂度为O(d)。
在替换vt之前，再做一些分析步骤，就可以得到稍微更好的算法。

wt + 1 = wt +αE

xρt(xt−γxt + 1)?t

和

xtx

t

−1 E[ρtδtxt]
= wt +α

和

ρtxtx吗?t

−γE

ρtxt x + 1 ?t

和

xtx

t

−1 E[ρtδtxt]
= wt +α

和

xtx

t

−γE

ρtxt x + 1 ?t

和

xtx

t

−1 E[ρtδtxt]
= wt +α

E[xtρtδt]−γE ?ρtxt x + 1 ?t E xtx ?t−1 E[ρtδtxt]

≈wt +α

E[xtρtδt]−γE ?ρtxt x + 1 ?t vt(基于(11.28))
≈wt +αρt

δtxt−γxt x + 1 ?t vt

, 					(抽样)

也就是O(d)如果最终产物是x?t (vt)是先做的。该算法被称为带有梯度修正(TDC)的TD(0)或作为GTD(0)。
图11.5显示了一个示例和Baird反例上TDC的预期行为。如预期的那样，PBE降至零，但请注意参数向量的各个分量不会接近零。事实上，这些价值还远远没有达到
 
5




2




0




-2.34


0 					1000步
 
0 					1000年清洁工

图11.5:TDC算法对Baird反例的行为。左边的是
显示典型的单次运行，右边显示的是该算法的预期行为
更新是同步完成的(类似于(11.9)，除了两个TDC参数向量)。步大小α= 0.005,β= 0.05。

一个最优解,v̂(s)= 0,所有的年代,而w必须成正比(1,1,1,1,1,1,4,−2)吗?。在1000次迭代之后，我们仍然远离一个最优解，正如我们从VE中看到的，它仍然几乎是2。系统实际上正在收敛到一个最优的解决方案，但进展极其缓慢，因为PBE已经如此接近于零。
GTD2和TDC都涉及两个学习过程，一个是w的初级学习过程，一个是v的二级学习过程。初级学习过程的逻辑依赖于二级学习过程的完成，至少是近似的，而二级学习过程则不受第一级学习的影响。我们称这种不对称依赖为级联。在叶栅中，我们经常假设二次学习过程进行得更快，因此总是在其渐近值，随时准备和准确地帮助初级学习过程。这些方法的收敛证明经常明确地做出这种假设。这些被称为双时间尺度证明。快速时间尺度是中等学习过程的时间尺度，较慢的时间尺度是初级学习过程的时间尺度。如果α的步长是主要学习过程,和β的步长是次要的学习过程,那么这些收敛性证明通常会要求在极限情况下β0和α→β→0。
梯度- td方法目前是最能被理解和广泛使用的稳定的非政策方法。有行动的扩展值和控制(《GQ》,Maei et al .,2010),合格的痕迹(GTD(λ)和《GQ》(λ)Maei,2011;和非线性函数逼近(Maei等，2009)。也有人提出了介于半梯度TD和梯度TD之间的混合算法(Hackman, 2012;白色,白色,2016)。混合- td算法在目标和行为策略非常不同的状态下表现得像梯度- td算法，在目标和行为策略相同的状态下表现得像半梯度算法。最后，将Gradient-TD idea与proximal method和control variates相结合，生成更高效的方法(Mahadevan et al.， 2014;杜et al .,2017)。


\section{Emphatic-TD方法}

现在我们转向第二种主要策略，它已经被广泛地探索，以获得一种具有函数逼近的廉价而有效的脱机学习方法。回想一下,线性semi-gradient TD方法是有效和稳定的训练在政策下分布时,我们在9.4节,这与积极的矩阵A的明确性(9.11)4和政策之间的匹配状态分布μπ和状态转换概率p(|年代,)目标的政策。在off-policy学习中，我们使用重要性抽样对状态转换进行重新加权，以便它们适合于学习目标策略，但是状态分布仍然是行为策略的分布。有一个不匹配。一个自然的想法是，以某种方式重新调整状态，强调某些状态，而不强调其他状态，以便将更新的分布返回到相应的策略分布。届时将会有一场比赛，而现有的结果将会带来稳定和融合。这就是。

4 off-policy情况下,矩阵通常定义为Es∼b

x(s)E ? x(圣+ 1)?吗?圣= s,∼π? ?。

着重介绍了在9.11事件中进行政策培训的方法。
实际上，“政策上的分配”的概念并不十分正确，因为有许多政策上的分配，其中任何一个都足以保证稳定。考虑一个不打折扣的情景问题。情节结束的方式完全取决于过渡的可能性，但是情节开始的方式可能有几种不同。然而，如果所有的状态转换都是由于目标策略而导致的，那么结果的状态分布就是一个on-policy分布。在结束之前，您可能会开始接近终点站状态，并且只访问少数几个有高概率的状态。或者你可能从很远的地方出发，在结束之前经过许多州。两者都是政策上的分布，用线性半梯度法对两者进行训练将保证是稳定的。无论过程如何启动，只要遇到的所有状态更新到终止，就会产生策略上的分发结果。
如果有折现，可以将其视为部分或概率终止。如果γ= 0.9,那么我们可以考虑用概率0.1进程终止在每个时间步,然后立即重新启动的状态转变。贴现的问题是不断地终止和重启的概率1−γ在每一步。这种考虑折现的方法是一个更普遍的伪终止的例子，它不影响状态转换的序列，但确实影响学习过程和学习的量。这种伪终止对于脱机学习很重要，因为重新启动是可选的——记住我们可以以任何方式启动——而终止免除了将遇到的状态包含在策略分布中的需要。也就是说，如果我们不把新州视为重新开始，那么贴现很快就会给我们一个有限的政策分配。
一步Emphatic-TD算法学习情景状态值被定义为:δt = Rt + 1 +γv̂(wt)圣+ 1−v̂(St,wt),

wt + 1 = wt +αMtρtδt∇v̂(St,wt),
太=γρt−1太−1 +,
,利息,是任意的,太强调,被初始化为太−1 = 0。这个算法对Baird的反例如何执行?图11.6显示了参数向量分量的期望轨迹(对于所有t都等于1的情况)，有一些振荡，但最终所有的都收敛，VE趋于0。这些轨迹是通过迭代计算参数向量轨迹的期望而得到的，而不是由于转换和奖励的抽样而产生的任何方差。由于该算法对Baird反例的方差很大，在计算实验中几乎不可能得到一致的结果，所以我们没有直接给出应用该算法的结果。该算法在理论上收敛于最优解，但在实际应用中并不收敛。下一节我们将讨论如何减少所有这些算法的方差。

11.9。减少方差 					283年
 

图11.6:对Baird的反例的期望的单步强化- td算法的行为。步长是α= 0.03。



11.9减少方差

政策外学习本质上比政策内学习具有更大的差异。这并不奇怪;如果您接收到的数据与策略的关系不太密切，那么您应该对策略的价值了解得更少。在极端情况下，一个人可能什么也学不到。例如，你不能指望通过做饭来学习开车。只有当目标和行为政策是相关的，如果他们访问相似的国家并采取类似的行动，一个人才能在政策之外的培训中取得显著的进展。
另一方面，任何政策都有许多邻国，许多相似的政策在访问过的国家和选择的行动中有相当大的重叠，但它们并不相同。雷森d本部̂off-policy学习是使概括的混乱关系大量related-but-not-identical政策。问题仍然是如何充分利用这段经历。既然我们已经有了一些在期望值上是稳定的方法(如果步骤大小设置正确的话)，那么注意力自然就会转向减少估计的方差。有很多可能的想法，我们可以在这篇介绍性文章中讨论其中的一些。
为什么在基于重要性抽样的非策略方法中控制方差特别重要?正如我们所看到的，重要性抽样通常涉及政策比率的产品。比率总是期望为1(5.13)，但它们的实际值可能非常高或低至0。连续比率是不相关的，所以它们的产品在期望值上也是一样的，但是它们的方差可能非常大。回想一下，在SGD方法中，这些比率将步长相乘，因此高方差意味着采取在大小上差异很大的步骤。这对SGD是有问题的，因为偶尔会有非常大的步骤。它们不能大到把参数带到空间的一个有非常不同的梯度的部分。SGD方法依赖于对多个步骤进行平均，以获得对梯度的良好感觉，如果它们从单个样本中进行较大的移动，就会变得不可靠。

如果步长参数设置得足够小以防止这种情况发生，那么预期的步骤可能会非常小，从而导致非常慢的学习。动量的概念(Derthick, 1984)， Polyak- ruppert平均值(Polyak, 1990;Ruppert,1988;Polyak和Juditsky, 1992)，或者进一步扩展这些思想可能会有很大的帮助。为参数向量的不同分量自适应地设置独立步长的方法也是相关的(例如，Jacobs, 1988;Sutton, 1992b, c)，以及Karampatziakis和Langford(2010)的“重要性体重意识”更新。
在第5章中，我们看到了加权重要性抽样比普通重要性抽样更能表现得更好，方差更新更低。然而，将加权重要性抽样应用于函数逼近是一种挑战，而且可能只能用O(d)复杂度完成(Mahmood和Sutton, 2015)。
树备份算法(第7.5节)表明，在不使用重要抽样的情况下，可以执行一些策略外的学习。这一想法已经扩展到off-policy案例，由Munos、Stepleton、Harutyunyan和Bellemare(2016)以及Mahmood、Yu和Sutton(2017)提出稳定和更有效的方法。
另一种补充策略是，允许目标策略在一定程度上由行为策略决定，其方式是，它与行为策略之间永远不会有如此大的差异，以创建重要的抽样比率。例如，目标策略可以通过引用行为策略来定义，如Precup等人(2006)提出的“识别器”。




\section{总结}

离线学习是一个很有诱惑力的挑战，考验我们在设计稳定有效的学习算法方面的独创性。表列Q-learning使偏离策略的学习看起来很容易，它对预期Sarsa和树备份算法有自然的归纳。但是正如我们在这一章中看到的，将这些思想推广到重要的函数逼近，甚至是线性函数逼近，会带来新的挑战，迫使我们加深对强化学习算法的理解。
为什么要走这么长的路?寻求偏离策略的算法的一个原因是在处理探索和开发之间的权衡时提供灵活性。另一种方法是将行为从学习中解放出来，避免目标政策的专制。TD学习似乎提供了同时学习多个事物的可能性，即使用一种经验流同时解决多个任务。我们当然可以在特殊情况下这样做，只是不是在我们想要或想要的任何情况下。
在本章中，我们将政策外学习的挑战分为两部分。第一部分，纠正行为策略的学习目标，直接处理使用前面为表格案例设计的技术，尽管其代价是增加更新的方差，从而减缓学习。对于偏离策略的学习来说，高方差可能始终是一个挑战。
脱机学习的挑战的第二部分是半梯度TD方法的不稳定性，包括自举。我们寻求强大的功能

近似值，非政策学习，以及bootstrapping TD方法的效率和灵活性，但在一种算法中结合这三种致命的三元算法，并没有引入不稳定性的可能性，这是一种挑战。有几次尝试。最流行的方法是在Bellman错误(即Bellman残余物)中寻求执行真正的随机梯度下降(SGD)。然而，我们的分析得出的结论是，在很多情况下，这并不是一个吸引人的目标，而且无论如何，用学习算法是不可能实现的。另一种方法，Gradient-TD方法，在预计Bellman错误中执行SGD。在复杂度为O(d)的情况下，PBE的梯度是可以学习的，但是代价是需要第二个参数向量，并且需要第二个步骤大小。最新的方法系列，强调- td方法，改进了一个旧的更新权重，强调了一些，而不强调其他的。通过这种方式，它们恢复了使用计算简单的半梯度方法使策略学习稳定的特殊属性。
非政策学习的整个领域相对较新，也不稳定。哪些方法是最好的，甚至是适当的，目前尚不清楚。本章末尾介绍的新方法的复杂性真的有必要吗?其中哪一种可以有效地与方差约简方法结合?偏离政策学习的可能性仍然诱人，实现它的最佳方式仍然是个谜。


\section{书目的和历史的言论}

11.1第一个semi-gradient方法是线性TD(λ)(萨顿,1988)。这个名字
“半梯度”是最近才出现的(Sutton, 2015a)。在Sutton、Mahmood和White(2016)之前，一般输入-采样率的半梯度非政策TD(0)可能没有明确的说明，但是Precup、Sutton和Singh(2000)引入了动作价值形式，他们也采用了这些算法的合格跟踪形式(见第12章)。他们持续的、未打折的形式还没有得到充分的探索。这里给出的n步形式是新的。

11.2最早的wto -2w示例是Tsitsiklis和Van Roy(1996)提出的
还在263页的方框中介绍了具体的反例。Baird的反例来自Baird(1995)，尽管我们在这里展示的版本稍作修改。函数近似的平均方法是由Gordon (1995, 1996b)开发的。其他的不稳定性的例子，还有非政策的DP方法和更复杂的函数近似方法，由Boyan和Moore(1995)给出。Bradtke(1993)给出了一个在线性二次调节问题中使用线性函数逼近的Q-learning收敛于不稳定策略的例子。

11.3死亡三位一体首先由Sutton(1995年b)鉴定并彻底分析
作者Tsitsiklis和Van Roy(1997)。“致命三合会”这个名字是由Sutton (2015a)起的。
11.4这种线性分析是tsitsitsiklis和Van Roy(1996)首创的;1997年),

包括动态编程操作符。像图11.3这样的图表是由Lagoudakis和Parr(2003)介绍的。
我们称为传达员操作符,表示Bπ,更常见的Tπ来表示,称为“动态规划算子,而广义的形式,表示T(λ),被称为“TD(λ)操作符”(Tsitsiklis和Van罗伊,1996,1997)。

11.5 BE首先作为动态规划的目标函数被提出
施韦策和Seidmann(1985)。Baird(1995, 1999)将其扩展为基于随机梯度下降的TD学习。在文献中，BE最小化常被称为Bellman剩余最小化。
最早的a分裂的例子是Dayan(1992)。这里给出的两种形式是Sutton等人(2009a)介绍的。

11.6本节的内容对本文来说是全新的。

11.7 Gradient-TD方法引入Sutton Szepesvári,和Maei(2009 b)。
本节重点介绍的方法由Sutton等人(2009a)和Mahmood等人(2014)介绍。由Mahadeval等人(2014)开发了近端TD方法的一个主要扩展。到目前为止，Gradient-TD及其相关方法最敏感的实证研究由Geist和Scherrer(2014)、Dann、Neumann和Peters(2014)、White(2015)和Ghiassian、White、White、White和Sutton(准备中)进行。Yu(2017)介绍了梯度- td方法理论的最新进展。

11.8 Sutton、Mahmood和White(2016)着重介绍了c- td方法。
Yu(2015年)建立了完整的收敛证明和其他理论;2016;Yu, Mahmood, and Sutton, 2017)， Hallak, Tamar, Mannor(2015)，以及Hallak, Tamar, Munos, Mannor(2016)。
