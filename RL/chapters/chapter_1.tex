
\chapter{第一章 引言}
\begin{summary}
	当我们思考学习的本质时，我们首先想到的可能就是通过与环境的互动来学习。当婴儿玩耍、挥动手臂或四处张望时，它没有明确的老师，但它确实与环境有直接的感觉运动联系。这种联系产生了大量关于因果关系、行为后果以及为了实现目标该做什么的信息。在我们的一生中，这样的互动无疑是了解我们的环境和我们自己的主要来源。无论我们是在学习开车还是在交谈，我们都清楚地意识到我们的环境是如何回应我们的行为的，我们试图通过我们的行为来影响我们的行为。从互动中学习是几乎所有学习和智力理论的基础。
	在这本书中，我们探索了一种从交互中学习的计算方法。我们主要探讨的是理想化的学习情况，并评估各种学习方法的有效性，而不是直接理论化人们或动物是如何学习的。也就是说，我们采用人工智能研究者或工程师的视角。我们通过数学分析或计算实验来评估设计，来探索能够有效解决科学或经济问题的机器的设计。我们所探索的方法，叫做强化学习，比机器学习的其他方法更专注于目标导向的交互学习。
\end{summary}

\section{强化学习}

强化学习是学习做什么——如何将情况映射到行动——从而最大化一个数字奖励信号。学习者并没有被告知要采取哪些行动，而是必须通过尝试来发现哪些行为可以获得最大的回报。在最有趣和最具挑战性的情况下，行动可能不仅影响眼前的事情

心理学和神经科学的关系在第14章和第15章总结。

奖励，但也包括下一个情境，通过这个，所有后续的奖励。这两个特征——试错搜索和延迟奖励——是强化学习的两个最重要的显著特征。
强化学习，就像许多以“ing”结尾的主题一样，比如机器学习和登山，同时也是一个问题，是一种很好的解决问题的方法，也是研究这个问题及其解决方法的领域。对于这三件事使用一个名字是很方便的，但同时也必须保持这三件事在概念上是分开的。特别是在强化学习中，问题与解决方法的区别是非常重要的;没有区分这一点是许多困惑的根源。
我们利用动力系统理论中的思想，具体地说，将强化学习问题形式化为不完全已知马尔可夫决策过程的最优控制。这种形式化的细节必须等到第3章，但是基本的思想仅仅是捕获学习代理与它的环境在一段时间内交互的真实问题的最重要的方面来实现目标。学习代理必须能够在一定程度上感知其环境的状态，并且必须能够采取影响状态的行动。代理还必须有一个与环境状态相关的目标或目标。马尔可夫决策过程旨在将这三个方面——感觉、行动和目标——以它们最简单的可能形式包含进来，而不忽略它们中的任何一个。任何适合解决此类问题的方法我们都认为是一种强化学习方法。
强化学习不同于监督学习，监督学习是目前机器学习领域中研究最多的一种学习。监督学习是指从一组由有知识的外部主管提供的标记示例中学习。每个示例都是对一种情况的描述，以及系统对这种情况应采取的正确行动的说明(标记)，这通常是为了确定该情况所属的类别。这种学习的目的是让系统对其反应进行外推或归纳，使其在训练集中不存在的情况下能够正确地发挥作用，这是一种重要的学习方式，但单独从交互中学习是不够的。在交互问题中，获取期望行为的实例通常是不现实的，这些行为既正确又能代表代理必须采取的所有情况。在未知领域——人们期望学习成为最有利的领域——一个代理人必须能够从自己的经验中学习。
强化学习也不同于机器学习研究者所称的无监督学习，无监督学习通常是指寻找隐藏在无标记数据集合中的结构。“监督学习”和“无监督学习”这两个术语似乎会对机器学习范式进行彻底的分类，但它们并非如此。虽然人们可能会认为强化学习是一种无监督学习，因为它不依赖于正确行为的例子，强化学习是试图最大化奖励信号而不是试图寻找隐藏的结构。在agent的经验中发现结构在强化学习中肯定是有用的，但是它本身并不能解决激励信号最大化的强化学习问题。因此，我们认为强化学习是第三种机器学习范式，除了监督学习和非监督学习，也许还有其他范式。

强化学习的挑战之一，而不是其他类型的学习，是探索和开发之间的权衡。为了获得大量的奖励，强化学习代理必须更喜欢它过去尝试过并且发现在产生奖励方面有效的行为。但是要发现这样的行为，它必须尝试以前没有选择过的行为。为了获得奖励，行为人必须挖掘自己已经经历过的东西，但为了在未来做出更好的行动选择，行为人也必须进行探索。现在的困境是，无论是勘探还是开发都不能在不失败的情况下进行。代理必须尝试各种各样的操作，并逐渐偏爱那些看起来最好的。在随机任务中，每个动作都必须反复尝试，以获得对预期回报的可靠估计。摘要探索-开发困境已被数学家研究了几十年，但仍未得到解决。目前，我们只是注意到，在监督和非监督学习中，平衡勘探和开发的整个问题甚至没有出现，至少在这些范例的最纯粹的形式中。
强化学习的另一个关键特性是，它明确地考虑了目标导向的代理与不确定环境交互的整个问题。这与许多考虑子问题的方法不同，这些方法没有解决如何将子问题融入更大的范围。例如，我们已经提到，许多机器学习研究涉及监督学习，但没有明确说明这种能力最终如何有用。其他研究人员已经开发出了具有一般目标的规划理论，但不考虑计划在实时决策中的作用，也不考虑规划所需要的预测模型从何而来的问题。虽然这些方法产生了许多有用的结果，但是它们对孤立子问题的关注是一个很大的限制。
强化学习采取了相反的策略，从一个完整的、交互式的、目标寻求的代理开始。所有增强学习代理都有明确的目标，能够感知环境的各个方面，并且可以选择影响环境的行为。此外，通常从一开始就假定代理必须操作，尽管它面临的环境存在很大的不确定性。当强化学习涉及到规划时，它必须解决规划和实时行动选择之间的相互作用，以及如何获得和改进环境模型的问题。当强化学习涉及到监督学习时，它这样做是因为特定的原因，这些原因决定了哪些能力是关键的，哪些不是。为了学习研究取得进展，重要的子问题必须被隔离和研究，但它们应该是在完整的、交互的、目标寻求的代理中起着明确作用的子问题，即使完整代理的所有细节还不能被填满。
我们所说的完全的、互动的、寻求目标的代理，并不总是指一个完整的有机体或机器人。这些显然是例子，但一个完整的、交互式的、目标寻求代理也可以是一个大型行为系统的组成部分。在这种情况下，代理直接与大型系统的其余部分交互，并间接地与大型系统的环境交互。一个简单的例子是一个代理，它监视机器人电池的充电水平，并向机器人的控制架构发送命令。这个agent的环境是机器人的其余部分以及机器人的环境。我们必须超越最明显的代理及其环境示例

欣赏强化学习框架的通用性。
现代强化学习最令人兴奋的一个方面是它与其他工程和科学学科之间的实质性和富有成效的互动。强化学习是人工智能和机器学习几十年发展趋势的一部分，这一趋势是为了更好地与统计、优化和其他数学科目相结合。例如，一些强化学习方法能够用参数化逼近器进行学习，这就解决了运筹学和控制理论中经典的“维数诅咒”问题。更明显的是，强化学习也与心理学和神经科学有着强烈的相互作用，两者都带来了巨大的好处。在机器学习的所有形式中，强化学习是最接近人类和其他动物的学习方式，许多强化学习的核心算法最初是受到生物学习系统的启发。强化学习也有回报，既通过动物学习的心理模型更好地匹配一些经验数据，也通过大脑奖励系统的部分有影响力的模型。这本书的主体发展与工程和人工智能有关的强化学习的思想，与心理学和神经科学的联系在第14和15章总结。
最后，强化学习也是人工智能回归简单的一般原则的更大趋势的一部分。自20世纪60年代末以来，许多人工智能研究人员认为没有普遍的原则可以被发现，取而代之的是由于拥有大量的特殊目的的诡计、程序和启发式。有时人们会说，如果我们能把足够的相关信息输入一台机器，比如一百万或十亿，那么它就会变得智能。基于一般原则(如搜索或学习)的方法被描述为“弱方法”，而基于特定知识的方法被称为“强方法”。这种观点在今天仍然很普遍，但并不占主导地位。从我们的观点来看，这完全是不成熟的:在寻求一般原则时，几乎没有付出什么努力来得出根本没有原则的结论。现代人工智能现在包括很多研究，寻找学习、搜索和决策的一般原理。目前尚不清楚这个钟摆将会摆动多远，但强化学习研究肯定是朝着更简单、更少的人工智能一般原则的方向倒退的一部分。


\section{强化学习的例子}

理解强化学习的一个好方法是考虑一些指导它发展的例子和可能的应用。

国际象棋大师棋步。选择是通过计划——预测可能的回答和反驳——以及直接的、直觉的判断特定职位和行动的可取性来通知的。

•自适应控制器实时调整炼油厂运行参数。控制器在指定的边际成本的基础上优化产量/成本/质量的权衡，而不严格遵循工程师最初建议的设置点。

\section{强化学习的要素}

一只小羚羊出生后几分钟就挣扎着站起来。半小时后，它以每小时20英里的速度运行。

•一个移动机器人决定是进入一个新的房间寻找更多的垃圾来收集，还是开始寻找返回电池充电站的方法。它根据电池当前的充电水平以及过去找到充电器的速度和容易程度做出决定。

菲尔准备早餐。仔细研究后，即使是这种看似平凡的活动，也揭示了一个由条件行为和相互关联的目标-子目标关系组成的复杂网络:走向橱柜，打开它，选择一个麦片盒子，然后伸手去拿，抓取，然后取回盒子。要得到一个碗、勺子和牛奶盒，还需要其他复杂的、经过调整的、相互作用的行为序列。每一步都需要一系列的眼球运动来获取信息，并引导接触和移动。人们不断做出快速的判断，比如如何携带这些物品，或者在获得其他物品之前，最好先将其中一些物品运送到餐桌上。每一步都有自己的目标，比如抓一把勺子或者去冰箱拿东西，这些目标都是为了实现其他的目标，比如在准备好麦片的时候用勺子吃饭，最终获得营养。不管他是否意识到这一点，菲尔正在获取关于他身体状况的信息，这些信息决定了他的营养需求、饥饿程度和食物偏好。

这些示例共享非常基本的特性，因此很容易被忽略。所有这些都涉及到一个活跃的决策代理和它的环境之间的交互，在这些交互中，代理试图在不确定的环境下实现目标。agent的行为被允许影响环境的未来状态(例如，下一个象棋位置，炼油厂的储藏量水平，机器人的下一个位置和电池的未来充电水平)，从而影响agent在以后的行为和机会。正确的选择需要考虑到行动的间接、延迟的后果，因此可能需要远见或计划。
与此同时，在所有这些例子中，行动的影响不能完全预测;因此，代理必须频繁地监视它的环境并作出适当的反应。例如，菲尔必须注意他倒进麦片碗里的牛奶，以防止牛奶溢出来。所有这些示例都涉及明确的目标，因为代理可以根据其能够直接感知的内容来判断实现目标的进展。棋手知道自己是否赢了，炼油厂的控制器知道生产了多少石油，羚羊知道石油何时坠落，移动机器人知道电池何时耗尽，菲尔知道自己是否在享用早餐。
在所有这些示例中，代理可以使用其经验改进其性能。国际象棋棋手改进他用来评估位置的直觉，从而改进他的比赛;小羚羊提高了奔跑的效率;菲尔学习流线型地做早餐。在开始时，代理带给任务的知识——无论是以前的相关任务经验，还是通过设计或演进构建的相关任务——会影响什么是有用的，什么是容易学习的，但是与环境的交互对于调整行为以利用任务的特定特性是必不可少的。



除了代理和环境之外，还可以识别增强学习系统的四个主要子元素:策略、奖励信号、值函数，以及可选的环境模型。
策略定义了学习代理在给定时间的行为方式。粗略地说，政策是一种从感知的环境状态映射到在这些状态下要采取的行动的映射。它对应于心理学上所谓的一套刺激反应规则或联想。在某些情况下，策略可能是一个简单的函数或查找表，而在其他情况下，它可能涉及大量的计算，比如搜索过程。策略是增强学习代理的核心，因为它本身就足以决定行为。一般来说，策略可能是随机的，为每个动作指定概率。
奖励信号定义了强化学习问题的目标。在每个时间步骤上，环境向增强学习代理发送一个称为奖励的单个数字。代理人的唯一目标是在长期内获得最大的回报。因此，奖励信号定义了对代理来说什么是好事，什么是坏事。在生物系统中，我们可能认为奖励类似于快乐或痛苦的体验。它们是代理所面临的问题的直接和明确的特征。奖励信号是改变政策的主要依据;如果一个由策略选择的动作后面跟着低回报，那么该策略可能会被更改，以便在将来选择该情况下的其他动作。一般来说，奖励信号可能是环境状态和所采取行动的随机函数。
而奖励信号在即时意义上表示什么是好的，而值函数指定了什么是长期的好。粗略地说，一个国家的价值是一个代理人在未来累积的回报总额，从这个国家开始。虽然奖励决定了环境状态的直接的、内在的可取性，但价值表明在考虑到可能遵循的国家和这些国家的可获得的奖励之后，国家的长期可取性。例如，一个国家可能总是会得到一个低的即时回报，但仍然有很高的价值，因为它经常被其他的国家所效仿，获得高回报。反之亦然。打个人类的比方，奖励有点像快乐(如果高的话)和痛苦(如果低的话)，而价值则对应于对我们的环境处于特定状态时的高兴或不愉快程度的更精确和更有远见的判断。
在某种意义上，奖励是首要的，而价值作为对奖励的预测是次要的。没有奖励就没有价值，估计价值的唯一目的就是获得更多的奖励。然而，我们最关心的是我们在制定和评估决策时最关心的价值观。行动选择是基于价值判断。我们寻求能带来最高价值的行为，而不是最高回报的行为，因为这些行为在长期内为我们获得最大的回报。不幸的是，要确定价值比确定回报要难得多。奖励基本上是由环境直接给出的，但是值必须通过一个代理在其整个生命周期中所做的观察序列来估计和重新估计。事实上，我们考虑的所有强化学习算法中最重要的部分是a

\section{限制和范围}



有效估计值的方法。价值评估的核心作用可以说是在过去60年中学到的关于强化学习的最重要的东西。
一些强化学习系统的第四个也是最后一个要素是环境模型。这是一种模仿环境行为的东西，或者更一般地说，它允许对环境的行为进行推断。例如，给定一个状态和动作，模型可能预测下一个状态和下一个奖励的结果。模型用于规划，我们指的是在实际经历之前，通过考虑可能的未来情况来决定行动的任何方式。解决使用模型和计划的强化学习问题的方法被称为基于模型的方法，而不是简单的无模型的方法，后者是显式的试错学习者——被视为几乎与计划相反。在第8章，我们探索强化学习系统，通过尝试和错误同时学习，学习环境模型，并使用模型进行规划。现代强化学习的范围从低水平的、反复试验的学习到高层次的、深思熟虑的计划。

强化学习在很大程度上依赖于状态作为策略和值函数的输入的概念，并且作为模型的输入和输出。非正式地，我们可以将状态视为在特定时间向代理传递“环境如何”的某种感觉的信号。我们在这里使用的状态的正式定义是由马尔可夫决策过程框架在第3章中给出的。然而，更一般地说，我们鼓励读者遵循非正式的含义，并将国家视为代理可以获得的关于其环境的任何信息。实际上，我们假设状态信号是由某些预处理系统产生的，而这些预处理系统名义上是代理环境的一部分。在本书中，我们不讨论构造、更改或学习状态信号的问题(除了在第17.3节中简要介绍)。我们采取这种做法不是因为我们认为国家代表性不重要，而是为了充分集中于决策问题。换句话说，我们在这本书中关注的不是设计状态信号，而是决定以任何状态信号可用的函数来采取什么动作。
我们在本书中考虑的大多数强化学习方法都是围绕估计价值函数来构建的，但并不是一定要这样做来解决强化学习的问题。例如，求解方法如遗传算法、遗传规划、模拟退火和其他优化方法都不估计值函数。这些方法应用多个静态策略，每个策略在一段时间内以独立的环境实例进行交互。获得最多奖励的策略，以及随机变化的策略，将被传递到下一代策略中，并重复这个过程。我们称这些进化方法为进化方法，因为它们的运作类似于生物进化产生具有熟练行为的生物体的方式，即使它们在它们的一生中没有学习过。如果政策的空间足够小，或者可以构造成好的政策。

常见的或容易找到的——或者如果大量的时间用于搜索，那么进化的方法是有效的。此外，进化方法在学习代理无法感知其环境完整状态的问题上具有优势。
我们的重点是强化学习方法，这些方法在与环境交互时学习，而进化方法则不这样做。在许多情况下，能够利用个体行为交互细节的方法比进化方法更有效。进化方法忽略了强化学习问题的许多有用结构:它们没有利用这样一个事实，即它们正在寻找的策略是一个从状态到行为的函数;他们没有注意到一个个体在其一生中通过了哪些状态，或者它选择了哪些行为。在某些情况下，这些信息可能具有误导性(例如，当状态被误解时)，但更经常的是，它应该使搜索更有效。虽然进化和学习有许多共同的特点，并且自然地共同工作，我们不认为进化方法本身特别适合强化学习问题，因此，我们在本书中不讨论它们。


\section{一个扩展的例子:井字游戏}

为了说明强化学习的一般概念，并将其与其他方法进行对比，我们接下来将更详细地考虑单个示例。



想想熟悉的孩子玩的井字游戏。两个玩家轮流在一块三乘三的棋盘上玩。一个玩家玩Xs游戏，另一个玩家玩Os游戏，直到其中一个玩家在游戏中以一排三分的成绩获胜，水平、垂直或对角线排列，就像X玩家在游戏中向右显示的那样。如果棋盘上没有一个玩家连续得到3个，那么游戏就是平局。因为一个有技巧的球员可以打得永远不会输，所以让我们假设我们在和一个不完美的球员比赛，这个球员的表现有时是错误的，让我们可以赢。目前,
事实上，让我们考虑平局和损失对我们同样不利。我们该如何构建一个能够发现对手在比赛中不完美之处，并学会最大限度地增加获胜机会的球员呢?
虽然这是一个简单的问题，但不能通过经典的技术以令人满意的方式解决它。例如，博弈论中的经典“极小极大”解在这里是不正确的，因为它假定对手有一种特定的玩法。例如，一个小游戏玩家永远不会达到它可能失败的游戏状态，即使事实上它总是从那个状态中获胜，因为对手的不正确的游戏。经典的顺序决策问题的优化方法，如动态规划，可以计算任何对手的最优解，但需要输入一个完整的对手的规格，包括对手在每个板状态下的每个动作的概率。让我们假定，这个信息不是为这个问题而事先获得的，因为它不是为大多数实际感兴趣的问题而提供的。另一方面，这种信息可以通过经验来估计，在这种情况下，可以通过与对手进行多场比赛来估计。这是最好的办法

在这个问题上，首先要学习一个对手行为的模型，直到一定程度的自信，然后应用动态规划来计算一个给定近似对手模型的最优解。最后，这与我们稍后在本书中研究的一些强化学习方法并没有什么不同。
一种应用于这个问题的进化方法将直接搜索可能的策略的空间，从而有可能赢得对手的胜利。在这里，策略是一条规则，告诉玩家在游戏的每一种状态下该做什么——三乘三的棋盘上所有可能的Xs和Os配置。对于每一种策略，通过对对手进行一定数量的游戏来估计其获胜的可能性。然后，该评估将指导下一步考虑哪些政策或政策。一种典型的进化方法是在政策空间中爬升，不断地生成和评估政策，试图获得渐进的改进。或者，也许可以使用一种遗传类型的算法来维护和评估一组策略。实际上可以应用数百种不同的优化方法。
下面是利用值函数的方法来解决井字问题的方法。首先，我们要建立一个数字表，每个数字对应游戏的可能状态。每个数字都是我们从那个州获胜的概率的最新估计值。我们把这个估计看作是状态的值，整个表是学习的值函数。状态值高于国家B,或被认为是比状态B“更好”,如果当前的概率的估计我们赢得来自一个高于B .假设我们总是玩x,然后连续所有州有三个Xs获胜的概率是1,因为我们已经赢了。同样，对于所有连续有三个Os的状态，或者被填满的状态，正确的概率是0，因为我们无法从它们中获胜。我们将所有其他州的初始值设为0.5，表示我们有50%的获胜几率。
然后我们和对手进行了很多比赛。为了选择我们的移动，我们检查每一个可能的移动的状态(黑板上每个空格对应一个)，并在表中查找它们的当前值。大多数时候，我们都是贪婪地前进，选择的移动会导致价值最高的状态，也就是说，赢得的概率最高。然而，我们偶尔会从其他动作中随机选择。这些被称为探索性行动，因为它们使我们经历了我们可能从未见过的状态。在游戏中进行和考虑的一系列动作可以如图1.1所示。
当我们在玩的时候，我们改变了我们在游戏中发现自己的状态的价值。我们试图使他们更准确地估计获胜的可能性。为此，我们“备份”了每个贪婪移动到状态之前的状态的值，如图1.1中的箭头所示。更准确地说，前面状态的当前值被更新为更接近后面状态的值。这可以通过将较早状态的值移动到较晚状态的值的一小部分来实现。如果我们让St表示贪婪移动前的状态，St+1表示移动后的状态，则更新St的估计值为V (St)

V(St)←(St)+α

V(圣+ 1)−V(St)


起始位置开始的位置


图1.1:一组井字棋的动作。实心的黑线代表在游戏中所采取的动作;虚线代表我们(我们的强化学习玩家)考虑过但没有做出的动作。我们的第二个行动是一个探索性的举动,这意味着它是即使另一个兄弟姐妹,一个导致e∗,排名更高。探索性移动不会导致任何学习，但是我们其他的每一个移动都会导致更新，如红色箭头所示，其中估计的值将从后面的节点移动到前面的节点，如文中所述。


在α是一个小的积极的部分称为步长参数,影响学习的速度。这个更新规则是temporal-difference学习方法的一个例子,所谓的,因为它的变化是基于不同,V(圣+ 1)−V(St),估计之间的连续两次。
上面描述的方法在这个任务上执行得很好。例如，如果步长参数随时间适当减小，那么对于任何固定的对手，该方法收敛到我们的玩家在最佳发挥下从每个状态中获胜的真实概率。此外，所采取的行动(除了探索性的行动)实际上是针对这个(不完美的)对手的最佳行动。换句话说，该方法收敛于与该对手博弈的最优策略。如果步长参数没有随着时间的推移一直降低到零，那么这个玩家也可以很好地对抗那些慢慢改变他们的游戏方式的对手。
这个例子说明了进化方法和学习价值函数的方法之间的区别。为了评估一个策略，演化方法将策略固定，并对对手进行许多游戏，或者使用对手的模型来模拟许多游戏。获胜的频率给出了概率的无偏估计


用该策略获胜，并可用于指导下一个策略的选择。但是每一个政策的改变都是在许多游戏之后才进行的，并且只有每个游戏的最终结果被使用:在游戏期间发生的事情被忽略了。例如，如果玩家赢了，那么他在游戏中的所有行为都得到了认可，这与具体的动作对获胜的关键程度无关。甚至对从未发生过的动作给予赞扬!与此相反，值函数方法允许对各个状态进行评估。最后，进化函数和价值函数方法都是搜索策略的空间，但是学习一个价值函数利用了在过程中可用的信息。
这个简单的例子说明了强化学习方法的一些关键特性。首先，强调的是在与环境交互时的学习，在这种情况下，与对手进行交互。第二，有一个明确的目标，正确的行为需要考虑到自己选择的延迟效应的计划或远见。例如，简单的增强学习玩家将学会设置多移动陷阱，为一个目光短浅的对手。增强学习解决方案的一个显著特点是，它不需要使用对手的模型，也不需要对未来状态和动作的可能序列进行显式搜索，就可以实现规划和预测的效果。
虽然这个例子说明了强化学习的一些关键特性，但是它是如此简单，以至于它可能会给人这样的印象，强化学习比实际的要有限。虽然井字游戏是一种双人游戏，强化学习也适用于没有外部对手的情况，也就是说，在“对抗自然”的情况下。强化学习也不局限于行为分解成不同阶段的问题，比如“井字游戏”的独立游戏，只有在每一集结束时才会有奖励。当行为无限期地持续下去，以及任何时候都能收到不同程度的奖励时，它同样适用。强化学习也适用于那些甚至不像井字游戏那样分解成离散时间步骤的问题。一般原则也适用于持续时间的问题，尽管理论变得更加复杂，我们在这个介绍的处理中省略了它。
井字棋有一个相对较小的有限状态集，当状态集非常大甚至无穷大时，可以使用强化学习。例如，Gerry Tesauro(1992, 1995)将上面描述的算法与一个人工神经网络相结合，以学习玩具有大约1020个状态的西洋双陆棋。有了这么多的状态，你不可能体验到其中的一小部分。Tesauro的程序学得比以前的程序好多了，最终也比世界上最好的人类玩家强(第16.1节)。人工神经网络为程序提供了从经验中归纳的能力，以便在新的状态中，它根据以前面对的相似状态所保存的信息来选择动作，这是由它的网络决定的。强化学习系统在如此大的状态集中如何工作与它从过去的经验中如何恰当地归纳密切相关。正是在这个角色中，我们最需要的是强化学习的监督学习方法。人工神经网络和深度学习(第9.6节)不是唯一的，也不一定是最好的方法。
在这个井字游戏的例子中，学习是在没有知识的情况下开始的

游戏规则，但强化学习绝不意味着学习和智力的一张白板。相反，先前的信息可以以各种对有效学习至关重要的方式纳入强化学习(例如，参见9.5、17.4和13.1节)。我们也可以在井字游戏中获得真实的状态，而强化学习也可以在部分状态被隐藏，或者当不同的状态在学习者看来是相同的时候被应用。
最后，井字游戏的玩家能够向前看，知道每一个可能的移动会产生什么状态。要做到这一点，它必须有一个游戏模型，让它能够预见到它的环境将如何随着它可能永远不会做出的动作而变化。许多问题都是这样，但在其他问题上，甚至缺乏行动效果的短期模型。强化学习可以应用于任何一种情况。模型不是必需的，但是如果模型是可用的或可以学习的，那么模型可以很容易地使用(第8章)。
另一方面，强化学习方法根本不需要任何环境模型。没有模型的系统甚至不能考虑它们的环境如何随着单个动作而变化。从这个意义上说，井字游戏玩家对其对手是没有模型的:它没有任何对手的模型。由于模型必须具有相当的准确性，因此，当解决问题的真正瓶颈是构建一个足够精确的环境模型的困难时，无模型方法比更复杂的方法具有优势。无模型方法也是基于模型的方法的重要构建块。在本书中，在讨论如何将无模型方法用作更复杂的基于模型的方法的组件之前，我们将用几章讨论无模型方法。
强化学习可以在系统的高水平和低水平上使用。虽然井字游戏玩家只学习了游戏的基本动作，但没有什么能阻止强化学习在更高的层次上进行，在更高的层次上，每一个“动作”本身都可能是一种精心设计的解决问题的方法的应用。在分层学习系统中，强化学习可以同时在几个层次上进行。
练习1.1:自玩假设，上面描述的强化学习算法不是与随机对手对抗，而是与自己对抗，双方都在学习。你认为在这种情况下会发生什么?它会学到不同的政策吗
选择移动吗? 					?
对称性:由于对称性，很多井字的位置看起来不同，但实际上是相同的。我们如何修改上面描述的学习过程来利用这一点呢?这种改变将在哪些方面改善学习过程?现在再想想。假设对手没有利用对称性。既然如此，我们应该这样做吗?那么，对称等价的位置应该是正确的吗
一定有相同的价值吗? 					?
练习1.3:贪心游戏假设强化学习玩家是贪心的，也就是说，它一直在做使它达到它认为最好的位置的动作。它会比一个不贪心的玩家玩得更好，还是更糟?会发生什么问题??
练习1.4:从探索中学习假设学习更新发生在所有动作之后，包括探索动作。如果步长参数适当减少


随着时间的推移(但不是探究的趋势)，状态值会收敛到一组不同的概率。当我们这样做的时候(从概念上)，当我们不这样做的时候，这两组概率是什么?假设我们继续进行探索性的行动，那一组概率可能会更好。
学习吗?哪个会导致更多的胜利? 					?
练习1.5:你能想出其他方法来提高强化学习玩家吗?你能想出更好的办法来解决井字游戏的问题吗
作为构成? 					?


\section{总结}

强化学习是理解和自动化目标导向学习和决策的一种计算方法。它与其他计算方法的区别在于，它强调由代理进行学习，而不是直接与环境交互，而不需要模范监督或完整的环境模型。在我们看来，强化学习是第一个认真处理从与环境交互中产生的计算问题，以实现长期目标的领域。
强化学习使用马尔可夫决策过程的正式框架，以状态、行为和奖励的形式定义学习主体与其环境之间的交互。这个框架旨在成为表示人工智能问题的基本特征的一种简单方法。这些特征包括因果感、不确定性和不确定性，以及明确目标的存在。
价值和价值函数的概念是我们在本书中考虑的大多数强化学习方法的关键。我们的立场是，在政策空间中，价值函数对于有效搜索非常重要。价值函数的使用区分了强化学习方法和进化方法，后者直接在整个政策评估的指导下在政策空间中搜索。

\section{强化学习的早期历史}

强化学习的早期历史有两个主要的主线，分别是长期和丰富的，在现代强化学习中相互交织。有一条线是关于尝试和错误的学习，它起源于动物学习心理学。这一思路贯穿于人工智能的早期研究，并在20世纪80年代早期引领了强化学习的复兴。第二个线程是利用值函数和动态规划的最优控制问题及其解决方案。在大多数情况下，这条线并不涉及学习。这两个线程大部分是独立的，但在某种程度上与第三个不同的线程相互关联，在这一章中，在井字的例子中使用的时间差方法就不那么明显了。这三个线索在20世纪80年代末汇聚在一起，产生了我们在本书中展示的现代强化学习领域。

关注试错学习的线索是我们最熟悉的，也是我们在这段简短的历史中最有发言权的。然而，在此之前，我们简要地讨论了最优控制线程。
“最优控制”一词在20世纪50年代末开始使用，用来描述设计一个控制器来最小化或最大化一个动态系统随时间变化的行为度量的问题。解决这个问题的方法之一是理查德·贝尔曼和其他人在20世纪50年代中期提出的通过扩展19世纪的汉密尔顿和雅可比理论。这种方法使用动态系统的状态和值函数的概念，或者“最优返回函数”来定义一个函数方程，现在通常称为Bellman方程。通过求解这个方程来求解最优控制问题的方法被称为动态规划(Bellman, 1957a)。Bellman (1957b)还引入了最优控制问题Markov decision process (MDPs)的离散随机版本。Ronald Howard(1960)设计了MDPs的策略迭代方法。所有这些都是现代强化学习理论和算法的基础要素。
动态规划被广泛认为是解决一般随机最优控制问题的唯一可行方法。它受Bellman所称的“维度的诅咒”的影响，这意味着它的计算需求随状态变量的数量呈指数增长，但它仍然比其他一般方法更有效和更广泛地适用。自20世纪50年代末以来，动态编程得到了广泛的发展，包括对部分可观测的MDPs的扩展(由洛夫乔伊调查，1991)，许多应用(由怀特调查，1985,1988,1993)，近似方法(由Rust调查，1996)和异步方法(Bertsekas, 1982, 1983)。许多优秀的动态规划的现代处理方法是可用的(例如，Bertsekas, 2005, 2012;Puterman,1994;罗斯,1983;1982年惠特尔,1983)。Bryson(1996)提供了最优控制的权威历史。
一方面，最优控制与动态规划之间的联系，另一方面，学习之间的联系很难被识别。我们不能确定这种分离的原因是什么，但是它的主要原因可能是所涉及的学科和它们的不同目标之间的分离。动态规划作为一种离线计算的普遍观点，在本质上依赖于精确的系统模型和Bellman方程的解析解的基础上，也可能有所贡献。此外，最简单的动态编程形式是一种向后进行的计算，这使我们很难看到它是如何参与到一个必须朝前进行的学习过程中来的。一些早期的动态编程工作，如Bellman和Dreyfus(1959)的工作，现在可以归类为遵循学习方法。Witten(1977)的作品(下文讨论)肯定是学习和动态规划思想的结合。Werbos(1987)明确主张动态规划和学习方法的更大的相互关系，以及动态规划与理解神经和认知机制的相关性。对于我们来说，直到1989年克里斯·沃特金斯(Chris Watkins)的作品，在使用MDP形式主义的强化学习的方法被广泛采用的情况下，才出现了动态规划方法与在线学习的完全融合。从那时起，这些关系被许多研究者广泛发展，尤其是迪米特里·贝尔采卡斯

John tsiklis(1996)发明了“神经动态规划”一词，指的是动态规划和人工神经网络的结合。目前使用的另一个术语是“近似动态规划”。这些不同的方法强调了这门学科的不同方面，但它们都与强化学习一样，都对绕过动态规划的经典缺陷感兴趣。
我们认为最优控制下的所有工作，在某种意义上，也是强化学习中的工作。我们将强化学习方法定义为解决强化学习问题的有效方法，现在很明显，这些问题与最优控制问题密切相关，特别是像MDPs这样的随机最优控制问题。因此，我们必须考虑最优控制的解决方法，如动态规划，也要考虑强化学习方法。因为几乎所有的常规方法都需要完全了解系统才能被控制，所以说它们是强化学习的一部分有点不自然。另一方面，许多动态规划算法是递增和迭代的。像学习方法一样，他们通过不断的逼近逐渐得到正确的答案。正如我们在本书其余部分所展示的，这些相似之处远不止表面现象。完全知识和不完全知识的情况下的理论和解决方法是如此紧密地联系在一起，我们认为它们必须作为同一主题的一部分来考虑。
现在让我们回到通向现代强化学习领域的另一条主线，这条主线围绕着试错学习的思想。我们只讨论这里的主要联系人，在第14.3节中更详细地讨论这个主题。根据美国心理学家r s Woodworth(1938)试误学习的想法可以追溯到1850年代,亚历山大·贝恩的讨论学习的“摸索和实验”,更明确地向英国动物行为学家和心理学家康威劳埃德·摩根的1894使用的术语来描述他的观察动物的行为。也许第一个简洁地表达了试错学习的本质的原则是爱德华桑代克:

在对相同情况作出的几种反应中，在其他条件相同的情况下，伴随或紧随其后的是对动物的满足，与这种情况更紧密地联系在一起，因此，当它再次出现时，它们将更有可能再次出现;在其他条件相同的情况下，那些伴随或紧随其后的动物会削弱它们与这种情况的联系，这样，当这种情况再次发生时，它们就不太可能发生。满足感或不舒服程度越高，这种联系就越牢固或减弱。(桑代克,1911,p . 244)
桑代克把这称为“效应定律”，因为它描述了强化事件对选择行为倾向的影响。后来，桑代克修改了法律，以便更好地解释关于动物学习的后续数据(比如奖励和惩罚效果之间的差异)，而法律的各种形式在学习理论家中引起了相当大的争议(例如，参见Gallistel, 2005;伯恩斯坦,1970;金布尔、1961、1967;Mazur,1994)。尽管如此，作为一种或另一种形式的有效法则被广泛认为是许多行为背后的基本原则(例如，Hilgard和Bower, 1975;丹尼特,1978;坎贝尔,1960;Cziko,1995)。它是有影响力的人的基础

克拉克·赫尔的学习理论(1943,1952)和斯金纳的影响实验方法(1938)。
在动物学习的语境中，“强化”一词是在桑代克表达“效应定律”之后才开始使用的，最早出现在这一语境中(据我们所知)。巴甫洛夫将强化描述为一种行为模式的强化，这种行为模式是由于动物受到刺激——一种强化——在与另一种刺激或反应的适当时间关系中。一些心理学家将强化的概念扩展到包括弱化和强化行为，并扩展了强化的概念，包括可能省略或终止刺激。要被认为是强化物，强化物被收回后，强化物或弱化物必须继续存在;仅仅吸引动物注意力或激励动物行为而不产生持久变化的刺激物不会被认为是强化物。
在计算机中实现试错学习的想法最早出现在关于人工智能的可能性的思想中。在1948年的一份报告中，艾伦·图灵(Alan Turing)描述了一种“愉悦-痛苦系统”的设计，该系统遵循的是效果定律:

当到达一个未确定操作的配置时，将对丢失的数据进行随机选择，并在描述中进行适当的输入，并将其应用于描述中。当疼痛刺激发生时，所有试探性的条目都被取消，当快乐刺激发生时，它们都是永久性的。(图灵,1948)
许多精巧的机电设备被制造出来，演示了反复试验的学习。最早的机器可能是由托马斯·罗斯(1933年)建造的机器，它能够通过一个简单的迷宫找到路，并通过开关的设置记住路径。在1951 W。格雷沃特建造了他的“机械乌龟”(沃尔特，1950年)的一个版本，他有一种简单的学习方式。1952年，克劳德·香农(Claude Shannon)演示了一种名为忒修斯(Theseus)的老鼠，它通过反复试验找到迷宫的路径，迷宫本身通过磁铁和地底继电器记住成功的方向(参见香农(Shannon, 1951))。J. a . Deutsch(1954)基于他的行为理论(Deutsch, 1953)描述了一台求解迷宫的机器，它与基于模型的强化学习有一些共同的特性(第八章)。马文·明斯基(1954)讨论了强化学习的计算模型，并描述了他构建的一个模拟机器，由他称之为“随机神经-模拟强化计算器”的组件组成，这些组件旨在模拟大脑中可修改的突触连接(第15章)。
建造机电学习机器让位于编程数字计算机来执行各种类型的学习，其中一些实现了试错学习。Farley和Clark(1954)描述了一种神经网络学习机器的数字模拟，这种机器通过反复试验来学习。但他们的兴趣很快从试错学习转移到泛化和模式识别，即从强化学习转移到监督学习(Clark and Farley, 1955)。这一模式开始

对这些类型的学习之间的关系感到困惑。许多研究人员似乎认为，他们在研究强化学习时，实际上是在研究监督学习。例如，像Rosenblatt(1962)和Widrow和Hoff(1960)这样的人工神经网络先驱显然是受到强化学习的激励——他们使用奖励和惩罚的语言——但他们研究的系统是监督学习系统，适合于模式识别和知觉学习。即使在今天，一些研究人员和教科书也淡化或模糊了这些类型的学习之间的区别。例如，一些人工神经网络教科书使用“试错法”一词来描述从训练例子中学到的网络。这是一种可以理解的混淆，因为这些网络使用错误信息来更新连接权值，但这忽略了试错学习的本质特征，即在不依赖于正确操作的知识的评估反馈的基础上选择操作。
部分由于这些混淆，真正的试错学习的研究在20世纪60年代和70年代变得罕见，尽管有明显的例外。20世纪60年代，在工程文献中首次使用“强化”和“强化学习”这两个术语来描述试错学习的工程应用(例如，华尔兹和傅，1965;孟德尔,1966;傅,1970;孟德尔和麦克拉伦,1970)。特别有影响力是明斯基的论文“步骤人工智能”(明斯基,1961),相关的试错学习讨论几个问题,包括预测,期望,和他所谓的基本credit-assignment复杂的强化学习系统的问题:你如何分配信贷成功的决策,可能是参与生产吗?我们在这本书中讨论的所有方法，在某种意义上，都是为了解决这个问题。明斯基的论文今天很值得一读。
在接下来的几段中，我们讨论了在20世纪60和70年代对真正的试错学习的计算和理论研究相对忽视的一些例外和部分例外情况。
新西兰研究人员约翰·安德烈亚(John Andreae)的工作是一个例外。他开发了一种名为STeLLA的系统，通过与周围环境的反复试验来学习。这个系统包括了一个世界的内部模型，以及后来处理隐藏状态问题的“内部独白”(Andreae, 1963, 1969a,b)。Andreae后来的作品(1977)更强调从老师那里学习，但仍然包括通过尝试和错误来学习，创造新事件是系统的目标之一。这项工作的一个特点是“泄漏过程”，在Andreae(1998)中更详细地阐述了这个过程，它实现了一种信贷分配机制，类似于我们所描述的备份更新操作。不幸的是，他的开创性研究并不为人所知，对后续的强化学习研究影响不大。最近的总结有(Andreae, 2017a,b)。
更有影响力的是Donald Michie的作品。在1961年和1963年，他描述了一个简单的试错学习系统，用来学习如何玩“三趾棋”(或“鹦鹉”和“十字”)。它由一个火柴盒组成，每个火柴盒包含了一些颜色的珠子，每个可能的移动都有不同的颜色。通过

从火柴盒中随机绘制出与当前游戏位置相对应的珠子，可以确定威胁的移动。当游戏结束时，在游戏中用来奖励或惩罚威胁的决定的盒子里添加或移除珠子。Michie和Chambers(1968)描述了另一种被称为“欢乐合唱团”(Game Learning Expectimaxing Engine)的井字游戏强化学习器，以及名为box的强化学习控制器。他们用箱子来学习平衡一根杆子和一辆可移动的车，因为只有当杆子落下或车走到轨道末端时才会出现故障信号。这项任务改编自Widrow和Smith(1964)的早期工作，他们使用了监督学习方法，假设老师已经能够平衡杆端。麦奇和钱伯斯版本的杆平衡是在不完全知识条件下强化学习任务的最佳早期范例之一。它影响了后来的强化学习工作，从我们自己的一些研究开始(Barto, Sutton, and Anderson, 1983;萨顿,1984)。Michie始终强调试验、错误和学习作为人工智能的基本方面的作用(Michie, 1974)。
Widrow, Gupta，和Maitra(1973)修改了Widrow和Hoff(1960)的最小均方(LMS)算法，以产生一个增强学习规则，可以从成功和失败的信号中学习，而不是从训练示例中学习。他们将这种学习方式称为“选择性自适应”，并将其描述为“与批评家一起学习”，而不是“与老师一起学习”。他们分析了这个规则，并展示了它是如何学会玩21点的。这是Widrow单独对强化学习的尝试，他对监督学习的贡献要大得多。我们对“批评家”一词的使用源自Widrow、Gupta和Maitra的论文。布坎南(Buchanan)、米切尔(Mitchell)、史密斯(Smith)和约翰逊(Johnson)(1978)在机器学习的背景下独立地使用了“批评家”一词(参见Dietterich和布坎南(Buchanan)， 1984)，但对他们来说，批评家是一个能够做的不仅仅是评价性能的专家系统。
学习自动机的研究对试错线有更直接的影响，从而导致了现代强化学习的研究。这些方法是用来解决一个非联想的，纯粹的选择学习的问题，被称为k-武装土匪，类似于老虎机，或“独臂赌博机”，除了k个杠杆(见第二章)。学习自动机是一种简单的，低内存的机器，用于提高这些问题中奖励的可能性。学习自动机起源于20世纪60年代的俄国数学家和物理学家m.l. Tsetlin和他的同事们(在1973年的Tsetlin出版后出版)，从那时起就在工程领域内得到了广泛的发展(见Narendra和Thathachar, 1974, 1989)。这些发展包括随机学习自动机的研究，这是一种基于奖励信号更新行动概率的方法。Harth和Tzanakou(1974)的Alopex算法(用于模式提取算法)虽然没有在随机学习自动机的传统中发展起来，但它是一种随机的方法，用于检测行为和强化之间的相关性，影响了我们早期的一些研究(Barto, Sutton，和Brouwer, 1981)。随机学习自动机是心理学早期工作的先兆，从William Estes(1950)研究学习的统计理论开始，并由其他人进一步发展(例如Bush和Mosteller, 1955;斯特恩伯格,1963)。
心理学中发展起来的统计学习理论被研究者采用

经济学，导致了这一领域的研究，致力于强化学习。这项工作始于1973年，当时布什和大多数人的学习理论被应用于古典经济模型的收集(Cross, 1973)。这项研究的一个目标是研究比传统理想化的经济主体更像真人的人工制剂(Arthur, 1991)。该方法扩展到博弈论背景下的强化学习研究。经济学中的强化学习在很大程度上独立于人工智能中强化学习的早期工作，尽管博弈论仍然是这两个领域的研究热点(超出了本书的范围)。卡默勒(2011)讨论了强化学习的传统经济学和Nowé,Vrancx,De Hauwere(2012)提供了一个概述的主题从的角度多主体扩展到这本书中,我们介绍的方法。在游戏理论的背景下，强化是一个与强化学习截然不同的学科，用于玩井字游戏、跳棋和其他娱乐游戏。例如，参见Szita(2012)对增强学习和游戏这一方面的概述。
John Holland(1975)概述了基于选择原则的适应性系统的一般理论。他早期的工作主要涉及试验和错误，主要是非联想形式，如进化方法和k武装匪徒。1976年和1986年，他更全面地介绍了分类器系统，真正的强化学习系统，包括关联和价值函数。Holland分类器系统的一个关键组成部分是信用分配的“桶-旅算法”，它与我们的井-塔-足趾示例中使用的时间差异算法密切相关，并在第6章中进行了讨论。另一个关键组成部分是遗传算法，这是一种进化方法，其作用是进化出有用的表示法。分类器系统已经被许多研究者广泛发展形成的一个主要分支强化学习研究(由Urbanowicz和摩尔,2009),但遗传算法不考虑强化学习系统的自己也得到了更多的关注,因为有其他的进化计算方法(例如,福格尔,欧文斯和沃尔什1966年Koza,1992)。
哈利·克洛普夫(1972年，1975年，1982年)是人工智能强化学习的最主要负责人。Klopf认识到，随着学习研究者几乎完全专注于监督学习，适应性行为的基本方面正在丧失。Klopf认为，缺少的是行为的享乐性方面，即从环境中获得某种结果的动力，即控制环境达到预期目的，远离不期望的目的(见第15.9节)。这是反复试验学习的基本思想。Klopf的观点对作者的影响尤其大，因为我们对它们的评估(Barto和Sutton, 1981a)导致我们对监督学习和强化学习之间的区别的理解，以及我们最终对强化学习的关注。我们和同事完成的许多早期工作是为了表明强化学习和监督学习确实是不同的(Barto, Sutton，和Brouwer, 1981;Barto和萨顿,1981 b;Barto和阿南丹,1985)。其他研究显示强化学习如何解决人工神经网络学习中的重要问题，特别是如何为多层网络生成学习算法(Barto, Anderson，和Sutton, 1982;Barto和安德森,1985;Barto、1985、1986;Barto和约旦,1987)。

现在我们来看第三条关于强化学习的历史，关于时间差异学习。时间差学习方法的独特之处在于，它是由同一数量的时间连续估计的差异所驱动的，例如，在井字游戏中获胜的概率。这条线比另外两条线更小，也不那么明显，但它在这一领域中发挥了特别重要的作用，部分原因是时间差方法似乎是加强学习的新方法和独特之处。
时间差异学习的起源部分是在动物学习心理学中，特别是在次级强化的概念中。次级强化物是一种刺激物与初级强化物(如食物或疼痛)相结合，因此，也具有类似的强化性质。Minsky(1954)可能是第一个认识到这一心理学原理对人工学习系统很重要的人。Arthur Samuel(1959)首先提出并实施了一种包括时间性差异的学习方法，作为他著名的跳棋项目的一部分(第16.2节)。
塞缪尔没有提及明斯基的作品，也没有提及与动物学习可能的联系。他的灵感显然来自克劳德·香农(Claude Shannon)(1950)的建议，即计算机可以被编程来使用一个评估函数来玩国际象棋，并且可以通过在线修改这个函数来改进它的游戏。(很可能香农的这些想法也影响了贝尔曼，但我们没有证据证明这一点。)明斯基(1961)在他的“步骤”论文中广泛地讨论了塞缪尔的工作，提出了与次级强化理论的联系，包括自然强化和人工强化。
正如我们所讨论的，在明斯基和萨缪尔的研究之后的十年里，很少有关于试错学习的计算工作，显然，在时间差异学习方面根本没有计算工作。1972年，Klopf将试错学习与时间性差异学习的一个重要组成部分结合起来。Klopf感兴趣的是能够扩展到大型系统学习的原理，因此对局部强化的概念很感兴趣，即一个整体学习系统的子组件可以相互增强。他提出了“广义强化”的概念，即每个组成部分(名义上是每个神经元)都以强化的方式看待所有输入:兴奋性输入作为奖励，抑制性输入作为惩罚。这与我们现在所知的时间差学习不一样，回想起来，它比塞缪尔的研究还远。另一方面，Klopf将这一想法与试错学习联系起来，并将其与庞大的动物学习心理学实证数据库联系起来。
Sutton (1978a,b,c)进一步发展了Klopf的思想，特别是与动物学习理论的联系，描述了由时间连续的预测变化所驱动的学习规则。他和Barto完善了这些思想，并建立了基于时间差异学习的经典条件反射的心理学模型(Sutton and Barto, 1981a;Barto和萨顿,1982)。此外，还有其他几个影响深远的基于时间差异学习的经典条件作用心理学模型(如Klopf, 1988;摩尔et al .,1986;萨顿和巴托，1987,1990)。当时发展起来的一些神经系统模型被很好地解释为时间差异学习(Hawkins and Kandel, 1984;伯恩·金里奇，巴克斯特，1990;Gelperin, Hopfield, Tank, 1985;Tesauro,

1986;Friston et al.， 1994)，虽然在大多数情况下没有历史联系。
我们早期对时间差学习的研究深受动物学习理论和Klopf工作的影响。与明斯基的“步数”文件和塞缪尔的跳棋球员的关系直到后来才被承认。然而，到1981年，我们充分意识到前面提到的作为时间差和试错线程的一部分的所有工作。此时，我们开发了一种利用时间差学习与试错学习相结合的方法，称为表演-批判架构，并将此方法应用于Michie和Chambers的杆平衡问题(Barto, Sutton, and Anderson, 1983)。该方法在Sutton(1984)的博士论文中得到了广泛的研究，并在Anderson(1986)的博士论文中扩展到利用反向传播神经网络。在此期间，Holland(1986)以他的bucket-brigade算法的形式，在他的分类器系统中显式地引入了时间差概念。Sutton(1988)将时变学习与控制分离，将其作为一种通用的预测方法，采取了关键步骤。这篇论文还介绍了TD(λ)算法并证明了它的一些收敛性质。
当我们在1981年完成我们关于演员-批评家架构的工作时，我们发现了Ian Witten (1977, 1976a)的一篇论文，这似乎是时间差异学习规则最早的出版。他提出了一种方法，我们现在称之为表格式TD(0)，作为自适应控制器的一部分来解决MDPs。这项研究于1974年首次提交给期刊出版，也出现在Witten 1976年的博士论文中。威滕的作品是安德烈亚早期对斯特拉和其他试错学习系统的研究的后代。因此，威滕1977年的论文跨越了强化学习研究的两条主线——试错学习和最优控制——同时对时间性差异学习做出了明显的早期贡献。
1989年，随着克里斯·沃特金斯(Chris Watkins)的Q-learning发展，时变差和最优控制线程得到了充分整合。这项工作扩展并整合了所有三个加强学习研究的线索。Paul Werbos(1987)提出了自1977年以来反复试验学习和动态编程的融合，从而促成了这种融合。到沃特金斯的工作时，强化学习的研究有了巨大的增长，主要是在人工智能的机器学习子领域，也在更广泛的人工神经网络和人工智能领域。1992年，Gerry Tesauro的“西洋双陆棋”(backgammon)游戏计划TD-Gammon取得了非凡的成功，这使该领域受到了更多的关注。
在这本书的第一版出版后，神经科学发展了一个蓬勃发展的分支领域，专注于强化学习算法和神经系统强化学习之间的关系。许多研究人员指出，造成这种现象的最主要原因是时间差异算法的行为与多巴胺在大脑中产生神经元的活动之间有着不可思议的相似性(Friston et al.， 1994;Barto,1995;1995年，Houk, Adams和Barto;蒙太古，大安，塞杰诺斯基，1996;还有舒尔茨、达安和蒙塔古，1997年)。第15章对强化学习这一令人兴奋的方面进行了介绍。在最近的强化学习历史中所作的其他重要贡献太多了，在这个简短的叙述中无法提及;我们在每一章的结尾都引用了其中的许多内容。

书目的言论

对于额外的强化学习的一般范围,我们参考读者的书Szepesvári(2010),Bertsekas和Tsitsiklis(1996),Kaelbling(1993),和Sugiyama Hachiya,Morimura(2013)。从控制或运营研究的角度来看，书中有Si、Barto、Powell和Wunsch(2004)、Powell(2011)、Lewis and Liu(2012)和Bertsekas(2012)。曹教授的(2009)回顾了在其他学习和优化随机动态系统的方法中加强学习的方法。《机器学习》杂志的三期特别集中在强化学习上:Sutton(1992年2月)、Kaelbling(1996年)和Singh(2002年)。Barto(1995年b)提供了有用的调查;凯尔布林，利特曼和摩尔(1996);Keerthi和Ravindran(1997)。由Weiring和van Otterlo(2012)编辑的卷提供了对最近发展的一个很好的概述。

在这一章中，菲尔的早餐的例子是由Agre(1988)启发的。

1.5建立了井字算例中所采用的时间差法
第六章。



