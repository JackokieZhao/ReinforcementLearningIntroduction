
\chapter{第二章 多臂赌博机}

区别强化学习和其他类型学习的最重要的特征是，它使用训练信息来评估所采取的行动，而不是通过给出正确的行动来指导。这就产生了主动探索的需要，即对良好行为的明确搜索。纯粹的评价反馈表明所采取的行动有多好，而不是它是最好的还是最坏的行动。从另一方面来说，纯粹的指导性反馈表明了采取独立于实际采取的行动的正确行动。这种反馈是监督学习的基础，包括模式分类、人工神经网络和系统识别等大部分内容。从纯粹的形式来看，这两种反馈是截然不同的:评价性反馈完全依赖于所采取的行动，而指导性反馈则独立于所采取的行动。
在这一章中，我们研究了强化学习的评估方面，在一个简化的环境下，一个不涉及学习在一个以上的情况下行动的环境。这种非联想设置是大多数涉及评估反馈的先前工作已经完成的设置，它避免了完全强化学习问题的复杂性。通过研究这个案例，我们可以清楚地看到评估性反馈与指导性反馈的区别，但又可以与指导性反馈相结合。
我们探索的特定的非联想、评估反馈问题是k武装匪徒问题的一个简单版本。我们用这个问题来介绍一些基本的学习方法，我们在后面的章节中扩展这些方法来应用于完整的强化学习问题。在这一章的结尾，我们通过讨论当班迪特问题成为联想时发生了什么，也就是说，当在多个情况下采取行动时，我们向完全强化学习问题迈进了一步。

\section{韩国武装匪徒问题}

考虑下面的学习问题。在k个不同的选项或操作中，你会反复面临选择。在每个选择之后，你会收到一个数字奖励，这个数字奖励来自一个固定的概率分布，这个分布取决于你选择的动作。你的

目标是在一段时间内最大化期望的总回报，例如，超过1000个动作选择，或时间步骤。
这是k-armed bandit问题的最初形式，类似于老虎机(老虎机)或“one-armed bandit”，只不过它有k个杠杆，而不是一个杠杆。每一个动作选择就像是老虎机的一个杠杆的游戏，而回报就是中奖的回报。通过重复的动作选择，你要通过将你的动作集中在最好的杠杆上来最大化你的奖金。另一个类比是医生在一系列重病患者的实验治疗中做出选择。每个行动都是治疗的选择，每个奖励都是病人的生存或幸福。今天，“班迪特问题”这个术语有时被用来概括上面描述的问题，但是在这本书中，我们用它来指代这个简单的问题
的情况。
在我们的k武装匪徒问题中，每个k个动作都有预期的或平均的奖励，前提是这个动作是被选择的;让我们把这叫做行动的价值。我们表示行动选择时间步t作为,和相应的奖励的价值然后沿任意行动,表示问∗(a),是期望的奖励,一个选择:

问∗(a)。= E(Rt | =一个)。

如果您知道每个动作的值，那么解决k -armed bandit问题就很简单了:您总是选择值最高的动作。我们假设您不确定地知道操作值，尽管您可能有估计。我们将时刻a的估计值t表示为Qt(a)。我们希望Qt(a)接近问∗(a)。
如果您保持对动作值的估计，那么在任何时候，至少有一个动作的估计值是最大的。我们称之为贪婪行为。当您选择其中一个动作时，我们说您正在利用您当前对操作的值的知识。如果您选择一个非贪婪操作，那么我们说您正在探索，因为这使您能够改进对非贪婪操作的值的估计。开发是一件正确的事情，可以使预期的回报最大化，但从长远来看，探索可能会产生更大的总体回报。例如，假设一个贪婪行为的值是确定的，而其他几个行为被估计为几乎一样好，但是有很大的不确定性。不确定的是，至少有一种行为可能比贪婪行为更好，但你不知道是哪一种。如果您有许多时间步骤可以进行操作选择，那么最好探索非贪婪的操作，并发现它们中哪个比贪婪的操作更好。短期而言，在探索过程中，回报较低，但长期而言回报较高，因为在你发现了更好的行为之后，你可以多次利用它们。由于不可能同时探索和利用任何单一的行动选择，人们经常提到探索和开发之间的“冲突”。
在任何特定的情况下，探索或利用是否更好取决于估算的精确值、不确定性和剩余步骤的数量。关于k -武装土匪的特殊数学公式和相关问题，有许多复杂的方法来平衡勘探和开发。

\section{行为价值的方法}
然而，这些方法中的大多数都对平稳性和先验知识做出了强有力的假设，这些假设在应用程序中或在我们在后续章节中考虑的完全强化学习问题中被违反或无法验证。当这些方法的假设不适用时，对这些方法的最佳性或有界损失的保证是没有什么安慰的。
在这本书中，我们不担心探索和开发之间的复杂平衡;我们只关心平衡它们。在这一章中，我们提出了几种简单的平衡方法来解决k -armed bandit问题，并证明它们比那些经常使用的方法要有效得多。平衡探索和开发的需要是强化学习中出现的一个独特的挑战;我们版本的k武装匪徒问题的简洁性使我们能够以一种特别清晰的形式展示这一点。

我们首先更仔细地研究评估行为值的方法，以及使用评估做出行为选择决策的方法，我们统称为行为价值方法。回想一下，一个行为的真正价值是选择该行为时的平均回报。估计这一点的一种自然方法是将实际收到的奖励平均下来:

Qt(a)。= a在t之前的奖励总和
a在t之前的次数
=
? t−1
我= 1 Ri·艾= ? t−1我= 1 Ai =
(2.1)

其中谓词表示谓词为真时为1，非为0的随机变量。如果分母为0，那么我们将Qt(a)定义为某个默认值，比如0。当分母趋于无穷时,大数定律,Qt(a)收敛于问∗(a)。我们称它为估算行动值的抽样平均方法，因为每个估计都是相关奖励样本的平均值。当然，这只是评估行动价值的一种方法，不一定是最好的。然而，现在让我们继续使用这个简单的估计方法，并转向如何使用估计来选择操作的问题。
最简单的操作选择规则是选择一个估计值最高的操作，即上一节定义的贪婪操作之一。如果有一个以上的贪心行为，那么就会以任意的方式在其中进行选择，可能是随机的。我们把这个贪心行为选择方法写成

在
。

= argmax
一个
Qt(a), 					(2.2)

其中argmaxa表示动作a，后面的表达式将为其最大化(同样，连接将被任意破坏)。贪婪的行为选择总是利用当前的知识以获得最大的即时回报;它没有花任何时间抽样明显的劣等行为，看看它们是否真的更好。一个简单的替代方法是贪婪行为的大部分时间,但每隔一段时间,说有小概率的ε,相反

从所有具有相等概率的动作中随机选择，独立于动作值估计。我们调用方法使用此near-greedy行动选择定则ε-greedy方法。这些方法的一个优点是,在极限的步数增加,每一个动作将取样无限次数,从而确保所有的Qt(a)收敛于问∗(a)。这当然意味着选择的概率收敛于最优行动大于1−ε,附近就是必然的。然而，这些只是渐进的保证，对这些方法的实际有效性只字未提。

练习2.1ε-greedy行动选择,两个动作和ε= 0.5的情况下,是什么
选择贪婪行为的概率? 					?


\section{的10-armed试验台}

粗略评估的相对有效性贪婪、ε-greedy行为价值方法,我们比较数值在一套测试问题。这是一组随机生成的k -武装土匪问题，k = 10。对于每一个土匪问题,如一个如图2.1所示,动作值,问∗(a),一个= 1,。10

行动

图2.1:来自10臂测试床的一个bandit问题示例。真正价值q∗(a)
这十个动作的每一个动作都是根据平均值为零和单位的正态分布来选择的
方差,然后选择实际奖励根据意思问∗(a)单位方差正态分布,这些灰色分布所显示。

2.3。10-armed测试平台 					29日



根据均值为0，方差为1的正态(高斯)分布进行选择。然后,当一个学习方法应用到这个问题在t时间步选择行动,实际的奖励,Rt,选择从一个正态分布的意思是问∗(At)和方差1。这些分布如图2.1所示。我们把这组测试任务称为10臂测试床。对于任何一种学习方法，我们都可以度量它的性能和行为，当它应用于某个土匪问题时，它会随着时间的推移而改进。这是一次跑步。在2000次独立运行中，我们重复了这个过程，每个运行都有不同的班迪特问题，我们得到了学习算法的平均行为的度量。
图2.2比较贪婪的方法与两个ε-greedy方法(ε= 0.01和ε= 0.1),如上所述,在10-armed试验台。所有的方法都使用样本平均技术来形成它们的动作值估计。上面的图表显示了期望报酬随着经验的增加而增加。贪心方法在一开始比其他方法改进得稍快一些，但随后在较低的级别趋于平稳。它的每步奖励只有1，而在这个测试台上，每步奖励的最高奖励是1.55。贪婪方法在长期运行中表现得非常糟糕，因为它

步骤

步骤

图2.2:平均性能ε-greedy行为价值方法10-armed试验台。
这些数据平均运行了2000次，并且有不同的盗版者问题。所有方法示例
平均作为他们的行动价值估计。

经常被卡在执行次优的动作中。下面的图显示贪婪方法只在大约三分之一的任务中找到了最优动作。在另外三分之二的情况下，其最优行为的初始样本令人失望，而且它从未返回。ε-greedy方法最终表现更好,因为他们继续探索和提高的机会识别最优行动。ε= 0.1方法探索更多,通常发现最优行动之前,但它从来没有选择的行动超过91%的时间。ε= 0.01方法改进的更慢,但最终会执行比ε= 0.1图所示方法在性能措施。还可以减少ε的随着时间的推移,试图得到最好的最高价和最低价。
ε-greedy在贪婪的优势取决于任务的方法。例如，假设奖励方差更大，比如10而不是1。与奖励吵着需要更多的探索发现最佳的行动,和ε-greedy方法应该表现更好的相对于贪婪的方法。另一方面，如果奖励方差为零，那么贪心方法尝试一次后就会知道每个行为的真实值。在这种情况下，贪婪方法实际上可能表现得最好，因为它很快就会找到最优动作，然后永远不会探索。但即使是在确定性的情况下，如果我们弱化了其他假设，也会有很大的优势。例如，假设bandit任务是非平稳的，也就是说，动作的真实值随着时间的变化而变化。在这种情况下，即使在确定性的情况下，也需要进行探索，以确保其中一个非贪婪行为没有改变，使其变得比贪婪行为更好。正如我们将在接下来的几章中看到的，非平稳性是在强化学习中最常见的情况。即使底层任务是固定的和确定性的，学习者也面临一组类似班迪特的决策任务，每一项任务都随着学习的进展和代理的决策策略的变化而变化。强化学习需要探索和开发之间的平衡。
练习2.2:土匪例子考虑一个k武装土匪问题，k = 4个动作，表示1、2、3和4。考虑申请这个问题一个赌博机算法使用ε-greedy行动选择,样本企业平均行为价值的估计,并初步估计Q1(a)= 0,所有。假设初始序列的行为和奖励是A1 = 1,R1 = 1,A2 = 2,R2 = 1,A3 = 2,R3 = 2,A4 = 2,R4 = 2,A5 = 3,R5 = 0。在这些时间步骤ε的情况可能发生,导致一个动作是随机选择的。这在什么时候发生过?在什么时间步可以做到这一点
可能发生呢? 					?
练习2.3在图2.2所示的比较中，从长期来看，哪种方法在累积奖励和选择最佳行为的概率方面表现最好?会有多好?定量地表达你的答案。?


\section{增量实现}

到目前为止我们已经讨论过的行动-价值方法都是作为观察到的奖励的样本平均值来估计行动值。现在我们要讨论的问题是，如何以一种计算效率很高的方式来计算这些平均数，特别是用恒定的内存

和持续per-time-step计算。
为了简化表示法，我们只关注一个动作。现在让Ri表示收到的奖励后第i个选择的行动,并让Qn表示其动作值的估计后选择n−1次,我们现在可以简单地写成
Qn。= R1 + R2 +···+ Rn−1
n−1
。

显然的实现方法是保存所有奖励的记录，然后在需要估计值时执行此计算。但是，如果这样做了，那么随着时间的推移，内存和计算需求会随着时间的推移而增长。每个额外的奖励将需要额外的内存来存储它，并需要额外的计算来计算分子的和。
正如您可能会怀疑的，这并不是真正必要的。我们很容易设计出增量公式来更新平均值，而处理每一个新的奖励都需要不断的小计算。给定Qn和第n个奖励Rn，可以计算所有n个奖励的新平均值

Qn + 1 = 1
n
n ?i = 1
国际扶轮


=
0 O

Rn +
n−1 ?i = 1
国际扶轮


这在n = 1时成立，对于任意Q1，得到Q2 = R1。这个实现只需要Qn和n的内存，并且每个新的奖励只需要很小的计算量(2.3)。
这个更新规则(2.3)是在本书中经常出现的形式。一般的形式是

NewEstimate←OldEstimate + StepSize

目标−OldEstimate

。(2.4)

表达式

目标−OldEstimate

是估计误差。它通过服用而减少
向“目标”迈出的一步。“虽然目标可能很吵，但它被认为是一个理想的移动方向。”例如，在上面的例子中，目标是第n个奖励。
注意，增量方法(2.3)中使用的步长参数(逐步大小)会随着时间步长而变化。在处理第n项奖励措施a时

方法使用步长参数1
n
。在本书中，我们表示步长参数
由α或更普遍,αt(a)。
完整的赌博机算法的伪代码使用增量计算样本平均值和ε-greedy行为选择下面的框所示。函数bandit(a)被假设采取一个动作并返回相应的奖励。




\section{跟踪非平稳问题}

到目前为止所讨论的平均方法适用于固定的赌博机问题，也就是说，适用于赌博机问题，在这种问题中，报酬概率不会随时间变化。如前所述，我们经常遇到有效的非平稳的强化学习问题。在这种情况下，给予近期奖励而不是长期奖励更有意义。最常用的一种方法是使用常量步长参数。例如,增量更新规则(2.3)的平均更新Qn n−1修改过去的奖励

Qn + 1。= Qn +α

Rn−Qn

, 					(2.5)

在步长参数α∈(0,1)是恒定的。这导致Qn+1是过去奖励和初始估计Q1的加权平均值:

Qn + 1 = Qn +α

Rn−Qn

Qn =αRn +(1−α)
=αRn +(1−α)[αRn−1 +(1−α)Qn−1]=αRn +(1−α)αRn−1 +(1−α)2 Qn−1
=αRn +(1−α)αRn−1 +(1−α)2αRn−2 +
n···+(1−α)−1αr1 +(1−α)nQ1
=(1−α)nQ1 +
n ?i = 1
α(1−α)n−iRi。 					(2.6)

我们称之为加权平均,因为重量的总和(1−α)n + ?n i = 1α(1−α)n−i = 1,你可以检查你自己。注意体重,α(1−α)n−我给奖励Ri取决于有多少奖励前,n−我观察到。数量1−α小于1,从而给国际扶轮的重量减少干预奖励数量的增加。事实上,体重指数衰减−1日根据指数α。(如果1−α= 0,那么所有的重量最后奖励,Rn,因为公约00 = 1)。因此，这有时被称为指数加权平均。
有时步长参数的变化很方便。让αn(a)表示步长参数用于处理接收到的奖励后n选择行动的。正如我们所指出的那样,选择αn(a)= 1 n结果样本均值的方法,这是保证收敛于真实的行动值由大数定律。当然收敛并不能保证对所有选择的序列{αn(a)}。随机逼近理论中一个众所周知的结果为我们提供了保证具有概率1的收敛性所需的条件:

∞吗?n = 1
αn(a)=∞
∞吗?n = 1
α2 n(a)<∞。 					(2.7)

第一个条件是保证步骤足够大，最终能够克服任何初始条件或随机波动。第二个条件保证最终步骤足够小，以确保收敛。
注意,两个样品平均情况下满足收敛条件,αn(a)= 1 n,但不是为步长参数不变的情况下,αn(a)=α。在后一种情况下，第二个条件没有满足，这表明估计数从不完全收敛，而是继续随最近收到的奖励而变化。正如我们上面提到的，这在非平稳环境中是可取的，有效的非平稳问题在强化学习中是最常见的。此外，满足(2.7)条件的步长参数序列通常收敛得很慢，或者需要进行大量的调优才能获得满意的收敛速度。虽然满足这些收敛条件的步长参数序列常用于理论研究，但在实际应用和实证研究中很少用到。
练习2.4如果步长参数、αn不是常数,那么估计Qn之前收到的奖励是一个加权平均权重不同,由(2.6)给出。类似地，一般情况下，每个事先奖励的权重是多少
对于(2.6)，根据步长参数序列? 					?
练习2.5(编程)设计并进行了一个实验来演示样本平均方法在非平稳问题上的困难。使用一种修改版的10-armed试验台所有q∗(a)开始平等,然后采取独立的随机漫步(比如通过添加一个正态分布与平均零和标准偏差0.01增加所有问∗(a)在每一个步骤)。准备图如图2.2所示的行为价值使用样本平均方法,逐步计算,和另一个行为价值的方法使用一个常数步长参数α= 0.1。使用
ε= 0.1和长跑,10000步说。 					?
\section{乐观的初始值}

到目前为止，我们讨论的所有方法在某种程度上都依赖于初始的行动值估计Q1(a)。在统计学的语言中，这些方法由于最初的估计而有偏差。样品平均方法,偏见消失一旦所有行动至少有一次被选中,但与常数α为方法,偏差是永久性的,但随着时间减少由(2.6)给出。在实践中，这种偏见通常不是问题，有时会很有帮助。缺点是初始估计实际上变成了用户必须选择的一组参数，如果只是将它们全部设置为0的话。好处是，它们提供了一种简单的方法，可以提供一些关于预期奖励水平的预先知识。
初始动作值也可以用作鼓励探索的简单方法。假设我们没有将初始动作值设置为0，而是将它们全部设置为+5。记得问∗(a)在这个问题上选择从一个正态分布均值为0,方差为1。因此，对+5的初步估计非常乐观。但这种乐观鼓励探索具有行动价值的方法。无论最初选择什么行动，奖励都小于初始估计;学习者转向其他行为，对所得到的奖励感到“失望”。结果是，所有操作都在值估计收敛之前进行了多次尝试。即使总是选择贪婪的行为，系统也会进行大量的探索。
图2.3显示了性能10-armed赌博机试验台的贪婪的方法使用Q1(a)= + 5,一。相比之下,也显示是一个ε-greedy方法Q1(a)= 0。起初，乐观的方法表现得更差，因为它探索得更多，但最终它表现得更好，因为它的探索随着时间的推移而减少。我们称这种鼓励勘探的技术为乐观初始值。我们认为它是一种简单的技巧，可以在固定的问题上非常有效，但它远不是鼓励探索的一种普遍有用的方法。例如，它不太适合非平稳问题，因为它的探索动力是内在的

戏剧的一步

图2.3:乐观的初始行动价值估计对10个武装试验台的影响。这两种方法都使用一个常数步长参数,α= 0.1。

\section{Upper-Confidence-Bound选择动作}

暂时的。如果任务发生变化，产生了对探索的新需求，那么这种方法就没有帮助了。实际上，任何以任何特殊方式集中于初始条件的方法都不太可能帮助处理一般的非平稳情况。时间的开始只发生一次，因此我们不应该过多地关注它。这种批评同样适用于抽样平均方法，它也将时间的开始视为一个特殊的事件，平均所有后续的奖励都是相同的权重。然而，所有这些方法都非常简单，其中的一种——或者是它们的简单组合——通常在实践中是足够的。在本书的其余部分，我们经常使用这些简单的探索技巧。
练习2.6:神秘的尖峰图2.3中显示的结果应该是相当可靠的，因为它们是超过2000个人的平均值，随机选择10个武装的土匪任务。那么，为什么乐观方法在曲线的早期会出现振荡和峰值呢?换句话说，是什么让这个方法表现得更好
更糟的是，在早期阶段? 					?
练习2.7:无偏恒步长技巧在本章的大部分内容中，我们使用样本平均来估计动作值，因为样本平均不会产生常数步长所产生的初始偏差(参见(2.6)的分析)。然而，样本平均值并不是一个完全令人满意的解决方案，因为它们可能在非平稳问题上表现不佳。是否有可能避免步长恒定的偏差，同时保留它们在非平稳问题上的优势?一种方法是使用步长。

βn。=α/ ōn, 					(2.8)
处理n奖励一个特定的行动,其中α> 0是一个传统的固定步长,和ōn是一个从0开始的跟踪:

ōn
。
= ōn−1 +α(1−ōn−1),与ōn≥0,0。= 0。 					(2.9)

在(2.6)中进行这样的分析，以表明Qn是指数加权的
平均没有最初的偏见。 					?


\section{pper-Confidence-Bound选择动作}

需要进行探索，因为对行动价值估计的准确性总是存在不确定性。贪婪的行为是那些目前看起来最好的行为，但是其他的一些行为实际上可能更好。ε-greedy选择动作迫使贪婪的行为进行审判,但不加区别地,几乎没有偏爱那些贪婪的或特别不确定。在非贪婪行为中，根据它们实际上是最优行为的潜力进行选择是更好的，要考虑到它们的估计离最大值有多近以及这些估计中的不确定性。这样做的一个有效方法是根据以下内容选择操作

ln t表示的自然对数(e≈2.71828数量必须提高到为了= t),Nt(a)表示,行动已经被选择的次数在时间t(2.1)(分母),和c > 0数量控制程度的探索。如果Nt(a) = 0，则a被认为是最大化行为。
上置信区间(UCB)作用选择的概念是平方根项是对a值估计的不确定性或方差的度量。因此，被最大值覆盖的量是作用a的可能真值的上界，c决定置信水平。每次选a时，不确定性大概会减少:Nt(a)增量，并且，当它出现在分母中，不确定项就会减少。另一方面，每一次除了a以外的动作，t增加，但Nt(a)不增加;因为t出现在分子中，所以不确定性估计值增加。自然对数的使用意味着随着时间的推移增加会越来越小，但是是无界的;所有操作最终都将被选中，但是值估计值较低的操作，或者已经频繁地被选中的操作，将会随着时间的推移而逐渐减少。
结果与UCB在10个武装试验床上显示在图2.4。UCB通常表现良好,如下所示,但是更困难比ε-greedy超越赌博机更一般的强化学习设置在这本书的其余部分。一个困难是处理非平稳问题;将需要比第2.5节中所介绍的方法更为复杂的方法。另一个困难是处理大的状态空间，特别是在使用函数逼近时，如本书第二部分所述。在这些更高级的设置中，UCB动作选择的想法通常是不实际的。


图2.4:10臂试验台UCB动作选择的平均性能。如图所示,
UCB一般执行比ε-greedy选择动作,除了在第一个k步骤,当它选择随机as-yet-untried行动之一。


练习2.8:UCB峰值在图2.4中UCB算法显示了在第11步上性能的明显峰值。这是为什么呢?注意，为了让你的回答完全令人满意，它必须解释为什么奖励在第11步增加，为什么它在随后的步骤减少。提示:如果c = 1，那么峰值就不那么明显了。?

\section{梯度赌博机算法}
到目前为止，在这一章中，我们已经考虑了估计动作值的方法，并使用这些估计来选择动作。这通常是一个很好的方法，但并不是唯一可行的方法。在本节中，我们考虑学习对每个动作a的数值偏好，我们将其命名为Ht(a)。偏好越大，采取行动的频率就越高，但这种偏好在回报方面没有解释。只有一种行动相对于另一种行动的相对偏好是重要的;如果我们将1000加到所有的操作偏好中，则不会对操作概率产生影响，操作概率是根据一个软最大值分布(即，吉布斯或波尔兹曼分布)如下:


在这里我们还引入了一个有用的新符号,πt(a),采取行动的概率在时间t。最初所有的行动偏好是相同的(例如,H1(a)= 0,所有a),所有的行动都有一个相同的概率被选中。
练习2.9表明，在两种行为的情况下，软最大值分布与经常用于统计和人工的logistic(或sigmoid)函数的分布相同
神经网络。 					?

基于随机梯度上升的思想，本文提出了一种自然学习算法。在每一步中，在选择动作并收到奖励Rt后，动作偏好更新为:


Ht + 1(在)。= Ht(在)+α?Rt−R̄t ? ?1−πt(在)?,
Ht + 1(a)。= Ht(一)−α?Rt−R̄t ?πt(a),对所有? =,(2.12)
α> 0是一个步长参数,̄t∈R是平均水平的回报,包括时间t,可以计算增量如2.4节所述(如果问题是不稳定或第2.5节)。R̄t项作为基线的奖励比较。如果奖励高于基线，那么未来接受At的概率就会增加，如果奖励低于基线，那么概率就会降低。非选择的动作则朝相反的方向移动。
图2.5显示了梯度班迪特算法对10臂测试床的一个变体的结果，在这个测试床中，真正的期望奖励是根据平均值为+4而不是0的正态分布(和以前一样，单位方差)来选择的。由于奖励基线条件的存在，所有奖励的上升对梯度班迪特算法没有任何影响，它会立即适应新的水平。但如果基线是省略了(也就是说,如果R̄t是常数零(2.12)),则性能会明显退化,如图。

80年



%
最佳的行动
60



40



20.




步骤

图2.5:平均梯度赌博机算法的性能与和没有回报基线10-armed试验台时问∗+ 4附近(a)被选择而不是接近于零。

\section{关联搜索(上下文盗匪)}
到目前为止，在这一章中，我们只考虑了非关联任务，也就是说，不需要将不同的操作与不同的情况关联起来的任务。在这些任务中，学习者要么试图在任务静止时找到一个最佳的动作，要么在任务不稳定时试图跟踪最佳的动作。然而，在一般的强化学习任务中有不止一种情况，目标是学习一种策略:从一种情况映射到在那种情况下最好的行动。为了为整个问题设置好舞台，我们简要地讨论了非关联任务扩展到关联设置的最简单方式。
举个例子，假设有几个不同的k武装的土匪任务，在每个步骤中你都随机地遇到其中的一个。因此，bandit任务从一个步骤随机地变化。在您看来，这是一个单一的、非平稳的k武装土匪任务，其真正的动作值从一步到一步随机变化。您可以尝试使用本章中描述的方法之一来处理非平稳性，但是除非真正的动作值变化缓慢，否则这些方法不会很好地工作。但是，现在假设，当为您选择一个bandit任务时，您会得到一些关于它的标识的独特线索(但不是它的操作值)。也许你面对的是一台真正的老虎机，当它改变其动作值时，它会改变显示的颜色。现在，您可以学习一种策略，将每个任务(用您看到的颜色表示)与面对任务时要采取的最佳行动相关联——例如，如果是红色，选择arm 1;如果是绿色，选择arm2。有了正确的策略，您通常可以做得比没有任何信息区分一个班迪特任务和另一个班迪特任务要好得多。
这是一个关联搜索任务的例子，之所以叫它，是因为它包含了尝试和错误学习来寻找最佳行为，以及这些行为与最佳情况的关联。联想搜索任务现在经常被称为文献中的上下文赌博机。关联搜索任务介于k -武装匪徒问题和充分强化学习问题之间。它们就像完整的强化学习问题因为它们涉及到学习一个策略，但是就像我们版本的k武装匪徒问题一样，每个动作只影响即时的奖励。如果行为被允许影响下一个情境以及奖励，那么我们就有了完全强化学习的问题。我们将在下一章中介绍这个问题，并考虑它在本书其余部分的影响。


练习2.10假设你面对一个2臂的土匪任务，它的真实动作值会随着时间的推移而随机变化。具体来说,假设,对于任何时间步,行动的真实值1和2分别为0.1和0.2的概率(情况下),0.5,0.9和0.8的概率0.5(B)。如果你不能告诉你的脸在任何一步,成功的最好的期望是什么,你可以实现,你应该如何实现?现在假设在每一步你都被告知你面对的是情形A还是情形B(尽管你仍然不知道真正的行为值)。这是一个关联搜索任务。在这方面，你能达到的最大成功期望是什么
任务，你应该怎么做才能完成? 					?

\section{总结}

在本章中，我们介绍了几种平衡勘探和开发的简单方法。ε-greedy方法选择随机的一小部分,而UCB选择确定性但实现勘探的方法巧妙地支持在每一步的行动到目前为止收到较少的样本。梯度班迪特算法不是估计动作值，而是估计动作偏好，并倾向于使用软最大值分布以一种分级的、概率的方式来估计更喜欢的动作。乐观地初始化估计的简单权宜之计导致甚至贪婪的方法也要进行大量的探索。
人们自然会问，哪种方法是最好的。虽然这是一个很难回答的问题，但我们可以在我们在本章中使用的十臂测试台上进行测试，并比较他们的表现。复杂的是它们都有一个参数;为了得到一个有意义的比较，我们必须考虑它们的性能作为参数的函数。到目前为止，我们的图已经显示了每个算法和参数设置的学习过程，为该算法和参数设置生成一个学习曲线。如果我们为所有的算法和所有的参数设置绘制学习曲线，那么图就会太复杂和拥挤，无法进行清晰的比较。相反，我们总结一个完整的学习曲线，用它的平均值除以1000步;这个值与学习曲线下的面积成正比。图2.6显示了本章中各种班迪特算法的度量，每个都作为自己的参数的函数，在x轴上的单个标度上显示。这种图称为参数研究。注意，参数值是根据两个因子的不同而变化的，并以对数尺度表示。还注意每种算法性能的特征逆变u型;所有算法在其参数的中间值上都表现得最好，既不太大也不太小。在评估

εαc Q0
图2.6:本章给出的各种土匪算法的参数研究。每一个点都是在特定的参数设定下，获得超过1000步的平均奖励。

作为一个方法，我们不仅要关注它在最佳参数设置中的表现，还要关注它对参数值的敏感性。所有这些算法都是相当不敏感的，在一个数量级的参数值范围内表现良好。总的来说，在这个问题上，UCB似乎表现最好。
尽管这些方法很简单，但我们认为这一章介绍的方法可以被认为是最先进的。有更复杂的方法，但是它们的复杂性和假设使它们不适合我们真正关注的完全强化学习问题。从第五章开始，我们提出了解决全强化学习问题的学习方法。
虽然本章探讨的简单方法可能是目前我们所能做的最好的方法，但对于平衡勘探和开发的问题，这些方法还远远不能完全令人满意的解决办法。
在k -armed bandit问题中，平衡勘探和开发的一个很好的方法是计算一种称为Gittins指数的特殊行为值。在某些重要的特殊情况下，这种计算是可处理的，并直接导致最优解，尽管它确实需要对可能的问题的先验分布有完整的了解，我们通常认为这是不可用的。此外，这种方法的理论和计算可追溯性似乎都不能推广到我们在本书其余部分中考虑的完全强化学习问题。
gittin -index方法是贝叶斯方法的一个实例，它假设在操作值上有一个已知的初始分布，然后在每一步之后更新这个分布(假设真正的操作值是平稳的)。通常，更新计算非常复杂，但是对于某些特殊的分布(称为共轭先验)，它们很容易。一种可能性是，根据每个步骤的后验概率选择最佳动作。这种方法，有时称为后验抽样或汤普森抽样，通常与我们在本章中所介绍的最佳无分布方法相似。
在贝叶斯条件下，甚至可以计算出勘探和开发之间的最优平衡。一个人可以计算任何可能的行动的概率每个可能的即时奖励和结果的后验分布超过行动值。这种不断发展的分布成为问题的信息状态。给定一个范围，比如说1000步，你可以考虑所有可能的行动，所有可能的结果奖励，所有可能的下一步行动，所有的下一个奖励，等等所有的1000步。在假设的前提下，每一个可能的事件链的回报和概率都是可以确定的，人们只需要选择最好的。但是可能性之树生长得非常快;即使只有两个动作和两个奖励，这棵树也会有22000片叶子。准确地进行这种巨大的计算通常是不可行的，但也许可以有效地近似。这种方法将有效地将土匪问题转化为完全强化学习问题的一个实例。最后，我们可以使用近似强化学习方法，如本书第二部分所介绍的方法，来接近这个最优解。但这是一个研究的话题，超出了这本入门书的范围。

练习2.11(编程)使图与练习2.5中列出的非平稳情况的图2.6相似。包括constant-step-sizeε-greedy算法与α= 0.1。使用20万步的运行，作为每个算法和参数设置的性能度量，使用最后10万步的平均奖励。?


书目的和历史的言论

2.1在统计学、工程、心理学等方面对土匪问题进行了研究。在
统计，赌博机问题属于“经验的顺序设计”，由汤普森(1933,1934)和罗宾斯(1952)介绍，贝尔曼(1956)研究。Berry和Fristedt(1985)从统计学的角度对赌博机问题进行了广泛的处理。纳伦德拉(Narendra and Thathachar)(1989)从工程的角度来处理土匪问题，很好地讨论了针对土匪的各种理论传统。在心理学上，班迪特问题在统计学习理论中发挥了作用(如Bush和Mosteller, 1955;埃斯蒂斯,1950)。
在启发式搜索文献中经常使用“贪心”一词(例如Pearl, 1984)。在控制工程中，勘探和开发之间的冲突被称为识别(或估计)和控制之间的冲突(例如，Witten, 1976b)。Feldbaum(1965)将其称为双控问题，指在不确定性控制系统时需要同时解决识别和控制两个问题。在讨论遗传算法的各个方面时，Holland(1975)强调了这种冲突的重要性，并将其称为需要利用和需要新信息之间的冲突。

2.2针对我国k武装土匪问题提出的行动价值方法
Thathachar和Sastry(1985)。在学习自动机文献中，这些通常被称为估计算法。动作值一词源于Watkins(1989)。首先使用ε-greedy方法也可能被沃特金斯(1989,第187页),但这个想法非常简单,一些早期使用可能。

2.4-5该材料属于随机迭代算法的一般标题，
这些都被Bertsekas和Tsitsiklis(1996)所涵盖。

2.6 Sutton(1996)在增强学习中使用了乐观初始化。

2.7早期的工作是使用估计的上置信值来选择动作
由Lai and Robbins(1985)、Kaelbling (1993b)和Agrawal(1995)完成。我们在这里展示的UCB算法在文献中被称为UCB1，最初由Auer、Cesa-Bianchi和Fischer(2002)开发。

2.8梯度土匪算法是基于梯度的强化的一个特例
由Williams(1992年)引入的学习算法，后来发展成为我们在本书后面所讨论的针对演员和政策梯度的算法。我们在这里的发展受到巴拉曼·拉文德拉(个人)的影响

沟通)。格林史密斯(Greensmith)、巴特莱特(Bartlett)、巴克斯特(Baxter, 2002, 2004)和迪克(Dick)(2015)提供了关于基线选择的进一步讨论。萨顿(1984)对这类算法进行了早期的系统研究。
动作选择规则(2.11)的“软最大值”一词源于《笼头》(1990)。这条规则似乎是卢斯(1959)首先提出的。

2.9提出了联想搜索这一术语及其对应的问题
Barto, Sutton和Brouwer(1981)。联合强化学习这个术语也被用于联合搜索(Barto和Anandan, 1985)，但是我们更倾向于将这个术语作为充分强化学习问题的同义词(如Sutton, 1984)。(正如我们所指出的，现代文学也用“语境土匪”这个词来描述这个问题。)我们注意到桑代克的效应定律(在第一章中引用)描述了关联搜索，它指的是情景(状态)和行为之间的关联关系的形成。根据操作性条件作用或工具性条件作用的术语(如斯金纳，1938)，区别性刺激是一种指示特定强化偶然性存在的刺激。用我们的话说，不同的辨别刺激对应不同的状态。

2.10 Bellman(1956)是第一个展示如何使用动态编程的
在贝叶斯公式中计算勘探开发的最优平衡问题。Gittins索引方法是由Gittins和Jones(1974)提出的。Duff(1995)展示了如何通过强化学习来学习赌博机问题的Gittins指数。Kumar(1985)的调查很好地讨论了贝叶斯方法和非贝叶斯方法。“信息状态”一词来源于关于部分可观测的MDPs的文献;见,例如,洛夫乔伊(1991)。
其他的理论研究集中在探索的效率上，通常表现为算法能以多快的速度接近最优的决策策略。确定搜索效率的一种方法是适应增强学习一种监督学习算法的样本复杂度的概念，这是算法在学习目标函数时需要的训练实例数量。对一个增强学习算法的探索样本复杂性的定义是算法没有选择接近最佳动作的时间步骤(Kakade, 2003)。Li(2012)在研究增强学习中探索效率的理论方法时，讨论了这一方法和其他几种方法。Russo、Van Roy、Kazerouni、Osband和Wen(2018)对汤普森采样进行了全面的现代处理。
