\chapter{第十二章 资格的痕迹}

\begin{summary}
	资格追索是强化学习的基本机制之一。例如,在受欢迎的TD(λ)算法,λ是指合格的使用痕迹。几乎任何时间差异(TD)方法，如Q-learning或Sarsa，都可以与合格跟踪相结合，以获得更通用的方法，从而可以更有效地学习。
	资格跟踪统一和推广TD和蒙特卡罗方法。TD方法增强与资格的痕迹时,产生一个家庭的方法生成一个频谱一端蒙特卡罗方法(λ= 1)和一步TD方法另(λ= 0)。中间是中间方法，它们通常比两个极端方法都要好。资格跟踪还提供了一种在线实现蒙特卡罗方法的方法，以及在没有发作的连续问题上。
	当然，我们已经看到了一种统一TD和蒙特卡罗方法的方法:第7章的n步TD方法。除了这些之外，合格跟踪还提供了一个优雅的算法机制，具有重要的计算优势。短期记忆的机制是一个向量,资格跟踪zt型∈Rd,相似之处长期体重向量wt∈Rd。粗略的想法是,当一个组件wt参与生产的估计价值,那么相应的组件zt型骤然上升,然后开始消失。如果在跟踪返回到0之前出现了一个非零的TD错误，那么就会在wt的那个组件中进行学习。痕迹减退参数λ∈[0,1]决定跟踪的速度下降。
	与n步方法相比，资格跟踪的主要计算优势是只需要一个跟踪向量，而不是最后n个特征向量的存储。学习也会不断地、均匀地发生，而不是被延迟，然后在这一集的结尾出现。此外，学习可以在遇到状态后立即发生并影响行为，而不是被延迟n步。
	合格跟踪说明，学习算法有时可以用不同的方式实现，以获得计算优势。许多算法最自然地被表述为状态值的更新，它基于在未来多个时间步骤中跟随该状态的事件。例如，蒙特卡罗方法(第5章)基于所有未来奖励更新状态，n步TD方法(第7章)
	
	更新基于未来n个奖励和状态n步。这种基于对更新状态的展望的公式称为前瞻性视图。Forward视图的实现总是有些复杂，因为更新依赖于以后的东西，而这些东西当时是不可用的。然而，正如我们在本章中所展示的，使用一个使用当前TD错误的算法来实现几乎相同的更新(有时甚至是完全相同的更新)，使用一个合格跟踪来回顾最近访问的状态。这些观察和实现学习算法的替代方法称为向后视图。后向视图、前向视图和后向视图之间的转换，以及它们之间的相等性，可以追溯到时间差异学习的引入，但自2014年以来变得更加强大和复杂。这里我们介绍现代观点的基础。
	和往常一样，我们首先充分发展状态值和预测的思想，然后将它们扩展到动作值和控制。我们首先针对政策上的情况开发它们，然后将它们扩展到政策外的学习。我们的处理特别注意线性函数逼近的情况，其结果具有更强的适用性。所有这些结果也适用于表列和状态聚集情况，因为这些是线性函数逼近的特殊情况。
		
\end{summary}


\section{的λ-return}

在第7章中，我们定义了n步返回作为前n个奖励的总和加上n个步骤中所达到的状态的估计值，每一个都适当地折现(7.1)。对于任何参数化函数近似器，这个方程的一般形式是
Gt:t + n。= Rt + 1 +γRt + 2 +···+γn−1 Rt + n +γnv̂(wt圣+ n + n−1),0≤t≤t−n、v(12.1)̂(s,w)的近似值s给定的权向量w(第九章),和t是一集的时间终止,如果任何。我们在第7章指出,每个n-step回报,对n≥1,是一个有效的更新表格学习更新的目标,就像它是一个近似SGD学习更新,如(9.7)。
现在我们注意到，一个有效的更新不仅可以针对任何n步返回，还可以针对不同ns的任何n步返回的平均值。例如，可以对目标进行更新，目标是两步返回的一半和四步返回的一半:
1
2 gt:t + 2 +
1
2 gt:t + 4。任何n步返回的集合都可以这样平均，即使是无穷大
设置，只要组件返回的权重为正，sum为1。复合返回具有与单个n步返回相似的错误减少属性(7.3)，因此可以使用有保证的收敛属性构造更新。平均产生了大量新的算法。例如，一个人可以平均一步和无限步返回来获得另一种与TD和蒙特卡罗方法相关的方法。原则上，我们甚至可以使用DP更新来平均基于经验的更新，以获得基于经验和基于模型的方法的简单组合(参见第8章)。
平均简单组件更新的更新称为复合更新。复合更新的备份关系图由每个组件更新的备份关系图组成，每个组件更新在其上面有一条水平线，下面是权重部分。

1 2
例如，在本节开始时提到的案例的复合更新，混合了两步返回的一半和四步返回的一半，将图显示在右边。复合更新只能在组件更新时间最长的时候完成。例如，右边的更新只能在t+4时刻完成t+4时刻形成的估计值。一般来说，我们希望限制最长组件更新的长度，因为更新中相应的延迟。
TD(λ)算法可以被理解为一个特定的方式平均n-step更新。这种平均包含所有n-step更新,每个加权比例λn−1(λ∈[0,1]),和规范化的因素1−λ,确保权重之和为1(图12.1)。对回归结果更新,称为λ-return,定义基于状态的形式


Gλt
。=(1−λ)
∞吗?n = 1
λn−1 gt:t + n。 					(12.2)

图12.2进一步说明了加权λ-return n-step返回的顺序。一步返回给定的最大重量,1−λ;两步返回都将最大重量(1−λ)λ;三步返回给出的重量(1−λ)λ2;等等。权重λ褪色的每个额外的步骤。在达到一个终端状态后，所有后续的n阶返回都等于常规返回Gt. If


TD(λ)
 
(1−λ)λ2

tλT−−1
−1

圣RT

= 1


图12.1:备份双字母组合TD(λ)。如果λ= 0,那么整个更新减少了它的第一个组件,一步TD更新,而如果λ= 1,然后整体更新减少其最后一个组件,蒙特卡洛更新。


图12.2:加权给出λ-return每个n-step回报。

我们想要，我们可以将这些终止后的条件与主要的和，产生分离。


Gλt =(1−λ)
T−−1 ?n = 1
λn−1 gt:t + n +λT t−−1 gt, 					(12.3)

如图所示。这个方程使它清晰当λ= 1。在这种情况下，主要的和为零，剩余的项减少到常规的回报。因此,对于λ= 1,更新根据λ-return蒙特卡罗算法。另一方面,如果λ= 0,然后λ-return减少Gt:t + 1,一步返回。因此,对于λ= 0,更新根据λ-return是一个一步TD方法。
练习12.1返回可以递归地写在第一个奖励和本身一步后(3.9),所以可以λ-return。导出类似的递归关系
从(12.2)和(12.1)。 					?
12.2运动参数λ特征速度指数加权图12.2脱落,因此多远未来λ-return算法看起来在决定它的更新。但利率因素如λ有时是一个尴尬的方式描述的速度衰减。出于某些目的，最好指定一个时间常数或半衰期。λ有关的方程和半衰期,τλ,加权序列的时间降至初始值的一半吗??

现在,我们可以定义我们的第一个学习算法基于λ-return:离线λ-return算法。作为一种离线算法，它不会改变集内的权重向量。的最后一集,整个序列的离线更新是根据我们平时semi-gradient规则,使用λ-return作为目标:


wt + 1
.
= wt +α

wt Gλt−v̂(圣)

wt∇v̂(圣),t = 0,。T−1。(12.4)

λ-return给我们的另一种方式之间移动顺利蒙特卡罗和一步TD方法可以与n-step引导方式发达在第7章。在那里，我们评估了19个状态随机行走任务的有效性(例子7.1，第144页)。图12.3显示了离线λ-return算法的性能在这个任务,n-step方法(重复从图7.2)。前面描述的实验一样,除了λ-return算法我们不同的λ代替n。使用的性能测量是正确估计之间的均方根误差和估计的每个状态测量值的最后一集,平均在第十集和19个州。注意,离线λ-return算法的总体性能是可比的n-step算法。在这两种情况下我们得到最佳性能的中间值引导参数,n为离线λ-return n-step方法和λ的算法。

0.4
最后是RMS错误。
这一集的第一集
0.35 10集

0.3



0.25

0.4 - 0.2 0 					0.8 - 0.6 					1

αα
 
0.4 - 0.2 0 					0.8 - 0.6 					1




图12.3:19个随机游走的结果(例如7.1):离线λ-return的性能
与n步TD方法并行的算法。在这两种情况下，中间值
引导参数(λ或n)表现最好。结果与离线λ-return算法
更能最好的α和λ的值,和在高α。



到目前为止，我们一直采用的方法就是我们所说的学习算法的理论观点。对于每一个访问过的州，我们都期待着所有未来的奖励，并决定如何最好地将它们结合起来。我们可以想象自己在状态流中前进，从每个状态中期待确定更新，如图12.4所示。在展望并更新一个状态之后，我们继续下一个状态，再也不必使用前一个状态。另一方面，对未来的状态进行反复的观察和处理，每次都是从它们前面的每个有利点开始。

图12.4:正向视图。我们通过展望未来的奖励和状态来决定如何更新每个状态。

\section{TD(λ)}
TD(λ)是一种最古老和最广泛使用的强化学习算法。这是第一个将正式的关系展示在一个更有理论的前视图和一个使用资格跟踪的更加计算一致的后视图之间的算法。在这里,我们将展示经验,它接近离线λ-return算法在前一节中提出的。
TD(λ)改善离线λ-return算法在三个方面。首先，它更新了每一集的每一步的重量矢量，而不仅仅是在最后，因此它的估计可能会更快更好。其次，它的计算在时间上是平均分布的，而不是在这集的结尾。第三，它可以应用于持续的问题而不仅仅是偶发的问题。在本节中,我们讨论TD的semi-gradient版本(λ)函数近似。
与资格跟踪是一个矢量函数近似zt型∈Rd与相同数量的组件作为权向量wt。而权向量是一个长期记忆,积累在系统的生命周期,资格跟踪是一个短期记忆,通常持续时间少于一集的长度。资格追踪协助学习过程;它们唯一的结果就是它们影响了权重向量，然后权重向量决定了估计值。
在TD(λ)资格跟踪矢量集开始时初始化为零,增加在每个时间步的梯度值,然后逐渐消退γλ:
z
−1
。= 0,
zt型
.
=γλzt−1 +∇v̂(St,wt),0≤t≤t, 					(12.5)
γ贴现率和λ参数介绍了在前面的小节中,我们从今以后称之为痕迹减退参数。资格跟踪跟踪哪些组件的权向量有贡献,积极的还是消极的,最近的状态估值,“最近”γλ的定义。(回想一下,在线性函数近似,∇v̂(St,wt)只是一个特征向量,xt,在这种情况下,资格跟踪矢量就是一笔过去,褪色,输入向量。)这一痕迹被认为是用来表示权重向量的每个组成部分是否有资格进行学习

加强事件发生时的变化。我们所关注的增强事件是时刻一步的TD错误。对状态值预测的TD误差是。

δt。= Rt + 1 +γv̂(wt)圣+ 1−v̂(St,wt)。 					(12.6)
在TD(λ),更新权向量在每个步骤与标量TD错误和向量资格跟踪:

wt + 1
.
= wt +αδtzt。 					(12.7)
 
 
图12.5:落后的或机械的TD(λ)。每次更新都取决于当前的更新
TD错误结合了过去事件的当前合格跟踪。

TD(λ)是面向落后。在每个时刻，我们查看当前的TD错误，并根据该状态在当时对当前资格跟踪的贡献大小，将其向后分配到每个先前状态。我们可以想象自己沿着状态流运行，计算TD错误，并将它们返回到先前访问的状态，如图12.5所示。当TD错误和跟踪一起出现时，我们得到(12.7)给出的更新，更改过去状态的值，以便在将来再次发生。
为了更好地理解落后的TD(λ),考虑在不同的λ值会发生什么。如果λ= 0,那么通过(12.5)跟踪t就是梯度值对应于圣因此TD(λ)更新(12.7)减少一步semi-gradient TD更新在第9章(在表格的情况下,简单的道明规则(6.2))。这就是为什么这个算法叫做TD(0)根据图12.5,TD(0)是只有当前状态之前的一个状态被TD错误改变的情况。λ值的增大,但仍然λ< 1,之前更多的状态改变,但每多暂时遥远的状态变化少,因为相应的资格跟踪比较小,所显示的人物。我们说，早期的州对TD错误的信任更少。
如果λ= 1,那么信贷给美国早些时候下跌只有γ每一步。这是实现蒙特卡洛行为的正确方法。例如,记住,TD错误,δt,包括一个尚未完全的Rt + 1。通过这个回k步骤需要打折,像任何奖励返回,由γk正是下降资格跟踪实现的。如果λ= 1和γ= 1,那么资格痕迹不随时间衰减。在这种情况下，该方法的行为就像蒙特卡罗方法，用于不打折扣的、周期性的任务。如果λ= 1,该算法也被称为TD(1)。
TD(1)是一种实现蒙特卡罗算法的方法，比前面提到的算法更通用，并且显著增加了它们的适用范围。早期的蒙特卡罗方法仅限于情景任务，而TD(1)也可以应用于贴现连续任务。此外，TD(1)可以增量地在线执行。蒙特卡罗方法的一个缺点是，在一段情节结束之前，他们什么也学不到。例如，如果蒙特卡罗控制方法采取的行动产生的报酬非常低，但没有结束这一集，那么在这一集中，代理重复这一行动的倾向将不会减弱。另一方面，在线TD(1)从不完整的正在进行的事件中学习了n步的TD方法，其中n步一直到当前的步骤。如果在某一集中发生了异常好的或坏的事情，基于TD(1)的控制方法可以立即学习并改变他们在同一集中的行为。
它是揭示重温19个随机漫步的例子(例如7.1)看看TD(λ)在离线λ-return近似算法。这两种算法的结果如图12.6所示。对于每一个λ值,如果选择α最优(或更小),那么这两个算法执行几乎相同。如果选择α比是最优的,然而,然后λ-return算法只是有点糟糕而TD(λ)是更糟,甚至可能是不稳定的。这不是灾难性的TD(λ)在这个问题上,因为这些更高的参数值并不是一个想要使用,但对于其他的问题可以是一个明显的弱点。

\section{n-step截断λ-return方法}

最后是RMS错误。
这一集的前10集
0.45




0.35 - 0.4




0.3



0.25



α
0.4 - 0.2 0 					0.8 - 0.6 					1
295年
 
0.4 - 0.2 0 					0.8 - 0.6 					1

α
图12.6:19个随机游走的结果(例如7.1):TD(λ)旁边的性能
的离线λ-return算法。这两种算法在低的情况下几乎完全相同。
(小于最优)α值,但TD(λ)更糟糕的是在高α值。


线性TD(λ)已经证明了在政策趋同的情况下,如果减少步长参数随着时间的推移,根据通常的条件(2.7)。正如9.4节中所讨论的,融合不是重量最小误差向量,但到附近的权向量,取决于λ。中提供的绑定解决方案质量,部分(9.14)现在可以推广到申请任何λ。对于持续打折的情况，
 
即渐近误差不超过1−−1γλγ*尽可能最小的错误。λ趋于1,绑定方法误差最小(松开λ= 0)。然而在实践中,λ= 1通常是最穷的选择,将在后面说明,如图12.14。
练习12.3一些洞察TD(λ)如何密切近似离线λ-return算法可以被看到了后者的误差项(12.4)(在括号中)可以写成TD错误(12.6)的总和为单个固定w。这显示,(6.6)的模式后,使用递归关系λ-return你获得的
12.1运动。 					?
练习12.4利用前面的锻炼的结果表明,如果重量更新一集在每一步计算但不实际使用改变权重(w保持固定的),然后TD的总和(λ)的重量更新将是相同的
离线λ-return之和算法的更新。 					?

离线λ-return算法是一个重要的理想,但这是有限的效用,因为它使用λ-return(12.2),这是未知的,直到最后一集。在

持续的情况下,λ-return在技术上不知道,因为它取决于n-step返回任意大n,从而在任意遥远未来的回报。然而,依赖变得弱longer-delayed奖励,下降γλ延迟的每一步。因此，一个自然的近似值是在一些步骤之后截断序列。我们现有的n步回报的概念提供了一种自然的方法，在这种方法中，缺失的回报被估计值所取代。
一般来说,我们定义了截断λ-return时间t,给定数据只是一些后来地平线,h,


Gλt:h
。=(1−λ)
h t−−1 ?n = 1
λn−1 gt:t + n +λh t−−1 gt:h,h≤0≤t < t(12.9)


与λ-return如果你把这个方程(12.3),很明显,地平线h是扮演相同的角色以前由T,终止的时间。而在λ-return有剩余重量给传统的Gt的回报,这是最长的可用n-step回报,Gt:h(图12.2)。
截断λ-return立即产生一系列n-stepλ-return算法类似于n-step第七章的方法。在所有这些算法,更新延迟了n步,只考虑第一个n奖励,但现在所有k-step返回包含1 k≤≤n(而早期n-step算法只用n-step返回),加权几何如图12.2所示。在州值的情况下,这个家庭的算法称为截断TD(λ),或运输大亨(λ)。复合备份图,如图12.7所示,类似于TD(λ)(图12.1),除了最多最长的组件更新n步而不是总是所有的方法
 
λn−1

图12.7:备份图截TD(λ)。

一集结束。运输大亨(λ)被定义为(cf(9.15)):

wt + n
.
=ωt + n−1 +α

Gλt:t + n−v̂(St,wt + n−1)

∇v̂(St,wt + n−1),0≤t < t。

这种算法可以有效地实现，这样每一步的计算就不会与n成比例(当然内存也必须如此)。n-step TD方法,没有更新在第n−1每一集的时间步骤,和n−1额外的更新在终止。有效的实现依赖于事实k-stepλ-return可以写一样


Gλt:t + k = v̂+(圣,wt−1)
t + k−1 ?我= t
(γλ)我−tδ吗?我, 					(12.10)

在哪里

δ?t
。= Rt + 1 +γv̂(wt)圣+ 1−v̂(St,wt−1)。

练习12.5在这本书中(通常在练习中)我们已经确定，如果值函数保持不变，返回可以写成TD错误的和。为什么
(12.10)是另一个例子吗?证明(12.10)。 					?

\section{重新更新:在线λ-return算法}

选择截断参数n截断TD(λ)包括一个权衡。n应该大,这样方法密切接近离线λ-return算法,但它也应该小,这样可以更快的更新,可以影响行为。我们能两者兼得吗?是的，原则上我们可以，尽管代价是计算复杂度。
其思想是，在每次收集新的数据增量时，都要返回并重新执行当前事件开始以来的所有更新。新的更新将比您以前所做的更好，因为现在它们可以考虑到time step的新数据。即更新总是朝着一个n-step截断λ-return目标,但他们总是使用最新的地平线。在每一段情节中，你都可以使用稍微长一点的视野，得到稍微好的结果。记得截断λ-return定义(12.9)

Gλt:h
。=(1−λ)
h t−−1 ?n = 1
λn−1 gt:t + n +λh t−−1 gt:h。

如果计算复杂度不是问题，那么让我们来看看理想情况下如何使用这个目标。本集以时间0的估计值开始，使用前一集末尾的权值w0。当数据视界扩展到第一步时，学习就开始了。在第0步给出的估计值的目标是，给定数据到horizon 1，只能是单步返回G0:1，其中包括R1和boot。
w0 v̂(S1)。注意,这正是Gλ0:1,总和的第一项方程的退化为零。使用这个更新目标，我们构建w1。然后，在将数据层推进到步骤2之后，我们要做什么?我们有新的数据形式的R2和S2,以及新的w1,所以现在我们可以构建一个更好的更新目标Gλ0:2第一更新从S0以及更好的更新目标Gλ1:2第二更新从S1。使用这些改进的目标，我们在S1和S2重新执行更新，从w0开始，生成w2。现在我们把地平线推进到第3步，重复，一路返回来产生3个新的目标，重新开始从原来的w0到w3的所有更新，等等。每一次视界被推进，所有的更新都从w0开始使用来自前一个视界的权重向量重新完成。
这个概念算法涉及到在这一集的多次传递，在每一个视界上，每一个都产生不同的权重向量序列。为了清晰地描述它，我们必须区分在不同视野下计算的权重向量。让我们使用wh t表示权重用于生成序列中的值在时间t地平线h。第一个权向量wh 0在每一个序列是继承自上一次(所以他们是相同的h),最后权向量wh h每个序列定义了最终的权向量序列的算法。在最终的视界h = T时，我们得到最终的权值wT T，它将被传递，形成下一集的初始权值。有了这些公约，前一段所述的前三个顺序可以明确地给出:

h = 1: w1
.
= 0 +α

Gλ0:1−v̂(S0 w1 0)

∇v̂(S0,w1 0),

h = 2: w2 1
.
= w2 0 +α

Gλ0:2−v̂(S0 w2 0)

∇v̂(S0 w2 0),
w2 2
.
= w2 1 +α

Gλ1:2−v̂(S1 w2 1)

∇v̂(S1 w2 1),

h = 3: w3 1
.
= w3 0 +α

Gλ0:3−v̂(S0,w3 0)

∇v̂(S0,w3 0),
w3 2
.
= w3 1 +α

Gλ1:3−v̂(S1,w3 1)

∇v̂(S1,w3 1),
w3 3
.
= w3 2 +α

Gλ2:3−v̂(S2,w3 2)

∇v̂(S2,w3 2)。
更新的一般形式是

wh t + 1
.
= wh t +α

Gλt:h−v̂(St,wh t)

∇v̂(圣,wh t),0≤t < h≤t。
此更新与wt一起
。=ωt定义在线λ-return算法。
完全在线,在线λ-return算法确定一个新的权向量wt每一步t在一集,仅使用信息在时间t,其主要缺点是计算复杂,经过集的一部分经历了到目前为止在每一步。注意,它比离线λ-return严格更复杂的算法,通过的所有步骤时终止,但不做任何更新在一集。作为回报，在线算法可以被期望比离线算法表现得更好，不仅在事件进行更新时，而离线算法没有更新时，而且在事件结束时，因为

权向量用于引导(Gλt:h)有更多的信息更新。如果仔细查看图12.8，就可以看到这种效果，图12.8比较了19状态随机漫步任务中的两种算法。
 
0.5


最后是RMS错误。
这一集的前10集
0.45




0.35 - 0.4




0.3



0.25



α
0.4 - 0.2 0 					0.8 - 0.6 					1
 
0.4 - 0.2 0 					0.8 - 0.6 					1

α
图12.8:19个随机游走的结果(例如7.1):在线和离线λ-return算法的性能。这里的性能度量是在事件结束时的VE，这应该是离线算法最好的情况。尽管如此，在线算法的表现还是相当不错。相比之下,λ= 0线是相同的两种方法。


\section{真实在线TD(λ)}
在线λ-return算法只是提出了目前表现最好的temporal-difference算法。它是一个理想的在线TD(λ)只有接近。了,然而,网上λ-return算法非常复杂。是否有一种方法可以将这种前视图算法转化为使用合格跟踪的有效的向后视图算法?事实证明,确实有一个精确的计算的实现在线λ-return算法的线性函数近似。这个实现被称为真正的在线TD(λ)算法,因为它是“真实”的理想在线λ-return算法比TD(λ)算法。
真在线TD的推导(λ)现在这里有点太复杂(见下一节和论文的附录van Seijen et al .,2016)但是它的策略很简单。权重向量的序列产生的在线λ-return算法可以排成一个三角形:

w0 0
w1 0 w
1 1
w2 0 w
2
1 w
2 - 2
w3 0 w
3
1 w
3
2 w
3 3

wT 0 w
T
1 w
T
2 w
T
···w
T T

这个三角形的一行在每个时间步上产生。结果是对角线上的权向量，wt t，是唯一真正需要的。第一个，w0 0，是这一集的初始权向量，最后一个，wT，是最终的权向量，每一个权向量，在这个过程中，wT，在更新的n步返回中起作用。在最后的算法中，对角权向量被重新命名，没有上标wt
。策略就是找到一种紧凑、高效的计算方法。
每个wt都是从1开始的。如果这样做,对于线性情况下的v̂(s,w)= w ? x(s),然后到达真正的在线TD(λ)算法:

wt + 1
.
= wt +αδtzt +α

w ?w t xt−

t−1 xt
(zt型−xt),
我们在哪里使用了速记xt
。= x(圣),δt被定义为在TD(λ)(12.6),和zt型
定义为

zt型
。=γλzt−1 +

1−αγλz吗?t−1 xt

xt。 					(12.11)
该算法已被证明产生完全相同的权重向量序列,wt,0≤t≤t,随着在线λ-return算法(van Seijen et al . 2016年)。因此，图12.8左边的随机行走任务的结果也是该任务的结果。然而，现在的算法要便宜得多。真正的内存需求在线TD(λ)与常规TD(λ)完全相同,而每一步计算增长了约50\%(还有一个内积eligibility-trace更新)。总的来说,每一步的计算复杂度是O(d),一样的TD(λ)。在方框中给出了完整算法的伪代码。


资格跟踪(12.11)中使用真实的在线TD(λ)被称为荷兰跟踪区别于跟踪(12.5)用于TD(λ),这被称为一个积累痕迹。早期的工作通常使用第三种称为替换跟踪的跟踪，这种跟踪只定义为表格或者二进制特征向量，比如由tile编码生成的特征向量。替换跟踪根据特征向量的分量是1还是0来定义:

1如果xi,t = 1γλzi,否则t−1。
(12.12)


如今，我们发现，将痕迹替换为荷兰痕迹的粗略近似值，这在很大程度上取代了它们。荷兰痕迹通常比替代痕迹表现得更好，并且有更清晰的理论基础。积累的痕迹仍然对非线性函数近似，荷兰的痕迹是不可用的。

\section{荷兰语在蒙特卡洛学习}

尽管从历史上看，资格跟踪与TD学习有着密切的联系，但实际上它们与此无关。事实上，资格的痕迹甚至在蒙特卡罗学习中也会出现，正如我们在本节中所展示的。我们展示了线性MC算法(第9章)，作为一种前向视图，可以用荷兰式跟踪来推导出一种等价但计算上更便宜的后向视图算法。这是我们在这本书中明确展示的前后观点的唯一等价性。它给的一些风味等价的证明真正的在线TD(λ)和在线λ-return算法,但要简单得多。
梯度蒙特卡罗预测算法的线性版本(第202页)进行了如下的更新，每一集的时间步骤都有一个更新:


wt + 1
.
= wt +α

G−w ?t xt

xt,0≤t < t。 					(12.13)

为了简化这个示例，我们在这里假设返回的G是在本集末尾收到的单个奖励(这就是为什么G没有被时间下标)，并且没有折扣。在这种情况下，更新也被称为最小均方(LMS)规则。作为一种蒙特卡罗算法，所有的更新都取决于最终的奖励/回报，所以在这一集结束之前都不能进行任何更新。MC算法是一种离线算法，我们不寻求改进它的这一方面。相反，我们只是寻求一种具有计算优势的算法的实现。我们仍将只在本集结束时更新权重向量，但我们将在本集的每一步中进行一些计算，而在本集结束时进行更少的计算。这将使计算的分布更加平均——o (d)每步，并消除了在每个步骤中存储特征向量的需要，以便在每一集结束时使用。相反，我们将引入一个额外的向量内存，称为资格跟踪，保留到目前为止看到的所有特征向量的摘要。这将足以有效地重新创建与MC序列完全相同的整体更新

更新(12.13),年底这一事件:wT =ω−1 +α

G−w ?T−1 xt−1

xT−1

=ω−1 +αxT−1

−x ?T wt−−1 1

+αGxT−1

=

我−αxT−1 x ?T−1

wT−1 +αGxT−1
=英尺−1 wt−1 +αGxT−1
在英国《金融时报》
。=我−αxtx吗?t是一个遗忘，或衰落，矩阵。现在,递归,
=英尺−1(英尺−2 wt−2 +αGxT−2)+αGxT−1
=英尺2 wt−−−1英尺2 +αG(英尺−1 xT−2 + xT−1)
= 2英尺−−1英尺(英国《金融时报》3 wt−−3 +αGxT−3)+αG(英尺−1 xT−2 + xT−1)
=英尺−−1英尺2英尺−3ω−3 +αG(英尺−−1英尺2 xT−−3 +英尺1 xT−2 + xT−1)

=−1 +αGzT−1 					(12.14)
−1和zT型−1的值在时间T−1的两个auxilary记忆向量可以没有知识的不断更新的时间复杂度为O(d)的G和每一步。zt向量实际上是一个荷兰式的资格跟踪。它被初始化为z0 = x0，然后根据。


zt型
。=
t ?k = 0
FtFt−1···颗+ 1 xk,1≤t < t


=
t−1 ?k = 0
FtFt−1···颗+ 1 xk + xt


英国《金融时报》=
t−1 ?k = 0
英尺−−1英尺2···Fk + 1 xk + xt队

= Ftzt−1 + xt

=

我−αxtx吗?t

zt型−1 + xt

= zt型−1−αxtx

t zt型−1 + xt
= zt型−1−α

z ?t−1 xt

xt + xt

= zt型−1 +

1−αz吗?t−1 xt

xt,

这是荷兰人跟踪的情况下γλ= 1(cf Eq。12.11)。将at辅助向量初始化为a0 = w0，然后根据

在
.
= FtFt−1···F0w0 = Ftat−1 =−1−αxtx吗?t−1,1≤t < t。

辅助向量at和zt在每个时间步t < t时更新，然后在观察G时，在(12.14)中使用它们来计算wT。通过这种方式，我们实现了与MC/LMS算法完全相同的最终结果，该算法的计算属性较差(12.13)，但现在我们使用的是增量算法，其每一步的时间和内存复杂度为O(d)。这是令人惊讶和有趣的，因为资格跟踪(特别是荷兰的跟踪)的概念出现在一个没有时间差异(TD)学习的环境中(与van Seijen和Sutton, 2014)。似乎资格的痕迹并不是TD学习所特有的;它们比这更重要。当一个人试图以一种有效的方式学习长期的预测时，就会产生对资格的需求。


\section{撒尔沙(λ)}
本章中已经提出的思想很少需要改变，以便将可信赖性-跟踪扩展到行动-价值方法。学习近似动作值,问̂(年代,w),而不是近似状态值,v̂(s,w),我们需要使用的行为价值形式n-step回报,从第十章:

Gt:t + n。= Rt + 1 +···+γn−1 Rt + n +γnq̂(圣+ n + n,wt + n−1),t + n < t,

Gt:t + n。如果t + n≥t = Gt。使用这个,我们可以形成的行为价值形式截断λ-return,否则相同的状态值形式(12.9)。的行为价值形式离线λ-return算法(12.4)仅仅使用q̂,而不是v̂:


wt + 1
.
= wt +α

Gλt−问̂(圣,wt)

wt∇问̂(圣),t = 0,。、T−1(12.15)

在Gλt
。= Gλt:∞。这个forward视图的复合备份关系图显示在下面
图12.9。注意到相似的图TD(λ)算法(图12.1)。第一个更新将向前跨一个完整的步骤，到下一个状态-动作对，第一个更新向前两个步骤，到第二个状态-动作对，等等。最后的更新基于完整的返回。每个n-step的权重更新λ-return同样在TD(λ)和λ-return算法(12.3)。
temporal-difference方法行动值,称为撒尔沙(λ),接近这个视图。它具有相同的更新规则正如前面给出的TD(λ):

wt + 1
.
= wt +αδtzt,

当然，除了使用TD错误的动作-值形式:

δt。= Rt + 1 +γq̂(圣+ 1 + 1,wt)−问̂(圣,在wt), 					(12.16)

以及合格跟踪的行动价值形式:

z
−1
。= 0,
zt型
。q =γλzt−1 +∇̂(圣,在wt),0≤t≤t。

图12.9:撒尔沙图(λ)的备份。与图12.1。

完整的伪代码撒尔沙(λ)和线性函数近似,二进制特征,并给出积累或取代痕迹的盒子在下一页。这个伪代码突出显示了二进制特性(特性要么是活动的(=1)，要么是不活动的(=0))的一些可能的优化。
示例12.1:在Gridworld中，使用合格跟踪可以大大提高控制算法对单步方法甚至n步方法的效率。原因如下面的gridworld示例所示。
 
第一个面板显示了代理在单个事件中采取的路径。初始估计值为零，所有的奖励都为零，除了目标位置为g的正奖励之外。其他面板上的箭头显示，对于各种算法，在达到目标后，动作值会增加，并增加多少。一步方法只增加最后一个动作的值，而n步方法将同样增加最后n个动作的值，而一个合格跟踪方法将更新所有动作的值，直到事件的开始，以不同的程度，随着时间的流逝而递减。衰落的策略往往是最好的。

练习12.6修改撒尔沙的伪代码(λ)使用荷兰痕迹(12.11)没有其他特色的一个真正的在线算法。假设线性函数近似
和二进制特征。 					?
例12.2:撒尔沙(λ)山车图12.10(左)在下一页显示结果与撒尔沙(λ)在山上汽车任务在例10.1中引入的。函数逼近、动作选择和环境细节与第10章完全一样，因此，用数字比较这些结果与第10章的n步Sarsa(图右侧)的结果是恰当的。早些时候结果不同长度n更新而在这里撒尔沙(λ)我们不同跟踪参数λ,扮演类似的角色。撒尔沙的fading-trace引导战略(λ)似乎在这个问题上更高效的学习。
还有我们的理想的行为价值版本TD方法,在线λ-return算法(12.4节)和有效实现真正的在线TD(λ)(12.5节)。在第12.4节中，除了使用在当前部分开始时给出的n步返回的动作值形式外，其余的部分都没有改变。第12.5和12.6节的分析也为行动值进行了分析，唯一的变化是使用。

撒尔沙(λ)取代痕迹n-step撒尔沙
 
×瓷砖的数量(8)α 					×瓷砖的数量(8)α

图12.10:早期性能在山上撒尔沙车的任务(λ)替换痕迹和n-step撒尔沙(复制从图10.4)作为一个函数的步长,α。

状态-动作特征向量为xt = x(St, At)，而不是状态特征向量xt = x(St)。由此产生的高效算法的伪代码,称为真正的在线撒尔沙(λ)在盒子在下一页。下图比较各种版本的撒尔沙的性能(λ)在山上汽车的例子。
 
×瓷砖的数量(8)α

图12.11:总结比较撒尔沙(λ)算法在山上车任务。真正的在线撒尔沙(λ)表现的更好比普通撒尔沙(λ)积累和取代的痕迹。还包括是一个版本的撒尔沙(λ)取代痕迹,在每个时间步,不选择状态和行为的痕迹被设置为0。

最后,还有一个截断版本的撒尔沙(λ),称为向前撒尔沙(λ)(van Seijen,2016),这似乎是一个特别有效的模范自由控制方法结合多层人工神经网络使用。

\section{变量λ和γ}

我们现在开始着手开发基本的TD学习算法。要以最一般的形式呈现最终的算法，可以将自举和折现的程度从常数参数推广到可能依赖于状态和动作的函数。也就是说,每个时间步将有不同的λ和γ,表示λtγt。现在我们改变符号,λ:S×→[0,1]现在是一个函数从州和行动单位间隔,这样λt
。=λ(St)
同样,γ:S→[0,1]是一个函数从国家到单位间隔,这样γt。=γ(St)。引入函数γ,终止功能,尤其重要,因为
它改变了回报，这是一个基本的随机变量它的期望是我们想要的

估计。现在，更一般地定义回报为

Gt。= Rt + 1 +γt + 1 gt + 1
= Rt + 1 +γt Rt + 2 + 1 +γt + 1γt Rt + 3 + 2 +γt + 1γt + 2γt Rt + 4 + 3 +···

=
∞吗?k = t
吗?k ?
我= t + 1
γi
Rk + 1, 					(12.17)

为了保证和是有限的，我们需要它
!∞
k = tγk = 0的概率
这个定义的一个方便的方面是，它允许情景设置和它的算法以单一的经验流的形式呈现，而不需要特殊的终端状态、开始分布或终止时间。一个昔日的终端状态,成为一个γ(s)= 0,开始转换分布。那样(通过选择γ(·)作为一个常数在其他州)我们可以恢复古典情景设置是一个特例。状态依赖终止包括其他预测情况，如伪终止，在这种情况下，我们寻求在不改变马尔可夫过程流的情况下预测一个量。折现收益可以被认为是这样一个量，在这种情况下，依赖于国家的终止结合了偶发性和折扣持续性的情况。(未讨论的持续病例仍然需要一些特殊治疗。)
对变量引导的泛化不是问题中的变化，比如折现，而是解决方案策略的变化。归纳影响λ-returns状态和行为。新的基于状态λ-return可以递归地写
作为

Gλs t
.
= Rt + 1 +γt + 1
?(1−λt + 1)v̂(wt)圣+ 1 +λt + 1 gλs t + 1,(12.18)
现在我们已经添加了“s”上标λ提醒我们,这是一个返回接连的状态值,区分它的返回从行动的价值观,引导我们与“a”下面的上标。这个方程表示,λ-return是第一个奖励,尚未完全和不受引导,加上可能连任,我们没有折扣在下次状态(也就是说,根据γt + 1;回想一下，如果下一个状态是终端，这是零。在某种程度上，我们没有在下一个状态终止，我们有第二个项它本身被划分成两种情况取决于状态的自举程度。在一定程度上引导,这一项的估计价值,然而,在某种程度上,我们不引导,术语是下次的λ-return一步。基于动作的λ-return是撒尔沙形式

Gλa t
.
= Rt + 1 +γt + 1

(1−λt + 1)问̂(圣+ 1 + 1,wt)+λt + 1 gλa t + 1

(12.19)

或者预期的Sarsa形式，

Gλa t
.
= Rt + 1 +γt + 1

(1−λt + 1)V̄t(圣+ 1)+λt + 1 gλa t + 1

, 					(12.20)

(7.8)是广义函数近似为V̄t(年代)。=

一个
π(|)问̂(年代,wt)。 					(12.21)

12.9。带控制变量的脱机跟踪 					309年



练习12.7将上面的三个递归方程归纳为它们的截断版本，
定义Gλs t h和Gλa t:h。 					?


\section{带有控制变量的非政策跟踪}

最后一步是合并重要性抽样。与n-step方法的情况下,完全non-truncatedλ-returns一个没有一个实际的选择的重要性抽样是目标回报之外完成的。相反，我们直接使用控制变量(7.4)对每个决策重要性抽样的引导一般化。在状态的情况下,我们最终λ-return概括的定义(12.18),(7.13),模型后

Gλs t
.

=ρt

Rt + 1 +γt + 1
?(1−λt + 1)v̂(wt)圣+ 1 +λt + 1 gλs t + 1

+(1−ρt)v̂(圣、wt)(12.22)

ρt =π(圣)| b(| St)是通常的单步重要性抽样比例。就像我们在书中看到的其他回报一样，这个回报的截断版本可以简单地近似于基于状态的TD错误的总和，

δs t
.
= Rt + 1 +γt + 1 v̂(wt)圣+ 1−v̂(St,wt), 					(12.23)

作为


Gλs t≈v̂(圣、wt)+ρt
∞吗?k = t
δs k
k ?我= t + 1
γiλiρi 					(12.24)


当近似函数不变时，近似趋于精确。练习12.8证明(12.24)如果值函数不改变，就变得准确。为了节省写作，考虑t = 0的情况，并使用符号Vk
。w = v̂(Sk)。?
12.9运动的截断版本一般off-policy返回Gλs t来标示:h。
根据(12.24)猜出正确的方程。 					?
上述形式的λ-return(12.24)就能够很方便的使用在一个前瞻更新、wt + 1 = wt +α

wt Gλs t−v̂(圣)

wt∇v̂(圣)

≈wt +αρt

∞吗?k = t
δs k
k ?我= t + 1
γiλiρi
wt∇v̂(圣),


对于有经验的人来说，这个产品看起来就像一个基于eligibilityt的更新——这个产品就像一个合格的跟踪，它被乘以TD错误。但这只是向前发展的时间步骤。我们正在寻找的关系是，对时间求和的forward-view更新，大约等于对时间求和的back -view更新(这个关系只是近似的，因为我们再次忽略了值的变化)

功能)。前瞻性更新随时间的总和是

∞吗?t = 1
(wt + 1−wt)≈
∞吗?t = 1
∞吗?k = t
wtαρtδs k∇v̂(圣)
k ?我= t + 1
γiλiρi

=
∞吗?k = 1
k ?t = 1
αρt∇v̂(圣wt)δs k
k ?我= t + 1
γiλiρi

(使用求和规则:?y t=x
? y k = t =
k ? y = x
? k t = x)

=
∞吗?k = 1
αδs k
k ?t = 1
wtρt∇v̂(圣)
k ?我= t + 1
γiλiρi,


如果可以将剩下的第二个和中的整个表达式作为资格跟踪进行增量编写和更新，那么将以向后视图TD更新的总和的形式进行更新。我们表明,如果k时刻跟踪这个表达式,然后我们可以从它的值更新它在时间的k−1:

zk =
k ?t = 1
wtρt∇v̂(圣)
k ?我= t + 1
γiλiρi

=
k−1 ?t = 1
wtρt∇v̂(圣)
k ?我= t + 1
γiλiρi +ρk∇v̂(Sk、工作)
 
其中，将索引从k更改为t，是状态值的一般累加跟踪更新:

zt型
.

=ρt

γtλtzt−1 +∇v̂(圣wt)

, 					(12.25)

这个资格的跟踪,以及通常的semi-gradient parameter-update规则TD(λ)(12.7),形成一个通用TD(λ)算法,可以应用于对政策或off-policy数据。在政策的情况下,该算法正是TD(λ)因为ρt总是1和(12.25)成为平时积累跟踪(12.5)(扩展到变量λ和γ)。在非策略情况下，该算法通常工作得很好，但作为一种半梯度方法，不能保证稳定。在接下来的几节中，我们将考虑对它进行扩展，以确保稳定性。
一个非常类似的一系列步骤可以按照推导出off-policy资格行为价值方法和相应的痕迹一般撒尔沙(λ)算法。一个可以开始递归形式一般基于动作的λ-return(12.19)或(12.20),但后者(预期的撒尔沙形式)是简单的工作。我们延长(12.20)到

在模型(7.14)之后的离线情况下生产

Gλa t
.
= Rt + 1 +γt + 1

(1−λt + 1)V̄t(圣+ 1)+λt + 1 ?ρt t + 1 + V + 1 gλāt(圣+ 1)−ρt̂+ 1 q(圣+ 1 + 1,wt)

= Rt + 1 +γt + 1

V̄t(圣+ 1)+λt + 1ρt + 1 ? Gλa t + 1−问̂(圣+ 1 + 1,wt)

(12.26)

圣+ V̄t(1)是由(12.21)给出。又可以编写λ-return大约TD错误的总和,

Gλa t≈问̂(圣,wt)+
∞吗?k = t
δa k
k ?我= t + 1
γiλiρi, 					(12.27)

使用基于活动的TD错误的期望形式:

δa t = Rt + 1 +γt + 1 v̄t(圣+ 1)−问̂(圣,在wt)。 					(12.28)

如前所述，如果近似值函数不变，则逼近将变得精确。
练习12.10证明(12.27)如果值函数不变，就会精确。为了节省写作,考虑t = 0的情况下,使用符号Qk =问̂(Sk,Ak,w)。提示:
先写出δa 0和Gλa 0,然后Gλa 0−Q0处。 					?
12.11运动的截断版本一般off-policy返回Gλa t来标示:h。
根据(12.27)猜出它的正确方程。 					?
使用完全类似于状态的步骤，可以基于(12.27)编写一个前视图更新，使用求和规则来转换更新的和，并最终得出以下形式的行为值的资格跟踪:

zt型
.
q =γtλtρtzt−1 +∇̂(圣,在wt)。 					(12.29)

这个资格的跟踪,再加上预想TD错误(12.28)和通常的semi-gradient parameter-update规则(12.7),形成一个优雅的、高效的预期撒尔沙(λ)算法,可以应用于对政策或off-policy数据。它可能是目前这种类型的最佳算法(当然，在以某种方式与下面几节中介绍的方法结合之前，它不能保证是稳定的)。在政策情况下常数λ和γ,和通常的政府行动TD错误(12.16),该算法将是相同的撒尔沙(λ)算法在12.7节。
练习12.12详细列出了上述从(12.27)推导(12.29)的步骤。开始更新(12.15),替代Gλa t从Gλ(12.26),然后相似
步骤如下(12.25)。 					?
在λ= 1,这些算法成为相应的蒙特卡罗算法密切相关。人们可能会认为，情景性问题和离线更新将具有确切的等价性，但事实上，这种关系更微妙，也更弱。在这些最有利的条件下，仍然没有一集接一集的更新，只有他们的期望。这并不像这些方法那样令人惊讶

在轨迹展开时进行不可撤销的更新，而真正的蒙特卡罗方法在目标策略下，如果轨迹内的任何动作都是零概率的，则不会对轨迹进行更新。特别是,所有这些方法,即使在λ= 1,还引导,他们的目标依赖于当前值估计,只是依赖消掉的期望值。这在实践中是好还是坏是另一个问题。最近，提出了实现精确等价的方法(Sutton, Mahmood, Precup and van Hasselt, 2014)。这些方法需要一个额外的“临时权重”向量来跟踪已更新的内容，但可能需要根据以后采取的行动收回(或强调)这些内容。国家和政府行动版本的这些方法被称为输配电(λ)和PQ(λ)分别在“P”代表的是临时的。
所有这些新的政策外方法的实际后果尚未确定。毫无疑问，在使用重要性抽样的所有非策略方法中都会出现高方差的问题(第11.9节)。
如果λ< 1,那么所有这些off-policy算法涉及引导和致命的三合会应用(11.3节),这意味着他们只能对于表格的情况,保证稳定的聚合状态,和其他有限形式的函数近似。对于线性和更一般的函数逼近形式，参数向量可能会发散到无穷大，就像第11章中的例子一样。正如我们在那里讨论过的，离线学习的挑战有两个部分。策略之外的合格跟踪有效地处理了挑战的第一部分，修正了目标的期望值，但完全没有处理挑战的第二部分，这与更新的分发有关。第12.11节总结了利用资格跟踪来应对脱机学习挑战的第二部分的算法策略。
练习12.13 off-policy的duch -trace和replacing-trace版本是什么
对状态值和动作值方法的资格跟踪? 					?

\section{沃特金斯的Q(λ)Tree-Backup(λ)}
多年来已经提出了几种方法来将Q-learning扩展到合格的跟踪。原来是沃特金斯的Q(λ),衰减其资格痕迹以通常的方式只要一个贪婪的行动,然后削减为零后第一个贪婪的操作痕迹。备份图沃特金斯的Q(λ)如图12.12所示。在第六章中,我们统一q学习的和预期的撒尔沙off-policy版本的后者,其中包括q学习作为一个特例,并推广到任意目标政策,本章在前面的小节中我们完成了治疗预期撒尔沙通过泛化off-policy资格痕迹。然而，在第七章中，我们区分了n步期望Sarsa和n步树备份，后者保留了不使用重要抽样的性质。仍然是那么的资格跟踪版本树备份,我们将称之为Tree-Backup(λ),或简称为结核病(λ)。这可以说是Q-learning的真正继承者，因为它保留了其吸引人的不重视抽样，即使它可以应用于非政策数据。

λn−1
图12.12:备份图沃特金斯的Q(λ)。组件更新系列结束
要么在这一集的结尾，要么在第一个非贪婪的动作中，哪个先出现。


结核病(λ)的概念很简单。作为其备份图如图12.13所示,tree-backup更新每个长度(从7.5节)的加权以通常的方式依赖于引导参数λ。详细的方程,与正确的指标一般引导和折扣参数,最好先递归形式λ-return使用行动值(12.20),然后扩大后的目标引导的情况下的模型(7.16):

Gλa t
.
= Rt + 1 +γt + 1

(1−λt + 1)V̄t(圣+ 1)+λt + 1

在+ 1 ? =
π(|圣+ 1)问̂(圣+ 1,wt)+π(+ 1 |圣+ 1)Gλa t + 1


= Rt + 1 +γt + 1

V̄t(圣+ 1)+λt + 1π(+ 1 |圣+ 1)

Gλa t + 1−问̂(圣+ 1 + 1,wt)

按照通常的模式，它也可以近似地(忽略近似值函数的变化)写成TD错误的总和，

Gλa t≈问̂(圣,wt)+
∞吗?k = t
δa k
k ?我= t + 1
γiλiπ(Ai | Si),

使用基于活动的TD错误的期望形式(12.28)。
与上一节相同的步骤，我们到达了一个特殊的资格跟踪更新，涉及所选操作的目标策略概率，

zt型
。在| =γtλtπ(St)zt型−1 +∇问̂(圣,在wt)。

314年 					第十二章:合格的痕迹
 

图12.13:树的λ版本备份的备份图算法。

加之通常parameter-update规则(12.7),定义了结核病(λ)算法。像所有semi-gradient算法,结核病(λ)不保证稳定在使用off-policy数据和一个强大的函数近似者。获得这些保障,结核病(λ)必须加上在下一小节中介绍的方法之一。
∗锻炼12.14双预期如何撒尔沙延长资格的痕迹??

\section{带有跟踪的稳定脱机方法}

已经提出了几种使用合格跟踪的方法来实现脱机训练下的稳定性保证，这里我们用这本书的标准表示法提出了四个最重要的方法，包括一般的自举和贴现函数。所有这些都基于第11.7和11.8节中提出的渐变- td或强调- td思想。所有的算法都假设线性函数近似，虽然也可以在文献中找到非线性函数近似的扩展。
GTD(λ)是类似于TDC eligibility-trace算法,更好的的两个州值Gradient-TD预测算法在11.7节讨论。它的目标是学习一个参数wt这样v̂(s,w)。= w ?t x(s)≈vπ(s),甚至从数据,是由于后另一个政策b。其更新


wt + 1
。= wt +αδs t zt型−αγt + 1(1−λt + 1)

z ?t vt

xt + 1,

δs t、zt型和ρt以通常的方式定义状态值(12.23)(12.25)(11.1),和

vt + 1
。= vt +βδs t zt型−β

v ?t xt

xt, 					(12.30)

在11.7节中,v∈Rd一样是一个向量的维度w,初始化为v0 = 0,和β> 0是第二步长参数。
《GQ》(λ)行动是Gradient-TD算法值与资格痕迹。它的目标是学习一个参数wt这样问̂(年代,wt)。= w ?t x(,)≈qπ从off-policy数据(年代)。如果目标政策ε-greedy或偏向贪婪的政策问̂,那么《GQ》(λ)可以用作控制算法。它的更新

wt + 1
。= wt +αδa t zt型−αγt + 1(1−λt + 1)

z ?t vt

x̄t + 1,

x̄t是圣目标政策下的平均特征向量,


x̄t
。=

一个
π(| St)x(圣,),

δa t是期望形式的TD错误,可以写

δa t
.
= Rt + 1 +γt + 1 w ?t x̄t + 1−w ?t xt,
zt型定义以通常的方式行动值(12.29),和其他在GTD(λ),包括更新vt(12.30)。
HTD(λ)是一个混合的州值算法的结合方面GTD(λ)和TD(λ)。最吸引人的特征是,它是一种严格的泛化TD(λ)off-policy学习,这意味着如果行为政策是一样的目标政策,然后HTD(λ)一样成为TD(λ),这对GTD(λ)是不正确的。这是吸引人的,因为TD(λ)通常是速度比GTD(λ)两种算法收敛时,和TD(λ)只需要设置一个步长。HTD(λ)被定义为

wt + 1
.
= wt +αδs t zt型+α
?(zt型−zb t)?vt(xt−γt + 1 xt + 1),
vt + 1
。= vt +βδs t zt型−β

zb t

vt

(xt−γt + 1 xt + 1),半。= 0,
zt型
.

=ρt

γtλtzt−1 + xt

,与z
−1
。= 0,
zb t
.
=γtλtzb t−1 + xt,zb−1
。= 0,

其中β> 0又是第二步长参数。除了第二组重量、vt,HTD(λ)也有第二组资格痕迹,zb t。这些都是传统的积累资格的行为痕迹政策,成为等于zt型如果所有ρt 1,导致最后一个学期wt更新为零和整体更新来减少TD(λ)。
强调TD(λ)的扩展一步Emphatic-TD算法(章节9.11和11.8)资格的痕迹。结果算法保留了很强的非策略收敛保证，同时允许任何程度的自举，尽管代价是

方差大，收敛速度慢。强调TD(λ)被定义为

wt + 1
.
= wt +αδtzt
δt。= Rt + 1 +γt + 1 w ?t xt + 1−w ?t xt zt型
.

=ρt

γtλtzt−1 + Mtxt

,与z
−1
。= 0,
太
。=λt +(1−λt)英国《金融时报》
英国《金融时报》
.
=ρt−1γtft−1 +,F0
。=我(S0),

那里太≥0的一般形式是强调,英国《金融时报》≥0称为followon跟踪,≥0是利息,如11.8节所述。注意,太δt一样,并不是真正的一个额外的内存变量。通过将其定义代入可列性-迹方程，可以将其从算法中删除。真正的在线版本的伪代码和软件Emphatic-TD(λ)是web上可用(萨顿,2015 b)。
在政策的情况下(ρt = 1,t)Emphatic-TD(λ)是类似传统TD(λ),但仍显著不同。事实上,而Emphatic-TD(λ)是保证为所有情境依靠λ函数收敛,TD(λ)不是。TD(λ)保证收敛只有常数λ。参见Yu的反例(Ghiassian, Rafiee, Sutton, 2016)。


\section{实现问题}

乍一看，使用资格跟踪的列表方法可能比单步方法复杂得多。一个幼稚的实现将要求每个状态(或状态操作对)在每个时间步骤上更新其值估计和它的资格跟踪。对于单指令、多数据、并行计算机或似是而非的人工神经网络(ANN)实现来说，这不是一个问题，但对于常规串行计算机的实现来说，这是一个问题。幸运的是,对于典型值λ和γ资格的痕迹几乎所有州几乎总是几乎为零;只有最近访问过的那些状态的跟踪值将显著大于0，只有这些很少的状态需要更新以接近这些算法。
因此，在实践中，常规计算机上的实现可能只跟踪和更新明显大于零的少数跟踪。使用这个技巧，在表格方法中使用跟踪的计算开销通常是单步方法的几倍。确切的多个当然取决于λ和γ和其他费用的计算。请注意，表格情况在某种意义上是资格跟踪计算复杂性的最坏情况。当使用函数逼近时，不使用跟踪的计算优势通常会降低。例如，如果使用了ANNs和反向传播，那么资格跟踪通常只会导致每一步所需的内存和计算加倍。截断λ-return方法(12.3节)可以在传统计算机计算效率虽然他们总是需要一些额外的内存。

\section{结论}

与TD错误结合的资格跟踪提供了一种有效的、递增的在蒙特卡罗和TD方法之间进行转换和选择的方法。第7章的n步方法也支持这一点，但是资格跟踪方法更通用，学习起来通常更快，并且提供不同的计算复杂度权衡。本章介绍了关于政策内外学习和变量自举和折现的资格跟踪的优雅、新兴的理论理解。这个优雅的理论的一个方面是真正的在线方法，它精确地再现了昂贵的理想方法的行为，同时保持了传统的TD方法的计算一致性。另一个方面是派生的可能性，它可以自动地从直观的前视图方法转换为更有效的增量后视图算法。我们用一种经典的、昂贵的蒙特卡罗算法推导出了这一总体思路，并以一种廉价的增量非TD实现方式结束，使用的是真正的在线TD方法中使用的同一种新的合格跟踪。
正如我们在第5章中提到的，蒙特卡罗方法在非马尔可夫任务中可能有优势，因为它们没有引导。由于资格跟踪使TD方法更像蒙特卡罗方法，因此它们在这些情况下也具有优势。如果由于TD方法的其他优点而希望使用TD方法，但是任务至少是部分非马尔可夫的，那么就需要使用资格跟踪方法。资格跟踪是针对长期延迟的奖励和非马尔可夫任务的第一道防线。
通过调整λ,我们可以将资格沿着连续跟踪方法在任何地方从蒙特卡罗一步TD方法。我们把它们放在哪里?对于这个问题，我们还没有一个好的理论答案，但一个明确的经验答案似乎正在出现。对于每集有许多步骤的任务，或者在折现的半衰期内有许多步骤的任务，使用资格跟踪比不使用要好得多(例如，参见图12.14)。另一方面，如果痕迹长到产生一个纯粹的蒙特卡罗方法，或者差不多，性能就会急剧下降。中间混合物似乎是最好的选择。资格的痕迹应该用来带我们走向蒙特卡罗方法，但不是一直那样。在将来它可能会更多的精细变化之间的权衡TD和蒙特卡罗方法通过使用变量λ,但目前尚不清楚这是如何做到的可靠和有效。
使用资格跟踪的方法需要比一步法更多的计算量，但反过来它们提供了显著更快的学习速度，特别是当奖励被许多步骤延迟时。因此，在数据稀缺且无法重复处理的情况下，使用资格跟踪通常是有意义的，这在在线应用程序中经常出现。另一方面，在离线应用程序中，数据可以很便宜地生成，可能来自廉价的模拟，然后通常不支付使用资格跟踪。在这些情况下，目标不是从有限的数据中获得更多，而是尽可能快地处理尽可能多的数据。在这些情况下，由于跟踪而导致的每个数据的加速率通常不值得它们的计算成本，并且支持一步法。

0	0.2	0.4	0.6	0.8
第十二章:合格的痕迹
 
 


图12.14:λ在强化学习性能的影响在四个不同的测试问题。在所有情况下,性能通常是最好的(图中较低的数量)的中间值λ。左边的两个面板是简单的连续状态控制的应用程序
任务使用撒尔沙(λ)算法和瓷砖编码、替换或积累痕迹(萨顿,1996)。政策评估的右上角面板随机漫步的任务使用TD(λ)(辛格和萨顿,1996)。右下角的面板是平衡杆任务的未发布数据
(例子3.4)来自早期的研究(Sutton, 1984)。

\section{书目的和历史的言论}

通过Klopf(1972)的丰富思想，获得了资格证书。我们使用资格追溯的依据是Klopf的工作(Sutton, 1978a, 1978b, 1978c;Barto和Sutton, 1981a, 1981b;萨顿和Barto,1981;巴托，萨顿，安德森，1983年;萨顿,1984)。我们可能是第一个使用“资格追踪”一词的(Sutton and Barto, 1981a)。刺激物在神经系统中产生后效应对学习很重要，这一观点由来已久(见第14章)。第13章讨论了演员-批评家的方法(Barto, Sutton, and Anderson, 1983;萨顿,1984)。

12.1复合更新在第一版中称为“复合备份”
书。
λ-return及其错误降低属性引入了沃特金斯(1989)和由Jaakkola进一步发展,约旦和辛格(1994)。在这个和后面的部分中，random walk的结果是新的，比如“forward view”和“向后视图”。“λ-return算法的概念引入本文的第一版。这里提出的更精细的处理方法是与危害范思珍(例如，van Seijen和Sutton, 2014)一起开发的。

12.2 TD(λ)积累了痕迹,萨顿(1988、1984)。共同
大研(1992)证明了平均值的收敛性，许多研究人员都有概率1，包括Peng(1993)、Dayan和Sejnowski(1994)、Tsitsiklis(1994)、Gurvits、Lin和Hanson(1994)。绑定的误差渐近λ-dependent解决线性TD(λ)是由于Tsitsiklis和范·罗伊(1997)。

12.3截断的TD方法由Cichosz(1995)和van Seijen(2016)开发。12.4重做更新的想法最初是由van Seijen提出的
以“最佳比赛学习”为名(van Seijen, 2011;van Seijen, Whiteson, van Hasselt和weling2011)。

12.5真实在线TD(λ)主要是由于伤害van Seijen(van Seijen和萨顿,
2014;van Seijen et al.， 2016)尽管它的一些关键思想是由Hado van Hasselt(个人交流)独立发现的。“荷兰痕迹”这个名字是为了表彰两位科学家的贡献。
替换痕迹要归功于辛格和萨顿(1996)。

12.6本节材料来自van Hasselt和Sutton(2015)。

12.7撒尔沙(λ)与积累痕迹最初探索的控制方法
酒店和Niranjan(1994;酒店,1995)。真正的在线撒尔沙(λ)介绍了van Seijen和萨顿(2014)。第307页的算法是

改编自van Seijen et al.(2016)。山车的结果是为本文制作的，除了图12.11改编自van Seijen和Sutton(2014)。

12.8也许第一个发表讨论变量λ是沃特金斯(1989)
指出,更新的切断序列(图12.12)在他问(λ)当nongreedy行动选择可以实现通过暂时设置λ为0。
介绍了变量λ第一版的文本。变量γ的根是在工作上选项(Precup萨顿,辛格1999)及其前体(萨顿,1995),成为《GQ》(λ)显式纸(Maei和萨顿,2010),也介绍了一些λ-returns递归形式。
不同的变量概念λ开发了玉(2012)。

12.9随后Precup等(2000年，2001年)引入了保单之外的资格追溯
由Bertsekas和Yu (2009)， Maei (2011;Maei和Sutton, 2010)， Yu (2012)， Sutton, Mahmood, Precup, van Hasselt(2014)。尤其是最后参考使一个强大的前锋视图off-policy TD方法依赖政府与通用λ和γ。这里的报告似乎是新的。
本节以一个优雅的预期撒尔沙(λ)算法。虽然它是一种自然算法，但据我们所知，它还没有在文献中被描述或测试过。

12.10沃特金斯的Q(λ)是由于沃特金斯(1989)。表格式的，情节式的，离线的版本
Munos、Stepleton、Harutyunyan和Bellemare(2016)证明了收敛性。替代Q(λ)算法提出的彭和威廉姆斯(1994、1996)和萨顿,马哈茂德,Precup,范特(2014)。树备份(λ)是由于Precup,萨顿,辛格(2000)。

12.11 GTD(λ)是由于Maei(2011)。《GQ》(λ)是由于Maei和萨顿(2010)。
HTD(λ)是由于白色和白色(2016)基于一步HTD算法引入了哈克曼(2012)。梯度- td方法理论的最新发展是Yu(2017)。强调TD(λ)引入的萨顿,马哈茂德,和白色(2016),证明了其稳定性。Yu(2015, 2016)证明了其收敛性，算法由Hallak et al.(2015, 2016)进一步开发。
