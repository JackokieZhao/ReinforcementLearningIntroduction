\chapter{第九章 在政策与近似预测}
\begin{summary}
	在这一章,我们开始我们的研究强化学习的函数近似考虑其使用在评估州值函数从政策数据,也就是说,在近似vπ从经验中使用一个已知的生成策略π。新奇的在这一章是近似值函数而不是表表示为一个参数化函数形式与权向量w∈Rd。我们将编写v̂(s,w)≈vπ(s)的近似值状态s给定的权向量w。例如,v̂可能是一个线性函数的功能状态,与w权重向量的特性。更普遍的是,v̂可能由多层人工神经网络函数计算,与w所有层的连接权值向量。通过调整权重，网络可以实现各种不同的功能。或v̂可能函数计算一个决策树,在w是所有数字定义分割点和叶树的值。通常，权值的数量(w的维数)要比状态数(d?而改变一个权重就会改变许多状态的估计值。因此，当一个状态被更新时，更改将从该状态推广到影响许多其他状态的值。这样的泛化使学习可能更强大，但也可能更难于管理和理解。
	也许令人惊讶的是，将强化学习扩展到函数逼近也使它适用于部分可观察的问题，在这种问题中，代理无法获得完整的状态。如果参数化函数形式v̂不允许估计价值取决于国家的某些方面,那么它同样如果这些方面是不可见的。事实上，书中这部分给出的函数逼近方法的所有理论结果都同样适用于局部可观测的情况。然而，函数逼近所不能做的是用过去观察的记忆来增加状态表示。第17.3节简要讨论了一些可能的进一步扩展。
\end{summary}

\section{值函数逼近}

本书所介绍的所有预测方法都被描述为对估计值函数的更新，该函数在特定状态下将其值转换为该状态的“备份值”或更新目标。让我们参考个体更新到符号年代?→u,s是状态更新和u是年代的估计价值的更新目标转向。例如,蒙特卡洛更新值预测是圣?→Gt,TD(0)更新是圣?→Rt + 1 +γv̂(wt)圣+ 1,圣是n-step TD更新?→Gt:t + n。DP(动态规划)政策评估更新,s ?→Eπ(Rt + 1 +γv̂(wt圣+ 1)|圣= s],任意状态更新,而在其他情况下国家遇到的实际经验,圣,更新。
将每次更新解释为指定值函数所需的输入-输出行为的示例是很自然的。在某种意义上,更新年代?→u意味着国家年代估计价值应该更像更新目标u。到目前为止,实际的更新一直微不足道:年代的估计价值的表条目只是一小部分的方式转向你,和所有其他国家的估计价值保持不变。现在，我们允许任意复杂和复杂的方法来实现更新，并在s处进行更新，以使许多其他状态的估计值也被更改。以这种方式学习模拟输入输出示例的机器学习方法称为监督学习方法，当输出是数字时，如u，这个过程通常称为函数逼近。函数逼近方法期望得到它们试图逼近的函数的预期输入-输出行为的例子。我们用这些方法来预测价值仅仅通过他们的年代?→g的每个更新作为一个培训的例子。然后我们将它们产生的近似函数解释为一个估计值函数。
以这种方式将每个更新看作常规的训练示例，使我们能够使用各种现有的函数逼近方法中的任何一种进行值预测。原则上，我们可以用任何方法来监督学习，包括人工神经网络、决策树和各种各样的多元回归。然而，并不是所有的函数逼近方法都同样适用于强化学习。最复杂的人工神经网络和统计方法都假设有一个静态训练集，在此基础上进行多次传递。然而，在强化学习中，重要的是学习能够在线进行，同时代理与它的环境或它的环境模型进行交互。要做到这一点，需要能够有效地从增量获得的数据中学习的方法。此外，增强学习通常需要函数逼近方法来处理非平稳目标函数(随着时间变化的目标函数)。例如,在控制方法基于谷歌价格指数(广义政策迭代)我们经常寻求学习qππ的变化。即使策略保持不变，如果训练示例的目标值是通过引导方法(DP和TD学习)生成的，那么它们也是非平稳的。不容易处理这种非平稳性的方法不太适合强化学习。

\section{预测目标(VE)}
到目前为止，我们还没有明确明确的预测目标。在表格式的情况下，不需要连续的预测质量，因为学习的值函数可以精确地等于真正的值函数。此外，每个状态的学习值都是解耦的——一个状态的更新不影响其他状态。但是用真正的近似值，一个状态的更新会影响许多其他状态，并且不可能使所有状态的值都完全正确。假设我们有更多的状态，而不是权重，所以让一个状态的估计值更准确必然意味着让另一个状态的估计值更不准确。我们有义务说出我们最关心的国家。我们必须指定一个国家分布μ(s)≥0 ? sμ(s)= 1,表示我们关心多少在每个州年代错误。错误的年代我们意味着区别近似值v的平方̂(s,w)和真正价值vπ(年代)。权重这个μ的状态空间,我们获得一个天然的目标函数,均方值错误,已经表示为:
 
这个度量的平方根，也就是根VE，给出了一个粗略的度量，来表示近似值与真实值之间的差异，并且通常在绘图中使用。选择经常μ(s)的一部分时间在s。在政策培训这叫做政策分布;在这一章中，我们完全关注这个案例。在持续的任务,在政策分布是π下的平稳分布。

这两种情况，连续的和偶发的，表现相似，但近似地，它们必须在形式分析中单独处理，我们将在本书的这一部分反复看到。这就完成了学习目标的说明。
但对于强化学习来说，VE是否是正确的性能目标还不完全清楚。记住，我们的终极目标——我们学习价值功能的原因——是找到更好的政策。为此目的的最佳值函数不一定是最小化VE的最佳值函数。然而，目前还不清楚价值预测的一个更有用的替代目标是什么。现在，我们将关注VE。
理想目标而言,已经是找到全局最优,权重向量w∗为VE(w∗)≤(w)所有可能的w。达到这个目标等简单的函数近似者有时可能是线性的,但很少有可能复杂函数近似者,如人工神经网络和决策树。短,复杂的函数近似者可能寻求而不是局部最优,收敛的权向量w∗VE(w∗)≤VE(w)w w∗的一些社区。虽然这种保证只是稍微让人放心，但对于非线性函数近似器来说，它通常是最好的，而且通常已经足够了。尽管如此，对于许多对强化学习感兴趣的情况来说，并不能保证收敛到一个最优值，甚至在一个最优值的有限距离内。有些方法实际上是有分歧的，因为它们的极限接近无穷。
在前两部分中，我们概述了一个框架，用于将广泛的增强学习方法与广泛的函数逼近方法结合起来，使用前者的更新为后者生成训练示例。我们还描述了一个VE性能度量，这些方法可能希望最小化它。可能的函数近似方法的范围太大了，无法涵盖所有的内容，而且无论如何，大多数人都不知道如何进行可靠的评估或推荐。当然，我们只考虑一些可能性。在本章的其余部分，我们将重点讨论基于梯度原理的函数逼近方法，特别是线性梯度下降法。我们关注这些方法，部分是因为我们认为它们特别有前途，也因为它们揭示了关键的理论问题，但也因为它们很简单，我们的空间有限。


\section{随机梯度和半梯度方法}

本文详细介绍了一类基于随机梯度下降(SGD)的函数逼近方法。SGD方法是所有函数逼近方法中应用最广泛的一种，特别适合在线强化学习。
在梯度下降法中，权向量是具有固定数目实值分量w的列向量。= (w1, w2，…),wd)?,1和近似值函数v̂(s,w)是一个可微函数的w s∈s .我们将更新w在一系列离散时间的每个步骤,t = 0,1,2,3,。,所以我们需要一个符号的wt

1的?表示转置，这里需要将文本中的水平行向量转换为垂直
列向量;在这本书中，向量通常被认为是列向量，除非显式地写出水平或转置。
每一步的权向量。现在,让我们假定,在每一步,我们观察一个新的例子圣?→vπ(St)组成的(可能是随机选择的)州圣和其真正价值的政策。这些状态可能是与环境交互的连续状态，但目前我们不这样认为。即使我们给出精确的,正确的价值观,vπ每个圣(St),还有一个困难的问题,因为我们的函数近似者资源有限,因此有限的决议。特别地，通常没有w能得到所有的状态，甚至所有的例子，完全正确。此外，我们必须对没有在例子中出现的所有其他国家一概而论。
我们假设状态出现在示例相同的分布,μ,我们试图最小化由(9.1)。在这种情况下，一个好的策略是尽量减少所观察到的示例上的错误。随机梯度下降(SGD)方法通过在每个示例之后对权重向量进行少量的调整，从而最大程度地减少该示例上的错误:
α是一个积极的步长参数,∇f(w),为任何标量表达式f(w),是一个向量的函数(w),表示列向量的偏导数表达式对向量的分量:
∇f(w)。=

∂w1∂f(w)
,
∂f(w)∂w2
,。,
∂f∂wd(w)

. 					(9.6)

这个导数向量是f对w的梯度，SGD方法是“梯度下降”方法，因为在wt中，整体的步长与例子的平方误差的负梯度成正比(9.4)。这是误差下降最快的方向。当更新完成时，梯度下降方法被称为“随机”，这里只对一个示例进行更新，这个示例可能是随机选择的。在许多示例中，进行小步骤，总体效果是最小化平均性能度量，如VE。
为什么SGD在梯度方向上只走了一小步，这一点可能不会马上显现出来。我们能不能沿着这个方向移动并完全消除这个例子中的错误?在许多情况下，这是可以做到的，但通常是不可取的。请记住，我们并不是在寻找或期望找到一个对所有状态都有零误差的值函数，而只是一个平衡不同状态下误差的近似。如果我们在一步中完全纠正了每个例子，那么我们就不会找到这样的平衡。事实上,收敛结果SGD假定α随时间的方法。如果以满足标准随机逼近条件(2.7)的方式减小，则保证SGD方法(9.5)收敛到局部最优。
我们现在的目标输出,这里表示Ut∈R,t的训练示例中,圣?→Ut,不是真正价值,vπ(圣),但一些,可能是随机的,近似。例如,Ut可能的系统版本的vπ(圣),也可能是一个引导使用v̂在前一节中提到的目标。在
这些情况我们不能执行的更新(9.5)因为vπ(St)是未知的,但是我们可以把它近似取代Ut代替vπ(St)。这就产生了以下状态值预测的通用SGD方法:


wt + 1
.
= wt +α

Ut−v̂(圣wt)

wt∇v̂(St)。 					(9.7)

如果Ut是一个无偏估计,也就是说,如果E[Ut |圣= s]= vπ(圣),对于每一个t,然后wt保证收敛于局部最优通常随机近似条件下降低α(2.7)。
例如,假设美国的例子是美国所产生的交互(或模拟交互)与环境使用政策π。因为一个状态的真实值是它之后的收益的期望值，蒙特卡罗目标Ut。= Gt是通过定义一个无偏估计的vπ(St)。有了这个选择,一般SGD方法(9.7)收敛于局部最优近似vπ(St)。从而保证了蒙特卡罗状态值预测的梯度下降版本能够找到局部最优解。完整算法的伪代码如下所示。
 

不获得相同的担保,如果引导估计vπ(St)作为目标Ut(9.7)。引导目标，如n步返回Gt:t+n或DP目标

一个年代?,rπ(| St)p(s ?r |圣,)[r +γv̂(s ? wt)]所有依赖的当前值
重量矢量wt，这意味着它们会有偏差，不会产生真正的梯度下降法。看这个的一个方法是,关键步骤从(9.4)、(9.5)依赖于目标独立wt。这一步不会有效如果引导估计是用于代替vπ(St)。引导方法实际上不是真正的梯度下降的实例(Barnard, 1993)。它们考虑了改变权重向量wt对估计的影响，但忽略了它对目标的影响。它们只包含梯度的一部分，因此，我们称之为半梯度法。
虽然半梯度(bootstrapping)方法不像梯度方法那样强收敛，但它们在重要的情况下确实可靠地收敛，如下一节讨论的线性情况。此外，他们还提供了一些重要的优势，这些优势使他们更喜欢。原因之一是，它们通常能显著提高学习速度，正如我们在第6章和第7章中看到的那样。另一个原因是他们能够学习。

持续在线，不要等待一集的结束。这使它们能够用于持续的问题，并提供计算优势。一个典型的半梯度方法是半梯度TD(0)，它使用Ut。= Rt + 1 +γv̂(圣+ 1 w)作为它的目标。该方法的完整伪代码在下面的框中给出。
状态聚合是一种简单的推广函数逼近形式，其中状态被分组在一起，每个组有一个估计值(权向量w的一个分量)。状态的值被估计为其组的组件，当状态被更新时，只更新该组件。国家SGD聚合是一种特殊情况(9.7)的梯度,∇v̂(St,wt),为圣的组件是1和0的其他组件。
示例9.1:1000状态随机游走上的状态聚合考虑一个1000状态的随机游走任务(第125和144页上的示例6.2和7.1)。这些州的编号从1到1000，从左到右，所有的剧集都在500州的中心开始。状态转换是从当前状态到它左边的100个相邻状态之一，或者到它右边的100个相邻状态之一，所有的概率都是相等的。当然，如果当前状态接近边缘，那么在它的一侧可能会有少于100个邻居。在这种情况下，所有消失的邻居的概率都变成了在这一边终止的概率(因此，状态1有0.5的概率在左边终止，状态950有0.25的概率在右边终止)。像往常一样,左边终止产生的奖励−1,右边和终止生产+ 1的奖励。所有其他的转变都是零奖励。我们将此任务作为贯穿本节的运行示例。
图9.1显示了真值函数vπ这项任务。它几乎是一条直线，但在每一端的最后100个状态中，它稍微向水平方向弯曲。也显示最后学到的近似值函数梯度蒙特卡罗算法与国家聚合后100000集的步长α= 2×10−5。对于状态聚合，1000个状态被划分为10组，每组100个状态(即美国的州1-100是一个群体，州101-200是另一个群体，以此类推。楼梯的效果
价值尺度
  近似MC v̂价值
分布
范围0
0
国家1000
图9.1在1000态随机漫步任务中，状态聚合的函数逼近，
使用梯度蒙特卡罗算法(第202页)。
图中所示为典型的状态聚合;在每一组中，近似值都是常数，并且从一组突然变化到另一组。这些近似值接近VE(9.1)的全局最小值。
一些细节的近似值是最好的赞赏,参照国家分布μ这个任务,较低部分的图所示右规模。位于中心的状态500是每一集的第一个状态，但很少再次访问。平均来说，大约1.37\%的时间步骤花费在开始状态。从一开始就可到达的状态是第二次访问，在每一个步骤中花费了0.17\%的时间。从那里几乎线性μ脱落,达到极端状态1和1000年的0.0147\%左右。最明显的分布影响是在最左边的组，其值明显地比组内状态的真实值的未加权平均值高，而在最右边的组，其值明显地降低。这是由于美国在这些领域拥有他们的权重最大的不对称μ。例如，在最左边的组中，状态100的权重是状态1的3倍。因此，对组的估计偏向于状态100的真实值，它高于状态1的真实值。

\section{线性方法}

最重要的一个特殊情况的近似函数,近似函数,v̂(·w),是一个线性函数的权向量,w。对应于每一个州,有一个实值向量x(年代)。= (x1(s)， x2(s)，…,xd(s))?与w的分量相同，线性方法近似于状态值函数。

w和x之间的内积(s):v̂(s,w)。= w ? x(s)。=
d ?i = 1
wixi(年代)。 					(9.8)

在这种情况下，近似值函数在权值中是线性的，或者仅仅是线性的。
向量x(s)被称为特征向量代表国家。每个组件xi(s)x(s)的值是一个函数xi:s→r .我们认为一个功能完整的其中一个功能,我们调用它的价值特性的年代。线性方法,特征基函数,因为它们形成一个近似的线性基础设置功能。构造表示状态的d维特征向量与选择一组d基函数是一样的。特性可以用许多不同的方式定义;我们将在下一节中讨论一些可能性。
使用线性函数近似的SGD更新是很自然的。在这种情况下，近似函数关于w的梯度是

∇v̂(s,w)= x(年代)。
因此，在线性情况下，一般SGD更新(9.7)简化为一种特别简单的形式:

wt + 1
.
= wt +α

Ut−v̂(圣wt)

x(St)。
因为线性SGD是如此简单，所以它是最适合进行数学分析的。几乎所有用于学习各种系统的有用的收敛结果都适用于线性(或更简单的)函数逼近方法。
特别是，在线性情况下，只有一个最优解(或者在退化情况下，有一组同样好的最优解)，因此，任何保证收敛到局部最优解或接近局部最优解的方法都自动保证收敛到全局最优解或接近全局最优解。例如,在前一节中给出的梯度蒙特卡罗算法收敛于全局最优线性函数近似下的VE如果α随时间减少根据通常的条件。
上一节介绍的半梯度TD(0)算法也在线性函数近似下收敛，但这并不符合SGD的一般结果;一个单独的定理是必要的。收敛到的权向量也不是全局最优，而是接近局部最优的点。更详细地考虑这个重要的情况是有用的，特别是对于持续的情况。每次t的更新是

wt + 1
.
= wt +α

Rt + 1 +γw吗?t xt + 1−w ?t xt

xt 					(9.9)

= wt +α

Rt + 1 xt−xt

xt−γxt + 1

wt

,



这里我们用了符号简写xt = x(St)一旦系统达到稳定状态，对于任何给定的wt，都可以写出期望的下一个权重向量

E(wt + 1 | wt)= wt +α(b−Awt) 					(9.10)

在哪里

b。= E(Rt + 1 xt)∈Rd和A。= E

xt

xt−γxt + 1

∈Rd×Rd(9.11)
由(9.10)可知，如果系统收敛，则它必须收敛于权向量wTD

b−AwTD = 0
⇒b = AwTD
⇒西医
.
=−1 b。 					(9.12)
这个量叫做TD定点。事实上，线性半梯度TD(0)收敛到这一点。盒子里给出了证明它收敛性的一些理论，以及上面的逆的存在性。
 
在TD的不动点，它也被证明(在继续的情况下)，VE在最小可能误差的有界扩展范围内:
 
即渐近TD方法误差不超过1−γ*尽可能最小的误差,达到在蒙特卡罗方法的限制。因为γ通常是附近的一个,这个扩展因数可能很大,所以有大量潜在损失与TD渐近性能的方法。另一方面，回想一下，TD方法通常比蒙特卡罗方法的方差大得多，因此更快，正如我们在第6章和第7章所看到的那样。哪种方法最好取决于近似和问题的性质，以及学习的时间。

类似于(9.14)的约束也适用于其他策略上的引导方法。例如，线性半梯度DP (Eq. 9.7 with Ut)。= ?π(圣)|

s?,r p(s ?r |圣,)[r +
γv̂(s ? wt)])与更新根据政策分布也将收敛于TD定点。一步半梯度动作值方法，如下一章所涉及的半梯度Sarsa(0)，收敛到一个类似的不动点和一个类似的界。对于情景性任务，有一个稍微不同但相关的界限(参见Bertsekas和tsiklis, 1996)。在奖励、特性和降低步长参数方面也有一些技术条件，我们在这里省略了这些。完整的细节可以在原文中找到(Tsitsiklis和Van Roy, 1997)。
对这些收敛结果至关重要的是，根据策略分布更新状态。对于其他更新分布，使用函数逼近的自举方法实际上可能会发散到无穷大。第11章给出了这方面的例子和可能的解决方法的讨论。
例9.2:对1000状态随机游走状态聚合的自举是线性函数逼近的一个特例，因此让我们回到1000状态随机游走来说明本章的一些观察结果。图9.2的左面板显示了使用相同的状态聚合(如示例9.1)所学习的半梯度TD(0)算法(page 203)所学习的最终值函数。我们看到，与图9.1所示的蒙特卡罗逼近相比，接近渐近的TD逼近确实距离真实值更远。
尽管如此，TD方法在学习速率方面仍有很大的潜在优势，并且在第7章中，我们用n-step TD方法对蒙特卡罗方法进行了全面的研究。图9.2右边的面板显示了使用n阶半梯度TD方法的结果，该方法使用了1000状态随机漫步上的状态聚合，这与我们之前用表格方法和19状态随机漫步得到的结果非常相似(图7.2)。为了获得定量相似的结果，我们将状态聚合转换为20组，每组50个状态。然后，这20组在数量上接近
 
0.5



0.45




0.35 - 0.4
平均均方根误差
超过1000个州和前10个州。
集

0.3



0.25
 


图9.2:在1000状态随机游走任务上使用状态聚合进行引导。左:半梯度TD的渐近值要比图9.1中的渐近蒙特卡罗值差。正确:具有状态聚合的n步方法的性能与具有表格表示的方法非常相似(cf.图7.2)。这些数据是超过100次的平均值。

到表格问题的19个状态。特别是，回想一下，状态转换在左边或右边最多有100个状态。一个典型的跃迁将是向右或向左50个状态，这在数量上类似于19个状态列表系统的单状态跃迁。为了完成匹配，我们在这里使用相同的性能度量——对所有状态和前10集的RMS误差的非加权平均——而不是使用函数逼近时更合适的VE目标。

上面示例中使用的半梯度n-step TD算法是第7章提出的表格n-step TD算法对半梯度函数逼近的自然扩展。伪代码在下面的框中给出。
 

该算法的关键方程类似于(7.2)

wt + n
。=ωt + n−1 +α(Gt:t + n−v̂(圣、wt + n−1)]∇v̂(St,wt + n−1),0≤t < t(9.15)

n阶返回从(7.1)推广到哪里

Gt:t + n。= Rt + 1 +γRt + 2 +···+γn−1 Rt + n +γnv̂(wt圣+ n + n−1),0≤t≤t−n。(9.16)练习9.1显示，如本书第一部分所介绍的表列方法是线性函数逼近的一个特例。特征向量是什么??

210年 					第9章:带有近似的政策预测


\section{线性方法的特征构造}

线性方法之所以有趣，是因为它们的收敛性保证，但也因为在实践中，它们在数据和计算方面都是非常有效的。无论这是否如此，关键取决于各州如何以特征来表现，我们在这一大块区域进行调查。选择适合于任务的特性是向强化学习系统中添加先验域知识的重要方法。直觉上，这些特征应该对应于状态空间的各个方面，在这些方面可以适当地进行泛化。例如，如果我们对几何对象进行估值，我们可能想要为每个可能的形状、颜色、大小或函数提供特征。如果我们评估一个移动机器人的状态，那么我们可能想要有关于位置、剩余电量的程度、最近的声纳读数等等的特性。
线性形式的局限性是它不能考虑任何之间的交互功能,如功能的存在我好只是在缺乏特性j。例如,在pole-balancing任务(例如3.4)高角速度可以是好是坏取决于角度。如果角度很高，那么高的角速度意味着即将下降的危险——一种糟糕的状态——而如果角度很低，那么高的角速度意味着极点正在调整自身——一种良好的状态。如果一个线性值函数的特征分别表示角度和角速度，它就不能表示这个。相反，它需要这两个底层状态维度的组合的特性。在下面的小节中，我们将考虑各种通用的方法。


\subsection{多项式}
许多问题的状态最初被表示为数字，例如在平衡的任务中的位置和速度(例3.4)，在杰克的汽车租赁问题中的每一辆车的数量(例4.2)，或赌徒的资本在赌徒问题(例4.3)。在这些类型的问题中，函数逼近增强学习与我们熟悉的插值和回归任务有很多共同之处。通常用于插补和回归的各种特征族也可用于强化学习。多项式是用于插值和回归的最简单的特征族之一。虽然我们在这里讨论的基本多项式特征在强化学习中并不像其他类型的特征那样有效，但它们作为一个很好的介绍，因为它们简单而熟悉。
作为一个例子，假设一个强化学习问题有两个数值维度的状态。一个代表国家,让它的两个数字是s1∈R和s2∈R .您可以选择代表年代仅仅通过两个州的维度,因此x(s)=(s1,s2)?但是，你将不能考虑这些维度之间的任何相互作用。此外，如果s1和s2都为0，那么近似值也必须为0。这两个限制都可以通过用四维特征向量x(s) = (1,s1, s2, s1s2)来表示s来克服。最初的1特性允许在原始状态数中表示仿射函数，而最终的产品特性s1s2则允许考虑交互。或者你可以选择使用高维的特征向量比如x(s) = (1 s1s2 s2 s1s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2 s2来
\subsection{傅里叶基础}

另一种线性函数近似方法基于傅立叶级数，它将周期函数表示为不同频率的正弦和余弦基函数的加权和(特征)。(一个函数是周期如果f(x)= f(x +τ)对于所有x和一些时间τ。)傅里叶级数和更一般的傅里叶变换广泛应用于应用科学,部分是因为如果一个函数近似,然后给出了基函数的权重通过简单的公式,此外,有足够的基函数本质上任何函数可以近似为准确。在强化学习中，当要逼近的函数未知时，傅里叶基函数很有趣，因为它们易于使用，并且在一系列强化学习问题中表现良好。
首先考虑一维情况。通常函数的傅里叶级数表示的一维时间τ表示函数的线性组合正弦和余弦函数均匀划分的每个周期与时间τ(换句话说,其频率是基频的整数倍1 /τ)。但是如果你感兴趣的近似一个非周期函数定义在有界区间,然后您可以使用这些傅里叶基础特性与τ区间长度。
有趣的函数就是正弦函数和余弦函数的周期线性组合。
此外,如果您设置τ间隔长度的两倍利息和限制注意近似半区间[0,τ/ 2],然后你可以用余弦特性。这是可能的，因为你可以表示任何偶函数，也就是说，任何关于原点对称的函数，只有余弦基。所以任何函数的半周期(0,τ/ 2)可以近似为密切的有足够的余弦特性。(说“任何函数”都不是完全正确的，因为函数必须在数学上表现良好，但这里我们跳过了这个技术性问题。)或者，也可以只使用正弦函数的特征，它们的线性组合总是奇函数，也就是关于原点的不对称函数。但是最好只保留余弦函数，因为“半偶数”函数比“半奇数”函数更容易近似，因为后者在原点处通常是不连续的。当然,这并不排除使用正弦和余弦特性近似在区间[0,τ/ 2],在某些情况下这可能是有利的。
按照这一逻辑,让τ= 2,这样定义的特点是half-τ区间[0,1],一维傅里叶余弦n基础由n + 1的特性

习近平(s)= cos(iπs)年代∈[0,1],
对于i = 0，…图9.3给出了一维傅里叶余弦的特征xi，对于i = 1,2,3,4;x0是一个常数函数。
 

图9.3:一维傅里叶余弦基特征xi, i = 1,2,3,4，用于在区间[0,1]上逼近函数。在Konidaris等(2011)之后。

这种推理同样适用于多维情况下的傅立叶余弦级数逼近，如下面的框所示。
 
产品年代?ci具有在{0，…在一维情况下，这个整数决定了特征沿该维度的频率。这些特性当然可以根据特定应用程序的有界状态空间进行移动和缩放。

以k = 2为例，其中s = (s1, s2)?，其中每个ci = (ci 1, ci 2)?图9.4显示了六个傅立叶余弦特征的选择，每个特征都由定义它的向量ci标记(s1是水平轴，ci显示为一个行向量，索引我省略了)。任何在c中的零意味着这个特征在状态维上是不变的。如果c = (0,0)，特征在两个维度上都是常数;如果c = (c1, 0)?该特征在第二个维度上是常数，在第一个维度上随着频率的变化而变化，取决于c1;类似地，对于c = (0,c2)?当c = (c1, c2)时?当cj都不为0时，特征在两个维度上都不同，表示两个状态变量之间的交互。c1和c2的值决定了每个维度上的频率，它们的比值给出了交互的方向。
 
 
1



图9.4:6个二维傅立叶余弦特征的选择，每个特征都由定义它的向量ci标记(s1是水平轴，ci用索引i省略)。在Konidaris等(2011)之后。

在学习算法(9.7)、半梯度TD(0)或半梯度Sarsa中使用傅里叶余弦特征时，可能需要对每个特征使用不同的步长参数。如果基本的步长参数α,然后Konidaris Osentoski,和托马斯(2011)建议设置步长参数特性xiαi =α/ ?(ci 1)2 +···+ k(ci)2(除非每个ci j = 0,在这种情况下αi =α)。
傅立叶余弦特征与Sarsa相比可以产生良好的性能

基函数的其他集合，包括多项式和径向基函数。然而，并不奇怪的是，傅里叶特征在不连续上有问题，因为除非包含非常高的频率基函数，否则很难避免在不连续点周围“环绕”。
n阶傅里叶基础的数量特征与状态空间的维数呈指数级增长,但如果这尺寸足够小(例如,k≤5),可以选择一个n,然后可以使用所有的n阶傅里叶特征。这使得特性的选择几乎是自动的。然而，对于高维状态空间，有必要选择这些特性的子集。这可以用先验的关于函数性质的信念来完成，并且一些自动化的选择方法可以适应增强学习的增量和非固定的性质。傅里叶基础特性在这方面的一个优势是,它很容易选择特征向量通过设置ci占疑似状态变量之间的相互作用和通过限制cj向量中的值,这样近似就可以滤除高频组件被认为是噪音。另一方面，由于傅里叶特性在整个状态空间上是非零的(除了少数几个零之外)，它们代表了状态的全局属性，这使得找到表示本地属性的好方法变得困难。
图9.5显示了在1000态随机漫步示例上比较傅里叶和多项式基础的学习曲线。一般来说，我们不推荐使用多项式进行在线学习
 
. 3

已经
平均
30多个运行
0
0 					5000年
集

图9.5:在1000状态随机游走时的傅立叶基与多项式。所示为梯度蒙特卡罗方法的学习曲线，该方法具有阶为5、10和20的傅里叶和多项式基。每种情况下的优化步长参数大致:α= 0.0001的多项式基础和α= 0.00005的傅里叶。性能度量(y轴)是均方根值误差(9.1)。


有一些多项式的族比我们讨论过的更复杂，例如，
不同的正交多项式的族，这些可能更好的工作，但目前很少有经验与它们在强化学习。
\subsection{粗编码}

图9.6:粗编码。Generaliza -
从状态s到状态s?取决于
他们的特征的数量
位域(在本例中是圆)重叠。
这些状态有一个共同点，
所以它们之间会有一些泛化。
考虑一个任务，其中状态集的自然表示是一个连续的二维空间。这种情况的一种表示是由状态空间中与圆对应的特征组成的，如图右所示。如果状态在一个圆内，则对应的特征值为1，并表示存在;否则，该特性为0，并表示不存在。这种1 - 0值的特性称为二进制特性。给定一个状态，其中的二进制特征表示状态所在的圆圈，因此对其位置进行粗码。以这种方式表示具有重叠特性的状态(尽管它们不必是圆或二进制)称为粗编码。
假设线性梯度下降函数近似，考虑圆的大小和密度的影响。每个圈对应的是受学习影响的单个权重(w的一个分量)。如果我们在一个状态下训练，空间中的一个点，那么所有圆的权值
相交的状态将会受到影响。因此，由(9.8)可知，近似值函数在圆的联盟内的所有状态都会受到影响，一个点与状态“共同”的圆越多，其影响越大，如图9.6所示。如果圆圈很小，那么泛化将超过一段很短的距离，如图9.7(左)所示，而如果圆圈很大，泛化将超过一段很长的距离，如图9.7(中)所示。此外,
 

图9.7线性函数逼近方法的概化
特征接受域的大小和形状。这三种情况大致相同
特征的数量和密度。

特征的形状将决定泛化的性质。例如，如果它们不是严格的圆形，而是在一个方向上拉长，那么泛化也会受到类似的影响，如图9.7所示(右)。
接受域较大的特征具有广泛的泛化性，但似乎也会将学习函数限制为粗略的近似，无法使识别比接受域的宽度更精细。令人高兴的是，事实并非如此。从一个点到另一个点的初始泛化实际上是由接受域的大小和形状控制的，但是灵敏度，最终可能的最佳辨别能力，更多的是由特征的总数控制的。
示例9.3:粗编码粗编码粗编码粗编码粗编码的粗编码粗编码对接收域大小学习的影响。利用基于粗编码的线性函数逼近(9.7)学习一维方波函数(如图9.8所示)。这个函数的值被用作目标Ut。只有一个维度，接受域是间隔而不是圆。学习是重复三种不同大小的间隔:窄、中、宽，如图底部所示。这三种情况都具有相同的特征密度，大约是学习函数的50倍。在此范围内随机生成训练示例。步长参数α= 0.2
n
,其中n是
一次出现的特性的数量。图9.8显示了在学习过程中在这三种情况下学习到的函数。注意，特征的宽度在早期学习中有很强的影响。具有广泛的特征，泛化趋于广泛;由于特征较窄，只有每个训练点的近邻被改变，使得函数学习起来更加崎岖不平。然而，最终的功能只受特征宽度的影响。接受域形状对泛化影响较大，但对渐近解质量影响不大。
 
狭窄的特性
媒介的功能
广泛的
特性


图9.8:特征宽度对初始泛化(第一行)的强影响和对渐近精度(最后一行)的弱影响的例子。

\subsection{瓦片编码}
块编码是一种用于多维连续空间的粗编码形式，具有灵活性和计算效率。它可能是现代顺序数字计算机最实用的特性表示。
在块编码中，特征的接受域被分组到状态空间的分区中。每个这样的分区都称为瓦片，分区的每个元素都称为瓦片。例如，二维状态空间的最简单的tiling是一个均匀网格，如图9.9所示。这里的磁贴或接收域是正方形而不是图9.6中的圆。如果只使用这个单一的瓦片，那么白点表示的状态将由它所属的单个特性表示;泛化对于同一块内的所有状态都是完整的，而对于它之外的状态则是不存在的。只有一个tiling，我们不会有粗糙的编码，只是一个状态聚合的例子。
 

图9.9:有限的二维空间上的多个重叠网格倾斜。这些瓷砖
在每个维度中，相互抵消的量是一致的。


为了获得粗编码的优势，需要重叠的接收域，根据定义，分区的块不会重叠。为了获得真正的粗代码块编码，使用多个倾斜，每一个偏移量都是块宽度的一小部分。图9.9右边显示了一个带有四个倾斜的简单案例。每一种状态，如白色点所示，在四种倾斜中每一种都正好落在一个瓦片上。这四个块对应于状态发生时变得活跃的四个特性。具体地说，特征向量x(s)对于每个瓦片中的每个瓦片都有一个组件。在本例中有4×4×4 = 64组件,这一切都将成为0除了相对应的四个年代属于的瓷砖。图9.10显示了在1000状态随机漫步示例上，多个偏移倾斜(粗编码)比单个平铺的优点。
块编码的一个直接的实际好处是，由于它与分区一起工作，所以一次活动的特性的总数对于任何状态都是相同的。每个瓦片上都有一个特性，所以显示的特性的总数总是与倾斜的数量相同。这使得步长参数α,设置在一个简单、直观的方法。例如,选择α= 1
n
n是这个数

图9.10:为什么使用粗编码。显示的是在1000状态随机上的学习曲线
用单片多片梯度蒙特卡罗算法进行例子
瓷砖。1000个状态的空间被视为一个单一的连续维度，每200个状态的瓷砖覆盖。多重倾斜被4个状态抵消。步长参数设置,初步学习速率两种情况是一样的,α= 0.0001
单一50瓷砖瓷砖和α= 0.0001/50。
倾斜的结果是精确的单次学习。如果示例年代?→v是训练,然后不管之前估计,v̂(年代,wt),新的估计将v̂(年代,wt + 1)= v .通常希望改变比这更慢,以允许泛化和随机变化的目标输出。例如,一个可能选择α= 1 10 n,在这种情况下,估计训练状态将十分之一的目标在一个更新的方法,和周边国家将更少,瓷砖的数量成正比的共同点。
使用二进制特征向量也获得了计算优势。由于每个分量都是0或1，组成近似值函数(9.8)的加权和几乎是微不足道的。一个简单地计算n的指数，而不是执行d乘法和加法。d主动特征，然后将权重向量的n个对应分量相加。
泛化发生在除训练过的状态之外的其他状态，如果这些状态落在任何一个相同的块中，与相同的块的数量成比例。甚至如何抵消彼此的倾斜也会影响泛化。如果它们在每个维度上都被均匀地偏移，如图9.9所示，那么不同的状态可以以不同的方式进行定性归纳，如图9.11的上半部分所示。八个子图中的每一个都显示了从训练状态到附近点的泛化模式。在这个例子中，有8个倾斜，因此在一个瓦片中的64个分区，这是很明显的，但都是根据这8个模式中的一个。注意，在许多模式中，均匀偏移是如何在对角线上产生强大的效果的。如果倾斜是不对称的，可以避免这些伪影，如图的下半部分所示。这些较低的泛化模式更好，因为它们都以训练状态为中心，没有明显的不对称性。

可能的
概括为统一
抵消瓷砖
 
 

图9.11:为什么不对称偏移量是块编码的首选。显示了从训练状态到附近状态的泛化强度，用小黑色加号表示
八个瓷砖。如果倾斜是均匀偏移(上图)，则泛化中存在对角线伪影和大量的变化，而非对称偏移则泛化更为球形和均匀。



所有情况下的倾斜都被每一个维度上的瓦片宽度的一小部分所抵消。如果w表示瓦片宽度，n表示倾斜次数，则w
n
是一项基本
单位。在小广场w
n
在一边，所有的状态激活相同的磁砖，都是一样的。
特征表示，和相同的近似值。如果一个状态被w移动
n
在任何笛卡尔的方向上，特征表示都由一个组件/块改变。均匀偏移的倾斜度由这个单位距离互相抵消。对于二维空间，我们说每个瓦片被位移矢量(1,1)抵消，这意味着它与之前的瓦片被w抵消
n
乘以这个向量。在这些方面,
图9.11下半部分所示的不对称偏移倾斜被(1,3)的位移矢量所抵消。
大量研究了不同位移矢量对地砖编码泛化的影响(Parks and Militzer, 1991;一个,1991分;一个,米勒和公园,

1991;米勒，An, Glanz和Carter, 1990)，评估他们的同质性和对角工件的趋势，就像(1,1)位移向量所见。基于这项工作，Miller和Glanz(1996)建议使用由第一个奇整数组成的位移向量。特别地，对于维k的连续空间，一个好的选择是使用第一个奇数(1,3,5,7)。,2 k−1),n(瓷砖)的数量设置为2的整数次幂大于或等于4 k。这是我们做过的瓷砖生产图9.11的下半部,k = 2,n = 23≥4 k和位移矢量(1,3)。在三维情况下,前四个瓷砖将抵消总从基地位置(0,0,0),(1、3、5),(2、6、10),(3、9、15)。开源软件可以有效地为任何k做这样的倾斜是容易得到的。
在选择瓦片策略时，必须选择倾斜的数量和瓦片的形状。倾斜的数量，以及贴图的大小，决定了渐近逼近的分辨率或细度，如一般的粗编码所示，如图9.8所示。瓦片的形状将决定泛化的性质，如图9.7所示。在图9.11(下图9.11)所示的每个维度中，方砖的泛化程度大致相同。在一个维度上延伸的瓷砖，如图9.12(中)中的条纹倾斜，将沿着这个维度推广推广。图9.12(中间)中的倾斜在左边也更密集和更薄，在水平方向上沿该维度的较低值促进了识别。图9.12(右)中的对角线条纹条将促进沿一个对角线的泛化。在更高的维度中，轴向条纹对应于忽略一些倾斜的维度，也就是超平面切片。图9.12(左)所示的不规则倾斜也是可能的，尽管在实践中很少出现，而且超出了标准软件。
 
不规则的b) Log条纹c)斜条纹。

图9.12:倾斜不需要网格。它们可以是任意形状和非均匀的
在很多情况下计算效率很高。


在实践中，通常需要在不同的倾斜中使用不同形状的瓷砖。例如，可以使用一些垂直条纹倾斜和一些水平条纹倾斜。这将鼓励在这两个维度上进行泛化。然而，仅凭条纹倾斜就不可能了解水平坐标和垂直坐标的特定结合具有独特的价值(无论学习到什么，它都会以相同的水平坐标和垂直坐标为基础进入状态)。为此，需要如图9.9所示的连接矩形块。有了多重顶点——一些是水平的，一些是垂直的，还有一些是结合点——我们就可以得到所有的东西:偏好沿着每个维度进行归纳，同时也有能力学习连接的特定值(参见Sutton，

1996为例)。倾斜的选择决定了一般化，并且在这个选择能够被有效地自动化之前，重要的是，tile编码能够灵活地做出选择，并且以一种对人们有意义的方式。
另一个减少内存需求的有用的技巧是hash- a，它是一种连续的伪随机崩溃，将一个大的tiling折叠成一个小得多的块。散列产生的块由非连续的、不相交的区域组成，这些区域在整个州内随机分布




一个瓷砖
空间，但那仍然构成一个详尽的分区。例如，一个块可能由右边显示的四个子块组成。通过散列，内存需求通常会被较大的因素所减少，而性能损失很小。这是可能的，因为只有一小部分的状态空间需要高分辨率。哈希将我们从维数的诅咒中解放出来，因为内存需求不必在维数上呈指数级增长，而只需满足任务的实际需求。块编码的开源实现通常包括高效散列。
练习9.4假设我们认为两个状态维度中的一个比另一个更有可能对值函数产生影响，那么泛化应该主要是在这个维度上而不是沿着这个维度。什么样的倾斜可以使用
这种先验知识的优势是什么? 					?


\subsection{径向基函数}
径向基函数(RBFs)是粗编码对连续值特征的自然泛化。而不是每一个特征都是0或1，它可以是区间[0,1]中的任何东西，反映了特征存在的不同程度。典型的RBF特性,习近平,高斯(钟形)响应xi(s)只依赖状态,之间的距离,和功能的原型或中心状态,ci,和相对于功能的宽度,σi:

习近平(年代)。=经验

−
| |年代−ci | | 2
2σ2我

.




标准或距离度量当然可以选择任何看起来最适合当前状态和任务的方式。下图显示了一个具有欧几里得距离度量的一维示例。
 
图9.13:一维径向基函数。

相对于二进制特征，RBFs的主要优点是它们产生的近似函数变化平滑且可微。尽管这很吸引人，但在大多数情况下，它并没有什么实际意义。然而，广泛的研究已经在tile coding (An, 1991)的语境中对RBFs等分级响应函数进行了广泛的研究。米勒et al .,1991;et al .,1991;莱恩，汉德尔曼和盖尔芬德，1992)。所有这些方法都需要大量额外的计算复杂性(通过块编码)，并且当有两个以上的状态维度时，常常会降低性能。在高维情况下，瓷砖的边缘要重要得多，而且已经证明很难在边缘附近得到控制良好的渐变瓷砖激活。
RBF网络是一种基于RBFs的线性函数逼近器。学习是由方程(9.7)和(9.8)定义的，与其他线性函数逼近器完全一样。此外，RBF网络的一些学习方法也会改变特征的中心和宽度，使它们进入非线性函数逼近器的领域。非线性方法可以更精确地拟合目标函数。RBF网络，尤其是非线性RBF网络的缺点是计算复杂度更高，而且通常在学习之前需要更多的手动调优，这是健壮且高效的。


\section{手动选择步长参数}

大多数SGD方法要求设计师选择合适的步长参数α。理想情况下，这个选择是自动的，在某些情况下是自动的，但是在大多数情况下，手动设置它仍然是常见的做法。要做到这一点，并更好地理解算法，开发一些关于步长参数的作用的直觉感觉是很有用的。我们能概括地说它应该如何设置吗?
不幸的是，理论上的考虑没有多大帮助。随机逼近理论给出了一个缓慢减小的步长序列的条件(2.7)，它足以保证收敛，但这些条件往往导致学习速度太慢。经典选择αt = 1 / t,生产样品平均表格MC方法,不适合TD方法,对于不稳定问题,或任何方法使用函数近似。对于线性方法，有设置最优矩阵步长的递归最小二乘方法，这些方法可以扩展到时间差异学习，如第9.8节描述的LSTD方法，但是这些方法需要O(d2)步长参数，或者是d乘以我们正在学习的参数。由于这个原因，我们把它们排除在最需要函数逼近的大问题中。
为了直观地了解如何手动设置步长参数，最好是暂时回到列表框中。我们可以理解,α= 1的步长将导致一个完整的样本误差的消除后一个目标(见(2.4)的步长)。正如第201页所讨论的，我们通常希望学习得慢一些。在表格的情况下,α= 1的步长10大约需要10经验收敛他们的目标,在100年,如果我们想学习经验我们会使用α= 1 100。一般来说,如果α=
1τ
，则状态的列表估计值将
方法的意思是目标,最近的目标有最大的影响,τ的经历后的状态。

对于一般的函数逼近，对于一个状态的经历的数量没有如此清晰的概念，因为每个状态可能在不同程度上与所有其他状态相似或不同。然而，在线性函数近似的情况下，有一个相似的规则给出相似的行为。假设你想学在τ经验实质上相同的特征向量。然后给出了线性SGD方法步长参数设置的一个很好的经验法则


α
。=

τE

x ?

−1
, 					(9.19)

其中x是一个随机特征向量，从与输入向量相同的分布中选择，在SGD中。当特征向量的长度变化不大时，该方法最有效;理想x ?x是一个常数。
练习9.5假设您使用的是瓷砖编码一个7维连续状态空间转换成二进制特征向量估计状态值函数v̂(s,w)≈vπ(年代)。你相信尺寸不相互作用强烈,所以你决定使用八瓷砖的每个维度分别(条纹瓷砖),7×8 = 56瓷砖。此外，如果在维度之间有一些成对的交互，您也需要全部。
7 2 ?

= 21
尺寸对和瓷砖每一对结合与矩形瓷砖。你让两个瓷砖每一对维度,进行总计21×2 + 56 = 98瓷砖。考虑到这些特征向量，你怀疑你仍然需要平均出一些噪音，所以你决定你想要学会循序渐进，在学习接近它的渐近线之前，用同样的特征向量做大约10个演示。什么步长参数α
你会使用吗?为什么? 					?


9.7非线性函数逼近:人工神经网络。

人工神经网络被广泛应用于非线性函数逼近。神经网络是由相互联系的单元组成的网络，它们具有神经元的某些特性，神经元是神经系统的主要组成部分。ANNs有着悠久的历史，在训练深度分层ANNs(深度学习)方面的最新进展是机器学习系统最令人印象深刻的能力，包括强化学习系统。在第16章中，我们描述了几个使用ANN函数逼近的强化学习系统的令人印象深刻的例子。
图9.14显示了一个通用的前馈神经网络，这意味着网络中没有循环，也就是说，网络中没有路径可以让一个单元的输出影响它的输入。图中的网络有一个输出层，由两个输出单元组成，一个输入层有四个输入单元，以及两个“隐藏层”:既不是输入层也不是输出层的层。实值权重与每个链接相关联。重量大致相当于真实神经网络中突触连接的效力(见第15.1节)。如果一个ANN在它的连接中至少有一个循环，那么它是一个循环，而不是前馈的ANN。虽然前馈和周期性ann都被用于强化学习，但这里我们只看比较简单的前馈情况。
 
图9.14:一个具有四个输入单元、两个输出单元和两个隐藏层的通用前馈神经网络。



单元(图9.14中的圆圈)通常是半线性单元，这意味着它们计算输入信号的加权和，然后将一个非线性函数(称为激活函数)应用于结果，以产生单元的输出或激活。使用不同的激活函数,但他们通常s形,或乙状结肠,物流等功能函数f(x)= 1 /(1 + e−x),尽管有时整流器非线性f(x)= max(0,x)。一个阶跃函数f(x)= 1如果x≥θ,和0否则,结果在一个二进制单元阈值θ。网络输入层中的单元在将它们的激活设置为外部提供的值时有所不同，这些值是网络正在逼近的函数的输入。
前馈神经网络的每个输出单元的激活是网络输入单元上激活模式的非线性函数。函数由网络的连接权重参数化。没有隐藏层的ANN只能表示可能的输入输出函数的很小一部分。然而，具有一个包含足够大的有限数量的乙状结肠单元的单一隐藏层的ANN，可以在网络输入空间的紧凑区域上以任何程度的精确度近似任何连续函数(Cybenko, 1989)。这也适用于其他非线性激活函数满足条件温和,但是非线性是至关重要的:如果所有的单位在一个多层前馈神经网络有线性的激活函数,整个网络就相当于一个网络没有隐藏层(因为线性函数的线性函数是线性)。
尽管这“万能逼近”属性one-hidden-layer ann,经验和理论表明,近似许多人工智能任务所需的复杂的功能是由easier-indeed可能require-abstractions层次组成的许多低级别的抽象层,即抽象由深架构如人工神经网络有许多隐藏层。(详见2009年的Bengio详细回顾。)深ANN的连续层逐渐地计算

网络的“原始”输入的抽象表示，每个单元提供一个特性，用于对网络的总体输入-输出函数进行分层表示。
因此，训练ANN的隐藏层是一种自动创建适合于给定问题的特性的方法，这样就可以在不依赖手工制作的特性的情况下生成分层表示。这对人工智能来说是一个持久的挑战，也解释了为什么具有隐藏层的人工智能学习算法近年来受到如此多的关注。ANNs通常通过随机梯度法学习(第9.3节)。每一权值的调整方向都是为了提高网络的整体性能，目标函数是最小化或最大化。在最常见的监督学习案例中，目标函数是预期的错误或损失，而不是一组标记的训练示例。在强化学习中，ANNs可以使用TD错误来学习价值函数，或者可以像梯度班迪特(第2.8节)或政策梯度算法(第13章)那样，以最大化预期的回报为目标。在所有这些情况下，需要估计每个连接权重的变化将如何影响网络的整体性能，换句话说，考虑到所有网络权重的当前值，估计目标函数对每个权重的偏导。梯度是这些偏导数的向量。
对于具有隐藏层的人工神经网络(如果这些单元具有可微的激活功能)，最成功的方法是反向传播算法，它由交替向前和向后穿过网络组成。每个前向传递计算每个单元的激活，给定网络输入单元的当前激活。每次向前传球后，向后传球有效地计算每一重量的偏导数。(和其他随机梯度学习算法一样，这些偏导数的向量是真实梯度的估计值。)在第15.10节中，我们讨论了使用增强学习原理而不是反向传播的隐藏层进行训练的方法。这些方法不如反向传播算法有效，但它们可能更接近于真实神经网络的学习方式。
对于具有1或2个隐藏层的浅层网络，反向传播算法可以产生良好的结果，但对于较深的ANNs，它可能不太适用。事实上，训练一个有k + 1隐藏层的网络实际上比训练一个有k隐藏层的网络性能差，即使深层网络可以代表浅层网络所能代表的所有功能(Bengio, 2009)。解释这样的结果并不容易，但有几个因素很重要。首先，典型的深度神经网络中权重的大量存在，使其难以避免过度拟合的问题，即不能正确地推广到尚未对网络进行训练的情况。第二,反向传播不适合深ann因为偏导数计算的向后传球要么衰变迅速向网络的输入端,使学习的深度层极慢,或偏导数快速增长对网络的输入端,使学习不稳定。处理这些问题的方法在很大程度上要归功于使用深的ANNs系统所取得的许多令人印象深刻的最新成果。
超拟合是任何函数逼近方法调整函数的问题

在有限的训练数据的基础上有许多自由度。不依赖有限的训练集的在线强化学习问题较少，但有效的归纳仍然是一个重要的问题。一般来说，过拟合是ANNs的一个问题，但对深的ANNs来说尤其如此，因为它们往往有大量的重量。人们已经开发出许多减少过拟合的方法。其中包括当验证数据(交叉验证)的性能开始下降时停止训练(交叉验证)，修改目标函数以阻止逼近的复杂性(正则化)，引入权重之间的依赖关系以减少自由度(例如，权重共享)。
Srivastava、Hinton、Krizhevsky、Sutskever和Salakhutdinov(2014)等人提出的辍学率法是一种特别有效的深部ANNs拟合方法。在训练过程中，单位与他们的连接一起被随机地从网络中删除(退出)。这可以被认为是训练了大量的“瘦”网络。结合测试时这些稀疏网络的结果，可以提高泛化性能。辍学率法通过将一个单元的每个出站重量乘以该单元在训练中被保留的概率来有效地近似这个组合。Srivastava等发现，该方法显著提高了泛化性能。它鼓励单个隐藏单元学习与其他特性的随机集合一起工作的特性。这增加了隐藏单元所形成的特征的多样性，这样网络就不会过度专门化到罕见的情况。
Hinton、Osindero和Teh(2006)在他们的研究中，利用深度信念网络，即与这里讨论的深度ANNs密切相关的分层网络，向解决训练深层ANN的问题迈出了重要的一步。在他们的方法中，最深层的层次一次一个的训练使用一个无监督的学习算法。无监督学习不依赖于整体目标函数，可以提取出捕获输入流统计规律的特征。最深的层首先被训练，然后由这个训练过的层提供输入，下一个最深的层被训练，等等，直到网络的所有或许多层的权重被设置为现在作为监督学习的初始值。然后通过反向传播对整个目标函数进行微调。研究表明，这种方法通常比用随机值初始化权值的反向传播效果更好。通过这样的方式来训练的网络性能越好，这可能是由许多因素造成的，但有一种观点认为，这种方法将网络置于一个权重空间的区域，而基于梯度的算法能够取得良好的进展。
批标准化(Ioffe和Szegedy, 2015)是另一种更容易训练深度ANNs的技术。人们早就知道，如果网络输入是规范化的，比如将每个输入变量调整为零均值和单位方差，那么学习ANN会更容易。深度ANNs训练的批处理规范化在深层输入到下一层之前将其输出规范化。Ioffe和Szegedy(2015)利用训练样本的子集或“小批量”的统计数据，将这些层间信号规范化，以提高深部ANNs的学习速度。
另一种有助于训练深层记忆的技术是深度剩余学习(他、张、任、孙，2016)。有时，我们更容易了解一个函数与另一个函数的区别

恒等函数比学习函数本身更重要。然后，将这个差值或剩余函数添加到输入中，就会产生所需的函数。在深度ANNs中，只需在块周围添加快捷键或跳过连接，就可以创建一组层来学习一个剩余函数。这些连接将输入添加到块的输出中，不需要额外的权重。他等人(2016)对这种方法进行了评估，使用了深度卷积网络，在每对相邻层之间都有跳过连接，发现在基准图像分类任务中没有跳过连接的网络有了实质性的改进。在第16章中所描述的“Go”游戏的强化学习应用中使用了批量标准化和深度剩余学习。
深层神经网络被证明在应用中非常成功，包括令人印象深刻的强化学习应用(第16章)，它就是深层卷积网络。这种类型的网络专门用于处理空间数组(如图像)中的高维数据。它的灵感来自于早期视觉处理在大脑中的工作原理(LeCun, Bottou, Bengio和Haffner, 1998)。由于其特殊的结构，深层卷积网络可以通过反向传播进行训练，而不需要使用上面描述的方法来训练深层层。
图9.15展示了深层卷积网络的架构。这个例子，来自LeCun等人(1998)，旨在识别手写字符。它由交替的卷积层和次采样层组成，然后是几个完全连接的最后层。每个卷积层产生许多特征映射。特征映射是一个活动的模式在一个数组的单元,在每个单元上执行相同的操作数据的接受域,数据的一部分它“看到”的前层(或从外部输入的第一个卷积层)。特征图的单位是相同的，除了它们的接收域都是相同的大小和形状，被移动到输入数据数组的不同位置。同一地形图中的单元拥有相同的权重。这意味着，无论功能映射位于输入数组的哪个位置，它都会检测到相同的功能。在网络在图9.15中,例如,第一次卷积层产生的6个特征图,每个28×28个单位组成。每个单元在每个特性图5×5接受域,这些接受领域重叠四列(在本例中,4
 

图9.15:深度卷积网络。经批准，重新出版
IEEE，从基于梯度的学习应用到文档识别，LeCun, Bottou, Bengio和Haffner，第86卷，1998;许可通过版权许可中心，Inc。

行)。因此，6个特征图中的每一个都由25个可调权值指定。深度卷积网络的子采样层降低了空间分辨率
特征图。在一个子采样层中，每一个特征图都由在前卷积层的特征图中接受单元域上的平均单元组成。例如,每个单元的6个特征图前的二次抽样层网络图9.15平均在2×2重叠接受域在产生的特征图的第一个卷积层,导致六14×14个特征图。子采样层降低了网络对检测到的特征的空间位置的敏感性，也就是说，它们有助于使网络的响应在空间上不变性。这是很有用的，因为在图像中某个位置检测到的特性可能在其他位置也有用。
在annis的设计和培训方面的进步——我们只提到了少数——都有助于强化学习。虽然目前的强化学习理论大多局限于使用表格函数或线性函数逼近方法的方法，但值得注意的强化学习应用的显著性能很大程度上归功于多层神经网络的非线性函数逼近。我们将在第16章中讨论其中的一些应用。


\section{最小二乘道明}

到目前为止，我们在本章讨论过的所有方法都需要计算与参数数量成正比的时间步长。然而，有了更多的计算，就可以做得更好。在本节中，我们将介绍一种线性函数逼近的方法，它可以被认为是这种情况下所能做的最好的方法。
正如我们在9.4 TD(0)中建立的线性函数逼近渐近收敛(对于适当减小的步长)到TD不动点:

西医=−1 b,

一个。= E

xt(xt−γxt + 1)?和b。= E(Rt + 1 xt)。
有人可能会问，为什么我们必须迭代地计算这个解?这是浪费数据!通过计算A和b的估计值，然后直接计算TD的不动点，难道不能做得更好吗?最小二乘TD算法(通常称为LSTD)就是这样做的。它形成了自然的估计


吗?在
。=
t−1 ?k = 0
xk(xk−γxk + 1)?+εI吗?英国电信。=
t−1 ?k = 0
Rt + 1 xk,(9.20)

我是单位矩阵,εI,对于一些小的ε> 0,确保吗?在都是可逆的。看起来这些估计都应该除以t，事实上它们应该;根据定义，这些是t乘以A和t乘以b的估计值。

然而，当LSTD使用这些估计来估计TD的不动点时，额外的t因子就会抵消掉
wt
。=
吗?−1 t ?英国电信。 					(9.21)

该算法是线性TD(0)中数据效率最高的一种算法，但计算开销也较大。回想一下，半梯度TD(0)只需要O(d)的内存和每步计算。
LSTD有多复杂?正如上面所写的那样，复杂性似乎随着t而增加，但是(9.20)中的两个近似可以用我们之前讨论过的技术(例如，在第2章中)增量地实现，这样它们就可以在每一步的固定时间内完成。即便如此，更新的目的是什么?将涉及一个外积(一个列向量乘以一个行向量)，因此将是一个矩阵更新;它的计算复杂度是O(d2)，当然，它所需要的内存是多少?矩阵是O(d2)
一个潜在的更大的问题是我们的最终计算(9.21)使用的是?一般逆向计算的计算复杂度为O(d3)。幸运的是，我们的特殊形式的一个矩阵的逆矩阵——外部产物的和——也可以只通过O(d2)计算来增量地更新
 

对于t > 0，有?A0
。=εI。虽然身份(9.22)，被称为Sherman-Morrison
公式，表面上是复杂的，它只涉及向量矩阵和向量向量乘法，因此只有O(d2)。这样我们就可以存储逆矩阵了?−1 t,维持它与(9.22),然后使用(9.21),所有只有O(d2)内存和每一步计算。完整的算法在下一页的框中给出。
当然，O(d2)仍然比半梯度TD的O(d)昂贵得多。LSTD的更大数据效率是否值得这个计算费用取决于d的大小，快速学习的重要性，以及系统其他部分的开销。LSTD不需要步长参数的事实有时也被吹捧，但这一优点可能被夸大了。LSTD并不需要一个步长,但它确实需要ε;如果ε选择太小的逆顺序可以大相径庭,如果选择ε太大学习是放缓。此外，LSTD缺少一个步长参数意味着它永远不会忘记。这有时是可取的,但它是有问题的,如果目标政策π与强化学习和谷歌价格指数的变化。在控制应用程序中，LSTD通常必须与其他一些机制相结合，以诱导forgeting，提出不需要步长参数的任何初始优势。

\section{基于内存的函数近似}

到目前为止，我们已经讨论了逼近值函数的参数化方法。在这种方法中，学习算法调整函数形式的参数，以便在问题的整个状态空间中估计值函数。每次更新,年代?→g,使用的是一个训练的例子学习算法改变参数,目的是减少近似误差。在更新之后，可以丢弃训练示例(尽管它可能被保存以再次使用)。当需要一个状态的近似值(我们称之为查询状态)时，只需使用学习算法生成的最新参数在该状态下对函数进行评估。
基于内存的函数逼近方法是非常不同的。它们只是在到达时将训练示例保存在内存中(或者至少保存示例的子集)，而不更新任何参数。然后，每当需要查询状态的值估计时，就从内存中检索一组示例并用于计算查询状态的值估计。这种方法有时被称为延迟学习，因为处理训练示例要延迟到查询系统以提供输出时。
基于记忆的函数近似方法是非参数方法的主要例子。与参数化方法不同，逼近函数的形式并不局限于一个固定的参数化函数类，如线性函数或多项式，而是由训练示例本身决定，以及将它们与查询状态的输出估计值相结合的一些方法。随着越来越多的训练例子在内存中积累，我们期望非参数方法能够对任何目标函数产生越来越精确的逼近。

根据如何选择存储的训练示例以及如何使用它们响应查询，有许多不同的基于内存的方法。在这里，我们将重点放在局部学习方法上，这些方法仅在当前查询状态的附近局部近似一个值函数。这些方法检索一组训练的例子从记忆的状态被认为是最相关的查询状态,在相关性通常取决于国家之间的距离:越接近训练实例的状态查询状态,更相关的是,距离可以以许多不同的方式定义。在给查询状态一个值之后，将丢弃局部逼近。
基于内存的方法最简单的例子是最近的邻居方法，它只在内存中找到最接近查询状态的实例，并返回该实例的值作为查询状态的近似值。换句话说，如果查询状态是s和s??→g内存中的示例在哪个年代?是最近的状态,然后返回g s。稍微复杂的近似值是加权平均的方法检索一组最近邻的例子并返回一个目标值的加权平均,权重的一般减少与增加查询状态和状态之间的距离。局部加权回归类似，但它通过参数逼近方法将加权误差度量(如(9.1)降至最小，使权重依赖于距离查询状态的距离，使其与一组最接近状态的值吻合。返回的值是对查询状态下的局部拟合曲面的求值，然后将局部逼近曲面丢弃。
由于是非参数的，基于内存的方法比不将逼近限制在预先指定的函数形式的参数方法具有优势。这允许随着数据的积累而提高准确性。基于记忆的局部逼近方法有其他的性质，使它们非常适合于强化学习。由于轨迹采样在增强学习中非常重要，如第8.6节所讨论的，基于内存的局部方法可以将函数逼近集中在真实轨迹或模拟轨迹中访问的状态(或状态-动作对)的局部邻域上。可能不需要全局近似，因为状态空间的许多区域永远(或几乎永远不会)到达。此外，基于内存的方法允许代理的经验对当前状态附近的值估计产生相对直接的影响，与参数方法需要增量地调整全局逼近的参数形成对比。
避免全局逼近也是解决维度诅咒的一种方法。例如,对于与k维状态空间,一个表存储全局近似方法需要记忆指数k。另一方面,在存储的例子基于内存的方法,每个例子都需要内存k成正比,和所需的内存来存储,说,n是线性的例子在n。没有什么是指数k或n。当然,剩下的关键问题是一个基于内存的方法是否能回答查询很快是有用的一个代理。一个相关的问题是，随着内存的增加，速度会如何降低。在大型数据库中查找最近的邻居可能会花费太长时间，在许多应用程序中都不太实用。

基于内存的方法的支持者已经开发出加速最近邻居搜索的方法。使用并行计算机或专用硬件是一种方法;另一种方法是使用特殊的多维数据结构来存储训练数据。这个应用程序研究的一个数据结构是k-d树(k维树的缩写)，它递归地将k维空间分割成一棵二叉树的节点。根据数据量和数据在状态空间上的分布方式，使用k-d树的最近邻搜索可以快速地消除搜索邻域中的大片区域，使搜索在一些朴素搜索时间过长的问题中可行。
局部加权回归还需要快速的方法来进行局部回归计算，这些计算必须重复来回答每个查询。研究人员已经开发了许多方法来解决这些问题，包括忘记条目的方法，以便使数据库的大小保持在一定范围内。本章末尾的书目和历史评论部分指出了一些相关的文献，包括描述基于记忆的学习在强化学习中的应用的论文选集。


\section{基于函数逼近}

基于内存的方法，如上面描述的加权平均和局部加权回归方法，依赖于给示例赋权??→g数据库中根据年代之间的距离?查询状态为s。分配这些权重的函数称为核函数，或者简单地称为核函数。加权平均和局部加权回归方法,例如,一个内核函数k:R→分配权重之间的距离。更一般地说，重量不必依赖于距离;它们可以依赖于国家间相似度的其他度量。在这种情况下,凯西:S×S→R,这k(S,S)给出的重量数据年代呢?它对回答关于s的问题的影响。
从稍微不同的角度来看，k(s, s?)是对从s到s的泛化强度的度量。对于s。核函数的数值表示任何状态的相关知识与其他状态的关系。例如，图9.11所示的块编码泛化的强度对应于由于均匀和不对称的块偏移而产生的不同的核函数。虽然块编码在其操作中没有显式地使用内核函数，但它是根据内核函数进行推广的。事实上，正如我们在下面讨论的那样，由线性参数函数逼近所产生的泛化强度总是可以用核函数来描述的。
内核回归是一种基于内存的方法，它计算存储在内存中的所有示例的内核加权平均，并将结果分配给查询状态。如果D是存储的示例集，而g(s?)表示状态s的目标?在存储的示例中，然后内核回归逼近目标函数，在本例中，值函数依赖于D，如

v̂(s、D)= ?
D s ?∈
k(s,s)g(s ?)。 					(9.23)

上面描述的加权平均方法是一种特殊情况，只有当s和s时k(s, s?)才是非零的。它们之间的距离很近，所以不需要对整个D进行求和。
常见的核是用于RBF函数近似的高斯径向基函数(RBF)，如第9.5.5节所述。在这里描述的方法中，RBFs是中心和宽度从一开始就固定的特性，中心可能集中在许多示例预计会下降的区域，或者在学习过程中以某种方式进行调整。这是一种线性参数化方法，其参数为每个RBF的权重，通常通过随机梯度或半梯度下降来学习。近似的形式是预先确定的RBFs的线性组合。使用RBF内核的内核回归在两个方面与此不同。首先，它是基于内存的:RBFs以存储示例的状态为中心。第二，它是非参数化的:没有参数可以学习;查询的响应由(9.23)给出。
当然，在实际实现内核回归时需要解决许多问题，这些问题超出了我们简短讨论的范围。然而，事实证明，任何线性参数回归方法，如我们在第9.4节中描述的，状态由特征向量x(s) = (x1(s)， x2(s)，…,xd(s))?k(s, s?)是s和s的特征向量表示的内积;这是

k(s,s)= x(s)? x(?)。 					(9.24)

使用这个核函数的核回归产生的近似与线性参数方法使用这些特征向量并学习相同的训练数据时所产生的近似相同。
我们跳过了任何现代机器学习文本(如Bishop(2006))中都可以找到的数学理由，只简单地指出一个重要的含义。与其构造线性参数函数近似器的特征，还不如直接构造核函数而不涉及特征向量。并不是所有的核函数都可以表示为特征向量的内积(9.24)，但是可以像这样表示的核函数可以在等价的参数化方法上提供显著的优势。对于许多特征向量集，(9.24)有一个紧凑的函数形式，可以在没有任何计算的d维特征空间中进行计算。在这些情况下，与直接使用由这些特征向量表示的状态的线性参数方法相比，核回归要简单得多。这就是所谓的“内核技巧”，它允许在扩展特性空间的高维空间中有效地工作，而实际上只使用存储的训练示例集。核心技巧是许多机器学习方法的基础，研究人员已经展示了它有时是如何有益于强化学习的。

\section{对政策学习的深入研究:兴趣和重点}

到目前为止，我们在本章中所考虑的算法已经把遇到的所有状态都平等地对待，就好像它们都同样重要一样。然而，在某些情况下，我们比其他国家更感兴趣。例如，在折现的偶发问题中，我们可能更感兴趣的是准确地估计事件中早期状态的价值，而不是在后来的状态中，打折可能会使奖励对起始状态的价值不那么重要。或者，如果正在学习一个行为价值函数，那么精确地评估价值远低于贪婪行为的糟糕行为可能就不那么重要了。函数逼近资源总是有限的，如果以更有针对性的方式使用它们，那么性能就可以得到改善。
我们平等对待所有国家的一个原因是，我们正在根据政策的分配情况进行更新，因为有更强的理论结果可以用于半梯度法。回想一下，on-policy分布定义为在执行目标策略时在MDP中遇到的状态分布。现在我们将显著地推广这个概念。我们不会为MDP提供一个政策上的分配，我们会有很多。它们都有一个共同点，它们都是在执行目标政策时，在轨迹中遇到的状态的分布，但它们在轨迹如何，在某种意义上，初始化上是不同的。
我们现在介绍一些新概念。首先我们引入一个非负标量测度，一个叫做兴趣的随机变量，表示我们对t时刻的状态(或状态-动作对)进行精确估值的程度。如果我们完全关心它，它可能是1，尽管它被正式地允许取任何非负值。兴趣可以随意设定;例如,它可能取决于时间t的轨迹或学习参数在时间t。VE中的分布μ(9.1)被定义为状态的分布时遇到的目标政策后,加权的兴趣。其次，我们引入另一个非负标量随机变量，强调Mt。这个标量乘以学习更新，从而强调或不强调在t时刻完成的学习


wt + n
。=ωt + n−1 +αMt(Gt:t + n−v̂(圣、wt + n−1)]∇v̂(St,wt + n−1),0≤t < t(9.25)

由(9.16)给出的n阶回报，以及由利息递归确定的重点:

太=,+γnMt−n,0≤t < t, 					(9.26)

与太
。= 0，对于所有t < 0。这些方程包括蒙特卡罗的情况，
的Gt:t + n = Gt,所有的更新一集结束,n = t−t,太=。
示例9.4说明了兴趣和重点是如何导致更精确的价值估计的。

9.11。更深入地研究政策学习:兴趣和重点。 					235年

\section{总结}
强化学习系统必须能够概括，如果它们适用于人工智能或大型工程应用。为了实现这一点，任何现有的用于监督学习函数近似的方法都可以简单地使用—将每个更新作为一个训练示例。
也许最合适的监督学习方法是使用参数化函数逼近的方法，其中策略是由权向量w参数化的。虽然权向量有很多分量，但状态空间更大，我们必须满足于一个近似解。我们定义了均方值错误,VE(w),作为一个测量的误差值vπw(s)的权向量w在政策下分布,μ。VE为我们提供了一种清晰的方法来对不同的价值函数逼近进行排序。
为了找到一个好的权值向量，最常用的方法是随机梯度下降法。在本章中，我们重点讨论了一个固定政策的政策案例，也就是政策评估或预测;这种情况下的自然学习算法是n-step semi-gradient TD,包括梯度蒙特卡罗和semi-gradient TD(0)算法作为特殊情况分别当n =∞和n = 1。半梯度TD方法不是真正的梯度方法。在这种bootstrapping方法(包括DP)中，权重向量出现在更新目标中，但在计算梯度时没有考虑到这一点——因此它们是半梯度方法。因此，他们不能依赖经典的SGD结果。
然而，在线性函数逼近的特殊情况下，半梯度方法可以得到很好的结果，其中值估计是特征的和乘以相应的权值。线性情况在理论上是最容易理解的，当提供适当的特征时，在实践中也很有效。选择特征是在强化学习系统中增加先验知识的最重要的方法之一。它们可以被选择为多项式，但这种情况在强化学习中通常考虑的在线学习设置中推广得很差。更好的方法是根据傅立叶基，或者根据具有稀疏重叠接受域的粗编码形式来选择特征。块编码是一种粗编码的形式，它在计算效率和灵活性方面尤为突出。径向基函数对于一个或两个二维的任务是有用的，其中一个平滑变化的响应是重要的。LSTD是最具数据效率的线性TD预测方法，但它要求计算与权值的平方成正比，而其他所有方法的权值都是线性的。非线性方法包括反向传播训练的人工神经网络和SGD的变异;这些方法近年来在“深度强化学习”这个名字下变得非常流行。
线性半梯度n阶TD保证在标准条件下收敛，对于所有n，收敛到一个在最优误差范围内的VE(蒙特卡罗方法渐近实现)。这个绑定总是严格高n和趋于0 n→∞。然而,在实践中非常高的n导致非常缓慢的学习,和某种程度的引导(n <∞)通常是preferrable,正如我们看到在第七章表格n-step方法的比较和对比的表格TD和蒙特卡罗方法在第六章。

\section{书目的和历史的言论}

概化和函数逼近一直是强化学习的重要组成部分。Bertsekas和tsiklis(1996)、Bertsekas(2012)和Sugiyama等人(2013)提出了函数逼近在强化学习中的最新进展。本文最后讨论了函数逼近在强化学习中的一些早期工作。

9.3监督下最小化均方误差的梯度下降法
学习是众所周知的。Widrow和Hoff(1960)引入了最小均方(LMS)算法，这是一种典型的增量梯度下降算法。这个和相关算法的细节在许多文本中都有提供(例如，Widrow和Stearns, 1985;主教,1995;杜达和Hart,1973)。
Semi-gradient TD(0)首次探索萨顿(1984、1988),作为线性TD(λ)算法的一部分,我们将在第12章。描述这些自举方法的术语“半梯度”是本书第二版的新术语。
在强化学习中最早使用状态聚合的可能是Michie和Chambers的box系统(1968)。国家在强化学习中的聚集理论是由Singh、Jaakkola和Jordan(1995)、Tsitsiklis和Van Roy(1996)提出的。状态聚合从早期就被用于动态编程(例如，Bellman, 1957年a)。

9.4 Sutton(1988)证明了线性TD(0)与最小值的收敛性。
已经解决的案例特征向量,{ x(s)的模式:s∈年代},是线性无关的。与概率1的收敛性几乎同时被几个研究者证明(Peng, 1993;达扬Sejnowski,1994;Tsitsiklis,1994;Gurvits, Lin和Hanson, 1994)。此外，Jaakkola、Jordan和Singh(1994)在在线更新的情况下证明了趋同。所有这些结果都假设了线性无关的特征向量，这意味着wt的分量至少和状态一样多。对于更重要的一般(依赖)特征向量的收敛性首先由Dayan(1992)展示。Tsitsiklis和Van Roy(1997)对Dayan的结果进行了一个重要的概括和强化。证明了本节给出的主要结果，即线性自举法渐近误差的界。

9.5我们对线性函数逼近的可能性范围的表示是
基于巴托(1990)的观点。

9.5.2 Konidaris, Osentoski, Thomas(2011)在a中引入了傅立叶基
简单的形式适用于多维连续状态空间和函数的强化学习问题，这些函数不必是周期性的。

9.5.3粗码一词来源于Hinton(1984)，我们的图9.6基于此
他的一个数字。Waltz和Fu(1965)在增强学习系统中提供了这种函数近似的早期例子。

238年 					第9章:带有近似的政策预测



9.5.4块编码(包括散列)由Albus引入(1971,1981)。他德-
用他的“小脑模型关节控制器”(CMAC)来描述它，就像我们在文献中所知道的那样。“tile coding”这个术语在本书的第一版中是新的，尽管在这些术语中描述CMAC的想法是取自Watkins(1989)。在许多增强学习系统中使用了Tile coding(例如Shewchuk和Dean, 1990);林和金姆,1991;米勒，Scalera和Kim, 1994;Sofge和白色,1992;Tham,1994;萨顿,1996;以及其他类型的学习控制系统(例如，Kraft和Campagna, 1990;克拉夫特，米勒和迪茨，1992)。这一节主要介绍了米勒和格兰仕(1996)的工作。用于tile编码的通用软件有几种语言(例如，请参见http://incompleteideas.net/tiles/tiles3.html)。

9.5.5利用径向基函数进行函数逼近得到了广泛的关注
自从与ANNs有血缘关系，由Broomhead和Lowe(1988)著。Powell(1987)回顾了RBFs的早期使用，Poggio和Girosi(1989, 1990)广泛开发和应用了这种方法。

9.6步长参数的自动调整方法包括RMSprop (Tiele-)
man and Hinton, 2012)， Adam (Kingma and Ba, 2015)，随机meta-descent方法，如Delta-Bar-Delta (Jacobs, 1988)，它的增量一般化(Sutton, 1992b, c);和非线性推广(Schraudolph, 1999, 2002)。明确设计用于强化学习的方法包括:AlphaBound (Dabney and Barto, 2012)、SID and NOSID (Dabney, 2014)、TIDBD (Kearney et al.， in preparation)以及随机元下降在政策梯度学习中的应用(Schraudolph, Yu, and Aberdeen, 2006)。

9.6阈值逻辑单元作为抽象模型神经元的引入
McCulloch和Pitts(1943)是ANNs的前身。ANNs作为分类或回归学习方法的历史大致经历了几个阶段:感知器(Rosenblatt, 1962)和ADALINE (ADAptive LINear Element, Widrow and Hoff, 1960)单层ANNs学习阶段，错误-反向传播阶段(LeCun, 1985);Rumelhart, Hinton, and Williams, 1986)的多层ANNs学习，以及目前以表现学习为主的深度学习阶段(如Bengio, Courville, Vincent, 2012);Goodfellow, Bengio和Courville, 2016)。很多关于ANNs的书有Haykin (1994)， Bishop(1995)和Ripley(2007)。
ANNs作为增强学习的函数近似，可以追溯到Farley和Clark(1954)的早期工作，他们使用强化类学习来修改代表策略的线性阈值函数的权值。Widrow, Gupta和Maitra(1973)提出了一个类似神经元的线性阈值单元，该单元实现了一个学习过程，他们称之为“批评家学习”或“选择性自适应学习”，这是ADALINE算法的强化学习变体。Werbos(1987, 1994)开发了一种预测和控制方法，该方法使用经过错误回溯训练的ANNs来学习策略和使用类td算法的值函数。Barto, Sutton, Brouwer(1981)和Barto和Sutton(1981)扩展了

书目的和历史的言论 					239年



联想记忆网络的概念(如Kohonen, 1977;Anderson, Silverstein, Ritz，和Jones, 1977)强化学习。Barto、Anderson和Sutton(1982)利用双层ANN学习非线性控制策略，强调了第一层学习合适的表示的作用。汉普森(1983,1989)是学习价值函数的早期支持者。Barto、Sutton和Anderson(1983)以ANN学习平衡模拟杆的形式提出了一种角色-批评家算法(见第15.7和15.8节)。Barto和阿南丹(1985)引入了随机版本Widrow et al。(1973)的选择性引导算法称为关联reward-penalty(AR−P)算法。Barto(1985、1986)和Barto和约旦(1987)描述了多层人工神经网络组成的基于“增大化现实”技术−P单位训练globally-broadcast强化信号学习分类规则,并不是线性可分的。Barto(1985)讨论了ANNs的这种方法，以及这种学习规则与当时文献中的其他规则之间的关系。(请参阅第15.10节，以进一步讨论这种培训多层人工神经网络的方法。)Anderson(1986年、1987年、1989年)对训练多层ANNs的多种方法进行了评估，结果表明，在河内站平衡和塔任务中，由错误反向传播训练的两层ANNs实现actor - critics算法优于单层ANNs算法。Williams(1988)描述了将反向传播和强化学习结合起来进行ANNs训练的几种方法。Gullapalli(1990)和Williams(1992)为具有连续而非二进制输出的神经元类单元设计了增强学习算法。Barto、Sutton和Watkins(1990)认为，ANNs可以在逼近解决顺序决策问题所需的函数方面发挥重要作用。Williams(1992)相关的加强学习规则(第13.3节)对于训练多层ANNs的误差反向传播方法。泰索罗的TD-Gammon(泰索罗1992,1994;16.1节)有力地证明了TD的学习能力(λ)算法和函数逼近的多层人工神经网络在学习玩西洋双陆棋。Silver等人的AlphaGo、AlphaGo Zero、AlphaZero程序(2016,2017a, b);第16.6节)使用强化学习和深度卷积ANNs在围棋游戏中取得令人印象深刻的结果。schmidt huber(2015)回顾了ANNs在强化学习中的应用，包括复发性ANNs的应用。

9.8 LSTD是由于Bradtke和Barto(见Bradtke, 1993, 1994;Bradtke Barto,
1996;Bradtke,Ydstie Barto,1994),并进一步由Boyan(1999、2002),Nedić和Bertsekas(2003),和Yu(2010)。逆矩阵的增量更新至少从1949年开始就已经为人所知(Sherman和Morrison, 1949)。Lagoudakis和Parr (2003;总线̧oniu、Lazaric Ghavamzadeh Munos,印度绅士̆ka,和德舒特,2012)。

9.9我们对基于记忆的函数逼近的讨论很大程度上基于此
Atkeson、Moore和Schaal(1997)对局部加权学习的回顾。Atkeson(1992)讨论了局部加权回归在基于记忆的机器人学习中的应用，并提供了涵盖这一概念历史的广泛参考文献。Stanfill和Waltz(1986)对人工智能中基于内存的方法的重要性进行了有影响的论证，特别是考虑到并行架构的出现，例如连接机。Baird和Klopf(1993)提出了一种新的基于记忆的方法，并将其作为Q-learning的函数逼近方法应用于杆平衡任务。Schaal和Atkeson(1994)将局部加权回归应用于一个机器人杂耍控制问题，并将其用于学习系统模型。Peng(1995)利用杆位平衡任务，用几种最近邻的方法对价值函数、策略和环境模型进行了近似实验。Tadepalli和Ok(1996)通过局部加权线性回归得到了有希望的结果，以学习一个模拟的自动导向车辆任务的值函数。Bottou和Vapnik(1992)在某些模式识别任务中，与非本地算法相比，几个本地学习算法的效率令人惊讶，讨论了本地学习对泛化的影响。

Bentley(1975)引入k-d树，并报告观察O(log n)的平均运行时间，对n个记录进行最近邻居搜索。Friedman, Bentley和Finkel(1977)用k-d树阐明了最近邻居搜索算法。Omohundro(1987)讨论了利用k-d-tree等分层数据结构可能获得的效率收益。Moore、Schneider和Deng(1997)介绍了k-d树用于有效的局部加权回归。


9.10内核回归的起源是Aizerman潜在函数的方法，
布雷弗曼,Rozonoer(1964)。他们把这些数据比作在空间上分布的各种符号和大小的电荷。将点电荷的势与插值曲面相对应，在空间上产生的电势。在这个类比中，核函数是点电荷的势，它随着距离电荷的倒数而减小。康奈尔和乌特戈夫(1987)在杆平衡任务中应用了一种行为-批评方法，在这个任务中，批评家使用核回归和反距离加权来近似价值函数。早在对机器学习中的内核回归产生广泛兴趣之前，这些作者就没有使用内核这个术语，而是引用了“Shepard方法”(Shepard 's method) (Shepard, 1968)。其他基于内核的强化学习方法包括ormonit和Sen(2002)、Dietterich和Wang(2002)、Xu、Xie、Hu和Lu(2005)、Taylor和Parr(2009)、Barreto、Precup和Pineau(2011)、Bhat、Farias和Moallemi(2012)。

为了强调td方法，请参阅第11.8节的参考书目。

我们知道的最早的函数近似方法用于学习价值函数的例子是Samuel的checker player(1959, 1967)。Samuel遵循Shannon(1950)的建议，一个值函数不必精确到可以作为在游戏中选择动作的有用指南，它可以通过特征的线性组合来近似。除了线性函数近似外，Samuel还试验了称为签名表的查找表和分层查找表(Griffith, 1966, 1974;页,1977;比尔曼，费尔菲尔德和贝莱斯，1982年)。
Bellman和Dreyfus(1959)几乎与Samuel的工作同时，用DP的函数逼近方法提出了这一观点。(人们很容易认为贝尔曼和塞缪尔对彼此产生了一些影响，但我们知道，在这两个人的作品中都没有提到对方。)关于函数逼近方法和DP有相当广泛的文献，如使用样条和正交多项式的多网格方法和方法(如Bellman和Dreyfus, 1959;Bellman, Kalaba和Kotkin, 1973;丹尼尔,1976;?威特1978;Reetz,1977;施韦策Seidmann,1985;Chow Tsitsiklis,1991;库什纳Dupuis,1992;生锈,1996)。
Holland(1986)分类器系统使用选择性特征匹配技术在状态-动作对之间泛化评价信息。每个分类器都匹配一个状态子集，该状态为特性的子集指定了值，而其余的特性具有任意的值(“通配符”)。然后，这些子集被用于常规的状态聚合方法来逼近函数。霍兰德的想法是使用一种遗传算法来进化一组分类器，这些分类器集体实现一个有用的动作-值函数。霍兰德的思想影响了作者对强化学习的早期研究，但我们关注的是函数逼近的不同方法。作为函数逼近器，分类器在许多方面都受到限制。首先，它们是状态聚合方法，在缩放和有效地表示平滑函数方面存在相应的限制。此外，分类器的匹配规则只能实现与特征轴平行的聚合边界。也许传统分类器系统最重要的局限性是，分类器是通过遗传算法(一种进化方法)来学习的。正如我们在第1章中讨论的那样，在学习更多关于如何学习的详细信息的过程中，我们可以用进化的方法来使用。这种观点引导我们采用监督学习方法来进行强化学习，特别是梯度下降法和神经网络方法。荷兰之间的这些差异的方法和我们并不令人惊讶,因为荷兰的思想开发期间当ann被普遍认为是太弱计算能力是有用的,而我们的工作是在一开始的时期,普遍的质疑,传统智慧。仍然有许多机会将这些不同方法的各个方面结合起来。
Christensen和Korf(1986)在国际象棋游戏中尝试了修正线性值函数近似系数的回归方法。Chapman和Kaelbling(1991)和Tan(1991)采用决策树方法来学习价值函数。基于解释的学习方法也被用于学习价值函数，产生紧凑的表述(Yee, Saxena, Utgoff, Barto, 1990;Dietterich和Flann,1995)。
