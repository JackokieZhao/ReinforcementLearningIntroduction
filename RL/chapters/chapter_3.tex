
\chapter{第三章 有限马尔可夫决策过程}

在本章中，我们将介绍有限的马尔可夫决策过程的形式问题，或有限的MDPs，我们试图在本书的其余部分中加以解决。这个问题涉及到评价反馈，如在土匪中，但也包括在不同情况下选择不同行动的联想方面。MDPs是顺序决策的典型形式化，在这种情况下，行动不仅会影响即时奖励，还会影响随后的情况或状态，并通过这些未来的奖励。因此，千年发展目标包括延迟奖励和权衡即时和延迟奖励。我们估计的价值而在土匪问题问∗(a)的每一个行动,在mdp我们估计价值问∗(,)的每一个行动在每个州年代,或我们估计价值v∗(s)的每个状态最优行动选择。这些与国家有关的量对于准确地为个人行为选择的长期后果分配信用至关重要。
MDPs是一种数学上理想的强化学习问题形式，可以对其进行精确的理论表述。我们介绍了问题数学结构的关键要素，如返回、值函数和Bellman方程。我们试图传达广泛的应用，这些应用可以被表述为有限的MDPs。在所有的人工智能中，适用性的广度和数学可追溯性之间存在着一种张力。在本章中，我们将介绍这种紧张关系，并讨论它所包含的一些权衡和挑战。在第17章中讨论了可以在多边开发计划署之外进行强化学习的一些方法。

\section{Agent-Environment接口}

千年发展目标应该是对从交互中学习以实现目标的问题的一个简单的框架。学习者和决策者被称为代理人。它与之交互的东西，包括代理之外的所有东西，叫做环境。这些交互是连续的，代理选择动作，环境响应

48 					第三章:有限马尔可夫决策过程



这些行为和向代理提供新情况。环境也会产生回报，这种特殊的数值是代理人通过选择行动寻求最大化的。

图3.1:马尔可夫决策过程中的代理-环境交互。


更具体地说，代理和环境在每个离散时间步骤的序列中相互作用，t = 0, 1, 2, 3，…2在每个时间步t,代理接收环境的的一些表示状态,圣∈年代,并在此基础上选择一个动作,∈(S)。3一个时间步后,部分作为其行动的后果,代理接收到一个数值奖励,Rt + 1∈⊂R,并发现自己在一个新的国家,圣+ 1.4 MDP和代理在一起从而产生一个序列或轨迹,开头是这样的:

S0 A0 R1 S1 A1 R2 S2 A2 R3… 					(3.1)

在一个有限的MDP中，状态、行为和奖励(S、a和R)的集合都有有限数量的元素。在这种情况下，随机变量Rt和St已经定义好了仅依赖于前一状态和行为的离散概率分布。也就是说，对于这些随机变量s的特定值?∈年代和∈r,这些值的概率发生在时间t,鉴于特定值前状态和行动:

p(s ?， r |s a)圣= =公关{年代?,Rt = r |圣−1 = s−1 = }, 					(3.2)

对于所有年代?∈年代,∈r,∈(s)。函数p定义了MDP的动力学。等式中等号两边的点提醒我们，它是一个定义(在这个例子中是函数p)，而不是遵循先前定义的事实。动态函数p:S×R×S×→[0,1]是一个普通的确定性函数的四个参数。中间的|来自条件概率的符号，

我们使用术语代理、环境和操作，而不是工程师的术语控制器、控制系统(或工厂)和控制信号，因为它们对更广泛的用户来说是有意义的。
我们将注意力限制在离散时间上，以使事情尽可能地简单，即使许多想法可以扩展到连续时间情况(例如，参见Bertsekas和tsiklis, 1996;没有事情,1996)。
为了简化表示法，我们有时假设在所有状态下动作集相同的特殊情况，并将其简单地写成A。
4我们用Rt+1而不是Rt来表示由于At引起的奖励，因为它强调下一个奖励和下一个状态，Rt+1和St+1是共同确定的。不幸的是,这两个公约
在文献中广泛使用。

但这里它只是提醒我们p指定了每个选项s和a的概率分布，也就是

s ?∈年代

r∈
p(s ?)= 1,r |年代,∈年代,∈(s)。 					(3.3)


在马尔可夫决策过程中，p给出的概率完全表征了环境的动态。,每个可能值的概率为St和Rt只取决于立即前状态和行动,圣在−−1和1,早些时候给他们,而不是在所有州和行动。这是最好的限制，不是在决策过程中，而是在状态上。该状态必须包含有关过去的代理-环境交互的所有方面的信息，这些交互将对未来产生影响。如果有，那么状态就有马尔可夫性质。我们将在整本书中假设马尔可夫性质，尽管从第二部分开始我们将考虑不依赖于它的近似方法，在第17章我们将考虑如何从非马尔可夫观察中学习和构造马尔可夫状态。
从four-argument动力学函数,p,一个可以计算任何一个想了解环境,如状态转换概率(我们表示轻微的虐待的符号,作为一个three-argument函数p:S××一→[0,1]),


p(s ?|年代,a)。圣= =公关{年代?圣−1 = s | } =−1 =

r∈
p(s ?， r |s, a)。(3.4)

我们也可以计算出预期回报政府行动对双参数函数r:S×→r:

r(年代)。= E[圣−1 = s Rt |在−1 = =

r∈
R

s ?∈年代
p(s ?， r |s a)， (3.5)

和预期回报state-action-next-state三元组作为three-argument函数r:S××S→r,

r(s,s)。= E[圣−1 = s Rt |−1 =一个圣= s ?]=

r∈
R
p(s ?r | s,)
p(s ?|年代,a)。(3.6)
在这本书中，我们通常使用四个参数的p函数(3.2)，但是这些其他的符号有时也很方便。
MDP框架是抽象和灵活的，可以用许多不同的方式应用于许多不同的问题。例如，时间步骤不需要参考固定的实时时间间隔;它们可以指任意连续的决策和行动阶段。这些动作可以是低级的控制，比如应用于机器人手臂马达上的电压，也可以是高级的决定，比如是否去吃午饭或去读研究生。同样，各州可以采取各种形式。它们可以完全由低层次的感觉(如直接的传感器读数)决定，也可以是更高层次和更抽象的感觉(如对房间内物体的象征性描述)。构成一种状态的一些因素可能是基于对过去感觉的记忆

甚至完全是心理或主观的。例如，代理可能处于不确定对象在哪里的状态，或者在某种明确定义的意义上感到惊讶的状态。类似地，有些动作可能完全是脑力或计算能力的。例如，一些操作可能控制代理选择考虑的内容，或者它关注的地方。总的来说，行动可以是我们想要学习的任何决定，而国家可以是我们知道的任何可能有用的东西。
特别是，agent与环境之间的边界通常不同于机器人或动物身体的物理边界。通常情况下，边界会更接近于代理。例如，机器人的马达和机械连接及其传感硬件通常应该被视为环境的一部分，而不是agent的一部分。同样，如果我们将MDP框架应用于人或动物，肌肉、骨骼和感觉器官应该被视为环境的一部分。奖励也可能是在自然和人工学习系统的身体内部计算出来的，但被认为是在主体外部。
我们遵循的一般规则是，任何不能被代理任意改变的事物都被认为是在它之外的，因此它是环境的一部分。我们不会假设环境中的所有东西都是未知的。例如，代理通常知道很多关于它的奖励是如何作为它的行为和它们所采取的状态的函数来计算的。但是我们总是认为奖励计算是在代理之外的，因为它定义了面向代理的任务，因此必须超出其任意更改的能力。事实上，在某些情况下，代理可能知道它的环境是如何工作的，并且仍然面临一个困难的强化学习任务，就像我们可能知道像Rubik的立方体一样的难题是如何工作的，但是仍然不能解决它。代理环境边界代表代理绝对控制的极限，而不是其知识的极限。
agent-environment边界可以位于不同的位置，用于不同的目的。在一个复杂的机器人中，许多不同的代理可能同时运行，每个代理都有自己的边界。例如，一个代理可以做出高层决策，这些决策是由执行高层决策的低级代理所面对的状态的一部分。在实践中，一旦选择了特定的状态、行为和奖励，并确定了感兴趣的特定决策任务，就确定了代理环境的边界。
MDP框架是对目标导向学习问题的一个相当抽象的概念。提出任何感官的细节,内存,和控制装置,以及任何一个目的是试图实现,任何问题的学习目标导向行为可以减少到三个信号之间来回传递一个代理及其环境:一个信号代表的选择由代理(行动),一个信号来表示的基础上选择是由(美国),和一个信号来定义代理的目标(奖励)。这个框架可能不足以代表所有的决策学习问题，但是它已经被证明是非常有用和适用的。
当然，每个任务的特定状态和操作都有很大的不同，它们的表示方式会对性能产生很大的影响。在强化学习中，就像在其他类型的学习中一样，这种表征性的选择比科学更具有艺术性。

在这本书中，我们提供了一些关于表示状态和行为的好方法的建议和例子，但是我们的主要重点是在选择了表示之后学习如何行为的一般原则。

例子3.1:生物反应器假设正在应用强化学习来确定生物反应器(用于生产有用化学物质的大量营养和细菌)的每时每刻的温度和搅拌速度。这种应用程序的操作可能是将目标温度和目标搅拌速率传递给较低水平的控制系统，从而直接激活加热元件和电机以达到目标。这些状态可能是热电偶和其他感官读数，可能是经过过滤和延迟的，加上表示容器中的成分和目标化学品的符号输入。回报可能是对生物反应器产生有用化学物质的速率的逐时刻测量。注意，这里每个状态都是传感器读数和符号输入的列表或向量，每个动作都是由目标温度和搅拌速度组成的向量。强化学习任务的典型特征是具有这种结构化表示形式的状态和动作。另一方面，奖励总是单一的数字。

例3.2:在重复的拾取和放置任务中，拾取和放置机器人考虑使用增强学习控制机器人手臂的运动。如果我们想要学习快速流畅的运动，学习代理将必须直接控制电机，并对机械连杆的当前位置和速度有低延迟的信息。这种情况下的动作可能是在每个关节上应用于每个电动机的电压，而状态可能是关节角度和速度的最新读数。每一个被成功拾取并放置的物体的奖励可能是+1。为了鼓励平稳的运动，在每一步上，一个小的，消极的奖励可以作为运动的每一刻的“摇摆”的函数。

练习3.1设计适合MDP框架的三个示例任务，确定每个任务的状态、行为和奖励。让这三个例子尽可能地不同。该框架是抽象和灵活的，可以用许多不同的方式应用。至少在你的一个例子中以某种方式扩展它的极限。?

练习3.2是充分有效地表示所有目标导向的MDP框架
学习任务?你能想到任何明显的例外吗? 					?

练习3考虑开车的问题。你可以用加速器、方向盘和刹车来定义动作，也就是你的身体与机器的交汇处。或者你可以更深入地定义它们——比如，橡胶和路面的交汇处，考虑到你的动作是轮胎扭矩。或者你可以进一步定义它们，比如，你的大脑和你的身体在哪里，肌肉抽搐来控制你的四肢。或者你可以去到一个非常高的水平，说你的行动是你开车的选择。什么是正确的级别，什么是在代理和环境之间划清界限的正确位置?在什么基础上，线路的一个位置优于另一个?选择一个地点而不是另一个地点有什么根本原因吗?或者这是一个自由的选择??

\section{目标和奖励}

练习3.4给出了一个类似于例子3.3的表，但是对于p(s?r |s a)它应该有s a s s s的列?、r和p(s ?， r |s a)，每4个元组对应一行
p(s ?， r |s a) > 0。 					?

3.2目标和奖励

在强化学习中，主体的目的或目标是通过一种特殊的信号被形式化的，称为奖励，从环境传递给主体。在每个时间步,奖励是一个简单的数字,Rt∈r .非正式代理的目标是最大化回报它收到的总量。这就意味着不能立即获得最大的回报，而是长期累积的回报。我们可以把这个非正式的想法明确地表述为奖励假设:

我们所说的目标和目的都可以被认为是一个接收到的标量信号(称为奖励)的累积和的期望值的最大值。

利用奖励信号将目标的概念形式化是强化学习最显著的特征之一。
虽然从奖励信号的角度来制定目标一开始可能显得有限，但在实践中证明它是灵活和广泛适用的。看到这一点的最好方法是考虑如何使用它的例子。例如，为了让机器人学会走路，研究人员提供了与机器人向前运动成正比的每一步奖励。在制作一个机器人学习如何逃离迷宫,奖励通常是−1每一个时间步,经过之前逃离;这鼓励代理尽可能快地转义。为了让机器人学会寻找和收集空的汽水罐回收利用，人们可能会在大部分时间给它一个零奖励，然后每个人都可以得到一个+1的奖励。当机器人撞到东西或者有人对它大喊大叫的时候，你可能也想给它一些负面的奖励。代理学习下棋或国际象棋,自然赢得奖励+ 1,失去−1,0为图纸和所有非终结符的位置。
你可以看到在这些例子中发生了什么。代理人总是学会使其报酬最大化。如果我们想让它为我们做点什么，我们必须给它提供奖励，这样在最大化它们的同时，代理也会实现我们的目标。因此，至关重要的是，我们设立的奖励真正表明了我们想要实现的目标。

特别地，奖励信号不是向代理传授如何实现我们想要它做的事情的先验知识的地方。例如，一个下棋的代理人应该只因为他确实赢了而得到奖励，而不是因为他完成了子目标，如夺取对手的棋子或控制棋盘的中心。如果实现这些子目标得到了奖励，那么代理可能会找到一种方法来实现它们，而不会实现真正的目标。例如，它可能会找到一种方法来夺取对手的棋子，即使是以输掉比赛为代价的。奖励信号是你向机器人传达你想让它实现什么的方式，而不是你想让它实现的方式


\section{回报和集}

到目前为止，我们已经讨论了非正式学习的目标。我们已经说过，代理人的目标是使其长期获得的累积回报最大化。这是如何正式定义的?如果时间步t后的奖励序列为Rt+1, Rt+2, Rt+3，…，那么这个序列的具体方面是什么呢?一般来说，我们寻求的是期望收益最大化，其中的收益(表示Gt)被定义为奖励序列的某个特定函数。在最简单的情况下，回报是回报的总和:

Gt。= Rt+1 +Rt+2 +Rt+3 +···+ Rt， 					(3.7)

T是最后一步。这种方法在应用程序中是有意义的，在这些应用程序中，有一个自然的时间步概念，即当代理环境交互自然地分解为子序列时，我们将其称为“插曲”，如游戏中的游戏，在迷宫中旅行，或任何类型的重复交互。每一集都以一种特殊的状态结束，这种状态被称为终端状态，然后重置为标准启动状态，或者从标准启动状态分布返回到样本。即使你认为每一集都以不同的方式结尾，比如输赢一场比赛，下一集也会独立于前一集的结局。因此，所有的情节都可以被认为以相同的终点状态结束，不同的结果会有不同的奖励。这种情况的任务叫做情景任务。在情景性任务中，我们有时需要区分所有非终结状态集合(表示S)和所有状态集合(表示S+)。终止时间T是一个随机变量，它通常随事件而变化。
另一方面，在许多情况下，代理-环境交互并不能自然地分解为可识别的事件，而是持续不断地进行。例如，这是一种自然的方式来制定一个正在进行的过程控制任务，或者是一个长寿命机器人的应用程序。我们称之为持续任务。回归公式(3.7)是有问题的继续任务,因为最后的时间步将T =∞,和回报,这是我们正试图最大化,能轻易本身

传授这种先验知识的更好的地方是最初的政策或初始价值函数，或者是对它们的影响。
第17.4节进一步探讨了设计有效奖励信号的问题。7集在文学作品中有时被称为“审判”。

是无限的。(例如，假设代理在每个时间步骤收到+1的奖励。)因此，在这本书中，我们通常使用返回的定义，这个定义在概念上稍微复杂一点，但在数学上要简单得多。
我们需要的另一个概念是折现。根据这种方法，代理尝试选择动作，以便在未来获得的折扣奖励的总和达到最大。特别是选择At，使预期折现收益最大化:


Gt。= Rt + 1 +γRt + 2 +γ2Rt + 3 +···=
∞吗?k = 0
γkRt + k + 1, 					(3.8)

γ是一个参数,0≤γ≤1,称为贴现率。
现值的折现率确定未来奖励:奖励获得k时间步骤在未来值得γk−1倍价值如果是立即收到。如果γ< 1,(3.8)的无限和有限值只要奖励序列{ Rk }是有界的。如果γ= 0,代理“近视”是只关心最大化即时回报:它的目标在这个案例中,是要学会如何选择在以最大化只有Rt + 1。如果代理的每一个行为碰巧只影响即时奖励，而不影响未来的奖励，那么近视代理可以通过分别最大化每一个即时奖励最大化(3.8)。但总的来说，为了即时奖励最大化而采取行动，可以减少获得未来奖励的机会，从而减少回报。当γ接近1,返回目标考虑了未来的回报更强烈;代理人变得更有远见。
连续时间的返回以一种对强化学习理论和算法很重要的方式相互关联:

Gt。= Rt + 1 +γRt + 2 +γ2Rt + 3 +γ3Rt + 4 +···= Rt + 1 +γ

Rt + 2 +γRt + 3 +γ2Rt + 4 +···

= Rt + 1 +γGt + 1 					(3.9)

注意，这适用于所有时间步骤t < t，即使终止发生在t+ 1，如果我们定义GT = 0。这通常使计算回报序列的返回变得很容易。
注意,尽管返回(3.8)是一个无限的术语,它仍然是有限的,如果非零和奖励不变γ< 1。例如，如果奖励是常数+1，那么回报是


练习3.5第3.1节中的方程是针对连续情况的，需要对其进行修改(非常轻微)以应用于情景性任务。显示您知道修改的内容
需要提供(3.3)的修改版本。 					?

例3.4:Pole-Balancing
这个任务的目的是运用部队手推车沿着轨道,以防止杆铰接在车上摔倒:据说失败发生如果过去给定杆落角垂直或如果大车运行轨道。在每次故障后，极点重新设置为垂直。这一任务可以被看作是偶然的，自然的。
情节是反复尝试平衡极点。这种情况下的奖励可以是+1，因为每次失败都没有发生，所以每次返回的次数都是失败的次数。在这种情况下，成功的永久平衡意味着无限的回归。或者，我们可以使用贴现将杆平衡视为一个持续的任务。在这种情况下,奖励是−1在每个失败在其他时间和零。相关的回报在每次将−γK,K是失败之前的时间步骤。在任何一种情况下，只要尽可能长时间保持极点平衡，回报就会最大化。


练习3.6假设你对待pole-balancing情景任务,但也使用打折,所有奖励0除了−1失败。那么每次的回报是多少呢?这个回报与折现的，持续的有什么不同
制定这项任务吗? 					?

练习3.7想象你正在设计一个机器人来运行一个迷宫。你决定给它一个+1的奖励以奖励它从迷宫中逃跑，而在其他任何时候给它一个0的奖励。这个任务似乎自然而然地分成了片段——连续的贯穿整个迷宫——所以你决定把它当作一个片段式的任务，目标是最大化期望的总回报(3.7)。在运行了一段时间的学习代理之后，您发现它在逃离迷宫方面没有任何改进。什么错了吗?你有有效
与代理商沟通你想让它实现什么? 					?
运动假设γ= 0.5和3.8以下收到的回报序列R1 =−1,R2 = 2,R3 = 6,R4 = 3,和R5 = 2,T = 5。G0 G1 G5是什么?提示:
向后的工作。 					?
锻炼3.9假设γ= 0.9和奖励序列R1 = 2无限紧随其后
7年代序列。G1和G0是什么? 					?
练习3.10证明(3.10)中的第二个等式。 					?

3.4。情节和持续任务的统一表示法 					57


3.4片段式和连续式任务的统一表示法

在前一节中，我们描述了两种强化学习任务，一种是agent-environment交互自然地分解成一系列独立的片段(情景性任务)，另一种是不存在的(持续任务)。前一种情况在数学上更容易，因为每一个行为只影响在事件中随后获得的有限数量的奖励。在这本书中，我们有时考虑一种问题，有时考虑另一种问题，但往往同时考虑这两种问题。因此，建立一种能让我们同时准确地讨论这两种情况的符号是很有用的。
要精确地描述情景任务，需要一些额外的符号。我们需要考虑一系列的事件，而不是一长串的时间步，每一个都由有限的时间步组成。我们把每一集的时间步骤从零重新开始。因此,我们必须不仅指圣,表示在时间t,但圣,我状态表示在时间t的第一集(Rt和类似,我,我,πt,我,Ti,等等)。然而，当我们讨论情景任务时，我们几乎不需要区分不同的事件。我们几乎总是在考虑某一集的情节，或说一些对所有情节都适用的事情。因此，在实践中，我们几乎总是通过删除对情节编号的显式引用来略微滥用表示法。也就是说，我们写St是指St,i，等等。
我们需要另一种惯例来获得一种单一的表示法，它包括情景性和持续性的任务。我们将回报定义为一种情况下有限项的和(3.7)另一种情况下无限项的和(3.8)。这两个可以通过考虑插曲结束进入一种特殊的吸收状态来统一，这种吸收状态只会转移到自身，只会产生零的奖励。例如，考虑状态转换图:



这里的实方表示一集结束时对应的特殊吸收状态。从S0开始，我们得到奖励序列+1 +1 +1 +1 0 0 0 0。把这些相加，我们得到相同的回报不管是对第一个T奖励(这里T = 3)还是对整个无穷序列。即使引入了折现，这仍然成立。因此,我们可以定义返回,一般来说,根据(3.8),使用省略的会议集不需要数字时,包括γ= 1的可能性,如果和定义(例如,因为所有事件终止)。或者,我们可以写


Gt。=
T ?k = t + 1
tγk−−1 rk, 					(3.11)

包括T =∞的可能性或γ= 1(但不是全部)。我们在本书的其余部分使用这些约定来简化符号，并表达情景任务和持续任务之间的密切相似之处。(稍后，在第10章，我们将介绍一种既持续又不打折扣的配方。)

58 					第三章:有限马尔可夫决策过程


\section{策略和价值功能}

几乎所有的增强学习算法都涉及到评估价值函数——状态函数(或状态-动作对)的函数，这些函数估计代理在给定状态下的状态有多好(或者在给定状态下执行给定动作有多好)。这里所说的“多好”指的是未来的回报，也就是预期的回报。当然，代理人在未来可能得到的回报取决于它将采取什么行动。因此，价值函数是根据特定的行为方式(称为策略)来定义的。
在形式上，策略是从状态到选择每个可能动作的概率的映射。如果代理政策π时间t后,然后π(|)的概率是在如果圣=。= p,π是一个普通的函数;“|”的π(|)只是提醒,它定义了一个概率分布在∈(s)为每个年代∈美国强化学习方法指定代理的政策是如何改变结果的经验。
练习3.11如果当前状态是圣,根据随机选择和行动政策π,那么什么是Rt + 1的π的期望和four-argument
函数p(3.2)? 					?
的价值功能状态下政策π,表示vπ(s),是预期收益后当从年代和π。mdp,我们可以定义vπ正式通过


vπ(年代)。= Eπ[Gt |圣= s]= Eπ

∞吗?k = 0
γkRt + k + 1

圣=年代

∈年代,(3.12)

在Eπ(·)表示一个随机变量的期望值,因为代理遵循政策π,和t是任何时间步。注意，终端状态(如果有的话)的值总是0。我们称之为政策πvπ州值函数的函数。
同样,我们定义了采取行动的价值在国家年代下政策π表示qπ(,),预期收益从年代、采取的行动,以及此后π以下政策:


qπ(年代)。圣= = Eπ[Gt |年代,在=]= Eπ

∞吗?k = 0
γkRt + k + 1

圣=,=

。(3.13)


我们称之为qπ政策π的行为价值函数。
练习3.12给出方程vπqπ和π。 					?
练习3.13给出方程qπvπ和four-argument p。价值函数vπ和qπ可以从经验估计。例如,如果一个
代理遵循政策π和保持平均,每个国家遇到的实际回报之后,状态,然后将收敛于国家的平均价值,vπ(s),遇到那种状态的次数趋于无穷。如果单独平均每个行动都在每一个州,那么这些平均也会收敛于动作值,qπ(年代)。我们称这种蒙特卡罗方法的评估方法,因为它们涉及在许多随机样本的平均实际回报。


第五章介绍了这些方法。当然，如果有非常多的州，那么单独为每个州保持单独的平均水平可能并不实际。相反,代理必须保持vπqπ作为参数化函数(参数少于状态)和调整参数以更好地匹配观察到的回报。这也可以产生准确的估计，尽管这在很大程度上取决于参数化函数近似器的性质。这些可能性在本书的第二部分中进行了讨论。
在增强学习和动态规划中使用的值函数的基本特性是，它们满足类似于我们已经为返回(3.9)建立的递归关系。任何政策π和任何国家年代,下面的一致性条件之间拥有的价值和其可能的继任者状态:


vπ(年代)。= Eπ[Gt |圣= s]
= Eπ[Rt + 1 +γGt + 1 |圣= s] 					(通过(3.9))

=

一个
π(|)?
s?

R
p(s ?r | s,)

r +γEπ(Gt + 1 |圣+ 1 = s ?)


=

一个
π(|)?
r s ?,
p(s ?r | s,)

r +γvπ(?)

∈年代,(3.14)


其中隐含的行为，a，是从集合a中取的，下一个状态，s?，从集合S中取(或从S+中，在情景性问题中)，得到的奖励，r，是从集合r中取的，也就是在上一个方程中，我们如何将两个和，1除以S的所有值?另一个除以r的所有值，变成两个值的总和。我们经常使用这种合并和来简化公式。注意如何将最终表达式轻松地读取为预期值。它实际上是三个变量的所有值的和，a, s?和r。三,我们计算概率,π(|)p(s ?，r |s, a)，将括号中的数量乘以这个概率，然后求和得到期望值的所有可能性。




π
年代




s?
π


r p
一个




备份图vπ
方程(3.14)是vπ贝尔曼方程。它表达了一个国家的价值与其继承国的价值之间的关系。设想一下，从一个国家到它可能的继承国，正如图中所建议的那样。每个开环代表一个状态，每个实环代表一个状态-动作对。从国家年代,顶部的根节点,代理可以采取的一些行动——三个政策π的基于图所示。从
每一种情况，环境都可以对接下来的几个状态之一s做出反应。(图中显示了两个)，以及一个奖励，r，取决于函数p给出的动力学。Bellman方程(3.14)的平均值除以所有的可能性，根据其发生的概率加权。它说明开始状态的值必须等于期望下一个状态的(折现的)值，加上沿途期望的回报。
价值函数vπ是贝尔曼方程的唯一解。我们在后面的章节中展示了这个Bellman方程是如何形成许多方法的基础的

计算、近似和vπ学习。我们将这些图称为备份图，因为它们的关系图构成了更新或备份操作的基础，而这些操作是增强学习方法的核心。这些操作将价值信息从继承状态(或状态操作对)转移回状态(或状态操作对)。我们在书中使用备份图表来提供我们讨论的算法的图形摘要。(注意，与转换图不同，备份图的状态节点不一定表示不同的状态;例如，一个国家可能是它自己的继承者。
示例3.5:Gridworld图3.2(左)显示了一个简单的有限MDP的矩形网格世界表示。网格的单元格对应于环境的状态。在每个单元格中，有四种操作是可能的:北部、南部、东部和西部，这决定了代理在网格上按各自的方向移动一个单元格。行动,将代理的网格离开它的位置不变,但也导致奖励−1。其他行为的奖励为0，除了那些将agent从特殊状态a和b中移出的行为外，这四个行为的奖励均为+10，并将agent移至a ?从状态B开始，所有行为都产生+5的奖励，并将代理带到B?


图3.2:Gridworld示例:等可能随机策略的异常奖励动态(左)和状态值函数(右)。

假设代理在所有状态中选择所有四个具有相同概率的操作。图3.2(右)显示了值函数,vπ,对于这一政策,γ= 0.9折扣奖励情况。这个值函数是通过求解线性方程组(3.14)得到的。注意下边缘附近的负值;这是在随机策略下撞到网格边缘的高概率的结果。状态A是这个策略下的最佳状态，但是它的预期回报小于10，它的即时回报，因为从代理被带到A?，它很可能从那里进入网格的边缘。另一方面，状态B的值大于5，它的即时奖励，因为从B代理被带到B?，它有一个正值。从B吗?可能撞到边的预期惩罚(负奖励)比可能撞到A或B的预期收益要多。
练习3.14贝尔曼方程(3.14)为每个国家的价值函数必须持有vπ如图3.5 3.2(右)的例子。显示数值状态方程适用于中心,价值+ 0.7,关于它的四个邻国,价值+ 2.3 + 0.4−0.4,+ 0.7。(这些数字只精确到小数点后一位。)在gridworld示例中，奖励对于目标是积极的，对于跑到世界的边缘是消极的，其余时间则为零。是这些迹象吗

奖励是重要的，还是仅仅是他们之间的间隔?使用(3.8)证明，在所有奖励中增加一个常数c，会给所有状态的值增加一个常数vc，因此不会影响任何策略下任何状态的相对值。什么是vc
c和γ吗? 					?
练习3.16现在考虑在情景性任务(如迷宫跑步)的所有奖励中增加一个常数c。这是否会产生任何影响，或者它会不会像上述的持续任务一样，将任务保持不变?为什么或为什么不呢?给一个例子。吗?例3.6:高尔夫制定打一个洞的高尔夫作为强化学习任务,我们计算一个点球(负奖励)−1为每个中风直到我们击球进洞里。状态是球的位置。状态的值是从那个位置到洞的笔画数的负数。我们的行动就是我们如何瞄准球，如何挥拍球，当然还有我们选择哪个球杆。让我们以前者为例，考虑俱乐部的选择，我们假设它是一个推杆还是一个车手。图3.3的上半部分显示了一个可能的状态值函数vputt(s)。




Q *(年代,驱动程序)
V推杆
沙子

绿色的年代
n维




沙子




绿色
是个
n维
vputt




问∗(年代,司机)

图3.3:一个高尔夫示例:状态值func-
设置(上)和使用驱动(下)的最优动作值函数。
总是使用推杆。洞中的终端状态的值为0。从绿色的任何地方我们假设我们可以做一个推杆;这些国家有价值−1。在绿色的地方，我们不能通过放置来到达洞，而且价值更大。如果我们能从国家通过将达到绿色,那一个国家必须有价值不到绿色的价值,也就是说,−2。为了简单起见，让我们假设我们可以非常精确和确定地推推，但范围有限。这给了我们锋利的轮廓线标记−2图中;在这条线和绿线之间的所有位置都需要精确地划两笔才能完成这个洞。同样,把范围内任何位置−2−3轮廓线必须有一个价值,等等,在图中所示的轮廓线。让不让我们摆脱砂陷阱,所以他们有价值的−∞。总的来说，我们要用6次击球才能从发球台到球洞。


R
s?
年代,




一个?
π
p




qπ备份图
3.17运动的贝尔曼方程是什么动作值,也就是说,qπ吗?它必须给行动价值qπ(,)的动作值,qπ(s ?提示:右边的备份图对应于这个方程。显示类似于(3.14)的方程序列，但用于操作
值。 					?

62年 					第三章:有限马尔可夫决策过程



一个国家的价值取决于该国家可能采取的行动的价值，以及在当前政策下采取每一个行动的可能性。我们可以把这看作是建立在状态的一个小备份图，并考虑每个可能的行动:

给相对应的方程这直觉和图根节点的值,vπ(s)的价值预期的叶子节点,qπ(,),鉴于圣= s。这个方程应该包括一个期望条件政策后,π。然后给第二个方程的期望值是写出明确的π(|)
这样方程中就不会出现期望值符号。 					?
练习3.19一个行动的价值,qπ(,),取决于预期的未来回报和预期的和剩下的奖励。再一次，我们可以把它想象成一个小的备份图，这个以动作(状态-动作对)为基础，分支到可能的下一个状态:

给对应的方程为行动的价值,这个直觉和图qπ(,),预计下一个奖励,Rt + 1,和预期的下一个状态值,vπ(圣+ 1),考虑到圣=和=一个。这个等式应该包含一个期望，而不是一个条件下遵循政策。然后给出第二个方程，用p(s)显式地写出期望值。， r |s, a)由(3.2)定义，不存在期望
值符号出现在等式中。 					?


\section{最优策略和最优值函数}

解决强化学习任务大致是指找到一种策略，从长期来看能获得很多回报。对于有限的MDPs，我们可以用以下方法精确地定义一个最优策略。值函数定义策略的部分排序。策略定义π是优于或等于π政策呢?如果它的预期收益大于或等于π的?所有国家。换句话说,π≥π吗?当且仅当vπ(s)≥vπ?(s)为所有年代∈美国总有至少一个政策优于或等于其他政策。这是一个最优的策略。虽然可能会有不止一个,我们都表示最优政策π∗。他们共享相同的状态值函数,称为最优状态值函数,表示v∗,定义为
v∗(年代)。=最大
π
vπ(s), 					(3.15)
∈年代。


最优政策也共享相同的最优行为价值函数,表示为q∗,定义为

问∗(年代)。=最大
π
qπ(,), 					(3.16)
对于所有∈年代和∈(s)。对于状态-动作对(s, a)，该函数给出在状态s中采取动作a的期望回报，并在之后遵循最优策略。因此,我们可以写问∗∗v如下:
问∗(,)= E(Rt + 1 +γv∗(圣+ 1)|圣=,=)。 					(3.17)

例3.7:最优值函数为高尔夫球的下方图3.3显示了一个可能的最优行为价值的轮廓函数q∗(年代,司机)。这些是每个状态的值，如果我们先和司机一起玩一次，然后选择司机或推杆，以哪个更好。驾驶员使我们能把球打得更远，但精确度较低。只有当我们离得很近的时候，我们才能用驾驶员一枪就能到达那个洞;从而为问∗−1轮廓(年代,司机)只涉及一小部分的绿色。然而,如果我们有两个中风,然后我们可以从多远,到达洞−2所示的轮廓。在这种情况下我们不需要开车到小−1内轮廓,但只有在绿色;从那里我们可以使用推杆。最优动作-值函数在将一个特定的第一个动作提交给驱动程序之后给出值，在本例中是这样的，但是在之后使用任何最好的动作。−3轮廓仍远,包括起始三通。从发球台开始，最好的动作顺序是两个发球点和一个推杆，三次击球使球下沉。
因为v∗的价值函数是一个政策,它必须满足的贝尔曼方程的自洽性条件状态值(3.14)。然而,因为它是最优值函数v∗的一致性条件可以写在一个特殊形式没有提及任何具体的政策。这是贝尔曼方程v∗或贝尔曼最优方程。直觉上，Bellman最优方程表达了这样一个事实，即最优策略下的状态的值必须等于该状态的最佳行为的期望回报:

v∗(s)= max
∈(s)qπ∗(,)
=最大
一个
Eπ∗(Gt |圣= s =)

=最大
一个
Eπ∗(Rt + 1 +γGt + 1 |圣=,=) 					(通过(3.9))

=最大
一个
E(Rt + 1 +γv∗(圣+ 1)|圣=,=) 					(3.18)

=最大
一个

r s ?,
p(s ?r | s,) r +γv∗(s ?)?。 					(3.19)

最后两个方程两种形式的v∗的贝尔曼最优方程。问∗的贝尔曼最优方程

问∗(,)= E

Rt + 1 +γmax
一个?
问∗(圣+ 1 ?)
吗?吗?吗?St = s,At = a。

=

r s ?,
p(s ?r | s,)

r +γmax
一个?
问∗(s ?,一个?)

。 					(3.20)


下图中的备份图显示图形的跨越未来状态和行为被认为是在贝尔曼最优性方程v∗∗。这些都是一样的备份vπ图和qπ除了弧添加了早些时候在代理的选择指向代表最大的选择是采取而不是期望值给出一些政策。左边的备份图用图形表示Bellman最优方程(3.19)，右边的备份图用图形表示(3.20)。


图3.4:备份为q v∗∗图


vπ有限mdp,贝尔曼最优方程(3.19)有一个独特的解决方案独立的政策。Bellman最优方程实际上是一个方程组，每个状态一个，如果有n个状态，那么n个未知数中有n个方程。如果环境的动力学p是已知的,那么原则上可以解决这个方程组v∗使用任何一个各种各样的方法求解非线性方程组。一个人可以问∗解一组相关的方程。
一旦一个v∗,它相对容易确定最优政策。对于每个状态s，在Bellman最优方程中会有一个或多个动作来获得最大值。任何只给这些行为分配非零概率的策略都是最优策略。你可以把这看作是一步搜索。如果你有最优值函数,v∗,之后出现的行为最好的一步法搜索最优行为。另一种说法是,任何政策都是贪婪与最优评价函数v∗是最优策略。“贪婪”一词在计算机科学中被用来描述任何一种搜索或决策程序，它只基于本地或即时的考虑，而不考虑这样的选择可能会阻止未来获得更好的选择。因此，它描述了只根据短期后果选择行动的政策。v∗的美妙之处在于,如果一个用它来评估actions-specifically的短期后果,一步后果贪婪策略实际上是最佳的长期意义上我们感兴趣,因为v∗已经考虑到所有可能的未来的回报后果的行为。通过v∗,最优预期长期回报是本地和变成一个量立即对每个状态。因此，一步一步的搜索会产生长期的最佳行动。
有问∗使选择最佳的操作更简单。问∗,甚至代理不需要做一个领先一步搜索:对于任何一个国家,它可以找到任何行动最大化问∗(年代)。行为价值函数有效地缓存所有领先一步的搜索结果。它提供了最佳的预期长期回报，作为每个状态-动作对的局部和立即可用的值。因此，代价是。

表示状态-动作对的函数，而不仅仅是状态，最优动作-值函数允许选择最优动作，而不需要知道任何可能的后续状态及其值，也就是说，不需要知道环境的动态。
例3.8:解决Gridworld假设我们解决贝尔曼方程v∗为简单的网格任务在例3.5中引入的,在图3.5(左)。回想一下，状态A后面跟着+10的奖励，然后过渡到状态A?，而状态B之后是+5的奖励，然后过渡到状态B?图3.5(中间)显示了最优值函数，图3.5(右)显示了相应的最优策略。在单元格中有多个箭头的地方，所有相应的操作都是最优的。


22.0 24.4 22.0 19.4 17.5。


19.8 22.0 19.8 17.8 16.0


17.8 19.8 17.8 16.0 14.4


14.0 14.0 14.0 14.0 14.0 14.0 14.0 14.0 14.0 14.4 13.0 11.7
一个B




一个“
B + 10
+ 5


)gridworld b)V * c)*πgridworld V∗π∗
图3.5:gridworld示例的最佳解决方案。


例3.9:回收机器人的Bellman最优方程(3.19)，我们可以显式地给出回收机器人的Bellman最优方程。为了使事情更紧凑，我们将状态分为高状态和低状态，分别用h、l、s、w和re进行动作搜索、等待和充值。因为只有两个状态，Bellman最优方程由两个方程组成。v∗方程(h)可以写成:

v∗(h)= max

p(h | h,s)[r(h,s、h)+γv∗(h)]+ p(l | h,s)[r(h,s、l)+γv∗(l)],p(h | h,w)[r(w h,h)+γv∗(h)]+ p(l | h,w)[r(h、w、l)+γv∗(l))

=最大

α(rs +γv∗(h))+(1−α)[rs +γv∗(l)],1[rw +γv∗(h)]+ 0(rw +γv∗(l))

=最大

rs +γ(αv∗(h)+(1−α)v∗(l)],rw +γv∗(h)

。

遵循同样的步骤为v∗(l)收益率方程


v∗(l)= max
⎧⎨⎩
βrs−3(1−β)+γ[(1−β)v∗(h)+βv∗(l)],rw +γv∗(左),
γv∗(h)
⎫⎬⎭。
对于任何选择rs,rw,α,β和γ0≤γ< 1 0≤α,β≤1,正是一对数字,v∗∗(h)和v(l),同时满足这两个非线性方程组。

显式求解贝尔曼最优方程为求解最优策略提供了一条途径，从而解决了强化学习问题。然而，这种解决方案很少直接有用。它类似于一个彻底的搜索，展望所有的可能性，计算它们发生的概率和他们的期望回报。这个解决方案依赖于至少三个在实践中很少出现的假设:(1)我们准确地知道环境的动态;(2)我们有足够的计算资源来完成解的计算;(3)马尔可夫性质。对于我们感兴趣的任务类型，一个人通常无法实现这个解决方案，因为这些假设的各种组合都被违反了。例如，虽然第一个和第三个假设对西洋双陆棋没有问题，但是第二个假设是主要的障碍。因为游戏大约有1020个国家,它将会花上几千年今天最快的计算机来解决v∗贝尔曼方程,和寻找问∗也是如此。在强化学习中，人们通常不得不满足于近似解。
许多不同的决策方法可以看作是近似求解Bellman最优方程的方法。例如,启发式搜索方法可以被视为扩大的右边(3.19)几次,一些深度,形成一个“树”的可能性,然后使用启发式评估函数近似v∗“叶子”节点。(启发式搜索方法如∗几乎总是基于情景的情况。)动态规划方法与Bellman最优方程的关系更为密切。许多强化学习方法可以被清楚地理解为近似求解Bellman最优方程，使用实际的经验转换来代替预期转换的知识。我们将在下一章中考虑各种这样的方法。


练习3.20绘制或描述高尔夫例子的最优状态值函数。吗?练习3.21绘制或描述最优动作-值函数的轮廓
将q∗(年代,推杆),高尔夫的例子。 					?

+ 2 0 0 + 1
左右
练习3.22考虑右边显示的持续的MDP。唯一要做的决定是在最上面的状态中，有两个动作可用，左和右。这些数字显示了每次行动后所获得的决定性奖励。正好有两个决定性的政策,πleftπright。最优如果γ= 0是什么政策?如果γ= 0.9 ?
如果γ= 0.5 ? 					?
练习3.23给贝尔曼方程问∗回收机器人。吗?图3.5给出网格世界的最佳状态的最佳值为24.4，到小数点后一位。使用你的最优策略知识和(3.8)来象征性地表达这个值，然后将其计算到小数点后三位。?

练习3.25 v的给一个方程的问∗∗。 					?

练习3.26给出方程问v的∗∗four-argument p。?

3.7。最优性和近似 					67年

练习3.27给出方程π∗问∗。 					?
练习3.28给出方程π∗∗v和four-argument p。练习3.29改写四个贝尔曼方程四个值函数(vπ,v∗、qπ和q∗)的三个参数函数p(3.4)和双参数函数r
(3.5)。 					?



\section{最优性和近似}

我们定义了最优值函数和最优策略。显然，了解最优策略的代理做得非常好，但在实践中这种情况很少发生。对于我们感兴趣的任务类型，最优策略只能以极高的计算成本生成。定义良好的最优性概念组织了我们在本书中描述的学习方法，并提供了一种理解各种学习算法的理论性质的方法，但理想情况是，代理只能在不同程度上近似。正如我们上面所讨论的，即使我们有一个完整而准确的环境动力学模型，通常也不可能仅仅通过求解Bellman最优方程来计算最优策略。例如，像国际象棋这样的棋类游戏只是人类经验的一小部分，然而大型定制设计的计算机仍然无法计算出最佳的移动速度。代理所面临的问题的一个关键方面是，它总是可用的计算能力，特别是它可以在单个时间步中执行的计算量。
可用内存也是一个重要的限制。通常需要大量内存来构建价值函数、策略和模型的近似。在具有小的、有限状态集的任务中，可以使用数组或表来形成这些近似，每个状态(或状态操作对)有一个条目。我们把这个叫做表格式案例，以及我们称之为表格法的相应方法。然而，在许多实际感兴趣的情况下，有比表中可能的条目多得多的状态。在这些情况下，必须使用某种更紧凑的参数化函数表示来逼近函数。
强化学习问题的框架迫使我们接受近似。然而，它也为我们提供了一些实现有用近似的独特机会。例如，在近似最优行为时，代理可能面临许多状态，其概率非常低，因此为其选择次优行为对代理收到的报酬影响不大。例如，泰索罗的西洋双陆棋棋手，即使在与专家的比赛中，他可能会在棋局配置上做出非常糟糕的决定。事实上,TD-Gammon可能使错误决策的一大部分游戏的状态集。在线强化学习的性质使得它可以近似最优政策的方式把更多的精力投入到学习为常见状态做出正确的决定,以牺牲较少的努力很少遇到。这是区分强化学习和近似求解MDPs的其他方法的一个关键属性。

\section{总结}

让我们总结一下我们在本章中提出的强化学习问题的要素。强化学习是指从互动中学习如何行为以达到目标。增强学习代理及其环境通过一系列离散时间步骤进行交互。它们的接口规范定义了一个特定的任务:操作是代理所做的选择;国家是作出选择的基础;奖励是评估选择的基础。代理商内部的一切都被代理商完全了解和控制;外界的一切都是不完全可控的，但可能是也可能不是完全可知。策略是一种随机规则，其中代理选择作为状态的函数。代理人的目标是在一段时间内获得最大的回报。
当前面描述的强化学习设置用定义良好的转移概率表示时，它构成了一个马尔可夫决策过程(MDP)。一个有限的MDP是一个具有有限状态、作用和(正如我们在这里所描述的)奖励集的MDP。目前的强化学习理论大多局限于有限的MDPs，但方法和思想的应用更为普遍。
回报是agent寻求最大化(期望值)的未来回报的函数。它有几个不同的定义，这取决于任务的性质，以及人们是否希望降低延迟奖励。不打折扣的提法适用于情景任务，在情景任务中，参与者与环境的交互自然地分解为多个片段;折后的公式适用于持续的任务，在这些任务中，交互不会自然地分解为多个片段，而是无限地继续。我们试图定义这两种任务的回报比如一组方程可以同时适用于情景和连续的情况。
如果代理使用策略，则策略的值函数分配给每个状态或状态操作对、该状态的预期回报或状态操作对。分配给每个状态或状态操作对的最优值函数，是任何策略所能实现的最大期望回报。价值函数是最优的策略是最优策略。对于给定的MDP，状态和状态操作对的最优值函数是唯一的，但是有许多最优策略。任何对最优值函数贪婪的策略都必须是最优策略。Bellman最优方程是最优值函数必须满足的特殊一致性条件，原则上可以求解最优值函数，从而相对容易地确定最优策略。
强化学习问题可以以各种不同的方式提出，这取决于对最初提供给代理的知识水平的假设。在完全知识的问题上，agent对环境的动力学有一个完整而准确的模型。如果环境是MDP，则该模型由完整的四参数动态函数p(3.2)组成。在知识不完整的问题上，没有一个完整的、完美的环境模型。
即使代理拥有完整和准确的环境模型，代理通常也无法执行足够的每次步骤计算来充分使用它。可用内存也是一个重要的限制。构建可能需要内存

精确逼近值函数、策略和模型。在大多数实际感兴趣的情况下，表中的状态远远多于可能的条目，必须进行近似。
定义良好的最优性概念组织了我们在本书中描述的学习方法，并提供了一种理解各种学习算法的理论性质的方法，但强化学习代理只能在不同程度上近似是理想的。在强化学习中，我们非常关注那些不能找到最优解但必须以某种方式近似的情况。

强化学习问题是最优控制领域的马尔可夫决策过程思想的产物。这些历史影响和其他来自心理学的主要影响在第一章的简要历史中被描述。强化学习增加了MDPs对逼近和不完全信息的关注，以解决实际的大问题。MDPs和强化学习问题与传统的人工智能学习和决策问题仅有微弱的联系。然而，人工智能现在正从不同的角度积极探索MDP的规划和决策公式。MDPs比以前在人工智能中使用的公式更普遍，因为它们允许更一般的目标和不确定性。
例如，Bertsekas (2005)， White (1969)， Whittle(1982, 1983)和Puterman(1994)研究了MDPs的理论。Ross(1983)对有限情况做了一个特别紧凑的处理。在随机最优控制的标题下也研究了多目标多目标规划，其中自适应最优控制方法与强化学习的关系最为密切(如Kumar, 1985;Kumar和Varaiya,1986)。
多边开发计划署的理论是从努力理解在不确定的情况下做出一系列决策的问题演变而来的，在这种情况下，每个决策都可以依赖于以前的决策及其结果。它有时被称为多级决策过程理论,或顺序决策过程,在序贯抽样统计文学的根开始论文的汤普森(1933、1934)和罗宾斯(1952),我们将在第二章与土匪问题(典型的mdp如果制定multiple-situation问题)。
使用MDP形式主义讨论强化学习的最早实例是Andreae (1969b)对学习机器的统一观点的描述。Witten and Corbin(1973)利用MDP形式主义实验了后来Witten (1977, 1976a)分析的强化学习系统。尽管Werbos(1977)没有明确地提到MDPs，但他对与现代强化学习方法相关的随机最优控制问题提出了近似的解决方法(参见Werbos, 1982, 1987, 1988, 1989, 1992)。尽管Werbos的想法在当时并没有得到广泛的认可，但他们有先见之明，强调了在包括人工智能在内的各种领域中近似解决最优控制问题的重要性。强化学习和MDPs最具影响的整合是由于Watkins(1989)。

3.1我们用p(s?r |s a)是
有点不寻常。在MDP文献中，用状态跃迁概率p(s?在强化学习中，我们更多的是参考个体实际的或样本的奖励(而不仅仅是他们的期望值)。我们的表示法也清楚地表明St和Rt通常是共同确定的，因此必须具有相同的时间索引。在强化学习教学中，我们发现我们的表示法在概念上更直观，更容易理解。
要更好地直观地讨论系统理论的状态概念，请参阅明斯基(1967)。
生物反应器的例子基于Ungar(1990)和Miller和Williams(1992)的成果。回收机器人的例子是受到乔纳森·康奈尔(Jonathan Connell)(1989)发明的can-collect机器人的启发。Kober和Peters(2012)展示了增强学习的机器人应用。

3.2奖赏假说由Michael Littman提出(personal communica-)
变形)。
3.3-4情景任务和持续任务的术语与通常不同
用于MDP文献。在该文献中，通常区分三种类型的任务:(1)有限范围任务，其中交互在特定的固定时间步数之后终止;(2)不确定视界任务，其中交互可以任意长时间，但最终必须终止;和(3)无限视界任务，其中相互作用不终止。我们的情景任务和连续任务分别类似于不确定的视界和无限的视界任务，但我们更喜欢强调交互性质的不同。这种差异似乎比通常术语所强调的目标函数的差异更为根本。情景性任务通常使用不确定层目标函数，而连续任务则使用无限层目标函数，但我们认为这是一个共同的巧合，而不是根本的区别。
平衡杆的例子来自于Michie和Chambers(1968)、Barto、Sutton和Anderson(1983)。
3.5-6根据长期的好坏来分配价值由来已久
的根源。在控制理论中，将状态映射到代表控制决策的长期后果的数值是最优控制理论的关键部分，该理论是在20世纪50年代通过扩展19世纪经典力学的状态函数理论而发展起来的(参见，Schultz和Melsa, 1967)。Shannon(1950)在描述计算机如何被编程去玩国际象棋时，建议使用一个考虑到国际象棋位置的长期优势和劣势的评估函数。
沃特金斯(1989)的q学习算法估计问∗(第六章)行为价值功能强化学习的一个重要组成部分,因此

这些函数通常被称为“q函数”。但动作价值函数的概念要比这个古老得多。Shannon(1950)认为，一个国际象棋程序可以使用一个函数h(P,M)来决定移动M是否值得探索。Michie’s(1961, 1963)的威胁系统和Michie和Chambers(1968)的盒子系统可以理解为估算行动价值函数。在经典物理学中，哈密尔顿的主函数是一个动作-值函数;牛顿动力学对于这个函数是贪婪的(例如，Goldstein, 1957)。动作-值函数在Denardo(1967)关于收缩映射的动态规划的理论处理中也发挥了中心作用。
贝尔曼最优方程(v∗)推广由理查德·贝尔曼(1957年),他称之为“基本函数方程。“连续时间和状态问题的Bellman最优方程的对应关系被称为哈密顿-雅可比- Bellman方程(或者通常是哈密顿-雅可比-雅可比方程)，表明它起源于古典物理学(例如，Schultz和Melsa, 1967)。高尔夫的例子是由克里斯·沃特金斯提出的。
