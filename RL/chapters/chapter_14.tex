\section{第14章 心理学}
\begin{summary}
	
	在前几章中，我们单独基于计算考虑开发了算法的思想。在这一章中，我们从另一个角度来看待这些算法:心理学的视角以及对动物如何学习的研究。本章的目标是，首先，讨论强化学习思想和算法与心理学家发现的动物学习相关的方法，其次，解释强化学习对学习动物学习的影响。强化学习系统化任务、返回和算法所提供的清晰形式证明对理解实验数据、提出新类型的实验以及指出可能对操作和测量至关重要的因素非常有用。长期优化回报的想法是强化学习的核心，它有助于我们理解动物学习和行为的其他令人困惑的特征。
	强化学习与心理学理论之间的一些对应关系并不令人惊讶，因为强化学习的发展受到了心理学学习理论的启发。然而，正如本书所阐述的，强化学习从人工智能研究者或工程师的角度探索理想化的情况，目的是用高效的算法解决计算问题，而不是复制或详细解释动物如何学习。因此，我们所描述的一些通信联系了各自领域中独立产生的思想。我们相信这些接触点是特别有意义的，因为它们揭示了对学习很重要的计算原理，无论是人工学习还是自然系统学习。
	在很大程度上，我们描述了强化学习和学习理论之间的对应关系，这些理论用来解释像老鼠、鸽子和兔子这样的动物是如何在受控的实验室实验中学习的。这些实验在整个20世纪进行了数千次，其中许多至今仍在进行。尽管这些实验有时被认为与心理学中更广泛的问题无关，但它们探究了动物学习的微妙特性，而这些特性往往是由精确的理论问题所激发的。随着心理学将注意力转移到行为的认知方面，也就是思维和推理等心理过程，动物学习实验开始发挥作用
	
	他们在心理学上的作用比以前小了。但是这种实验导致了学习原理的发现，这些原则在动物王国中是基本且普遍的，在设计人工学习系统时不应忽视这些原则。此外，正如我们将看到的，认知处理的某些方面自然地与强化学习提供的计算视角相连。
	本章的最后一节包含了与我们讨论的连接以及我们忽略的连接相关的引用。我们希望本章鼓励读者更深入地探究所有这些联系。最后一节还讨论了强化学习中使用的术语与心理学术语之间的关系。强化学习中使用的许多术语和短语都是从动物学习理论中借来的，但是这些术语和短语的计算/工程含义并不总是与它们在心理学中的含义一致。
	
\end{summary}

\section{预测和控制}

我们在本书中描述的算法分为两大类:预测算法和控制算法。在第三章提出的强化学习问题的解决方法中，这些类别自然产生。在许多方面，这些类别分别对应于心理学家广泛研究的学习类别:古典或巴甫洛夫，条件作用和工具性，或操作性，条件作用。这些对应关系并不是完全偶然的，因为心理学对强化学习的影响，但它们仍然是惊人的，因为它们把来自不同目标的想法联系在一起。
在这本书中提出的预测算法估计数量，这取决于一个代理环境的特征在未来会如何展开。我们特别关注的是评估一个代理在与环境交互时在未来可能收到的回报的数量。在这个角色中，预测算法是策略评估算法，是改进策略的算法的组成部分。但预测算法并不局限于预测未来的回报;他们可以预测环境的任何特征(比如，Modayil, White，和Sutton, 2014)。预测算法和经典条件作用之间的对应关系建立在预测即将到来的刺激的共同属性上，无论这些刺激是否有益(或有害)。
在工具性或操作性条件反射实验中，情况是不同的。在这里，实验装置被设置好，这样动物就会得到它喜欢的东西(奖励)或它不喜欢的东西(惩罚)，这取决于动物做了什么。动物学会增加产生奖励行为的倾向，减少产生惩罚行为的倾向。强化刺激据说取决于动物的行为，而在经典条件作用下则不是(尽管在经典条件作用实验中很难去除所有行为的偶然性)。工具条件作用实验就像那些启发了桑代克效应定律的实验

控制对我们的意义不同于动物学习理论的意义;在那里，环境控制着代理，而不是反过来。请看我们在本章末尾对术语的评论。

我们将在第一章中简要讨论。控制是这种学习形式的核心，它对应于强化学习策略改进算法的操作。
从预测的角度考虑经典条件作用，从控制的角度考虑工具条件作用，是我们将强化学习的计算观与动物学习联系起来的起点，但现实情况比这更为复杂。经典条件反射比预测更重要;它也包括行动，还有一种控制模式，有时被称为巴甫洛夫控制。此外，经典的和工具性的条件作用以有趣的方式相互作用，这两种学习方式都可能参与到大多数实验情境中。尽管有这些复杂性，将经典/工具区分与预测/控制区分对齐是将强化学习与动物学习联系起来的一种方便的第一近似。
在心理学中，强化一词被用来描述古典条件作用和工具条件作用下的学习。它最初只指行为模式的强化，也经常用于弱化行为模式。被认为是行为变化原因的刺激被称为强化物，不管它是否取决于动物先前的行为。在本章末尾，我们将更详细地讨论这个术语，以及它与机器学习中使用的术语之间的关系。

\section{经典条件作用}

俄罗斯著名生理学家伊凡·巴甫洛夫在研究消化系统的活动时发现，动物对某些触发刺激的先天反应可能是由与先天诱因完全无关的其他刺激触发的。他的实验对象是那些接受过小手术的狗，以便精确测量它们唾液反射的强度。在他描述的一个案例中，这只狗在大多数情况下都不会流口水，但在喂食5秒后，它会在接下来的几秒内分泌出6滴唾液。在多次重复呈现与食物无关的刺激之后，在这个例子中是节拍器的声音，在食物引入前不久，狗会对节拍器的声音产生唾液反应，就像它对食物的反应一样。“因此，唾液腺的活动被声音的冲动所激活——一种与食物完全不同的刺激”(巴甫洛夫，1927，第22页)。巴甫洛夫总结了这一发现的意义，写道:

很明显，在自然条件下，正常的动物不仅必须对刺激做出反应，这些刺激本身就能带来直接的好处或伤害，而且还必须对其他物理或化学介质——声音、光和类似的波——做出反应，而这本身只能表明这些刺激的接近;虽然它不是野兽的视觉和声音，它本身对较小的动物是有害的，但它的牙齿和爪子。(巴甫洛夫,1927年,p . 14)

以这种方式将新刺激与先天反射联系起来，现在被称为经典条件作用。巴甫洛夫(或者更确切地说，他的译者)称之为“天生反应”(如上图所示的唾液分泌)“无条件反应”(URs)，他们的。

自然触发刺激(如食物)“无条件刺激”(USs)，以及由预测刺激(如唾液分泌)触发的新反应“条件反应”(CRs)。一种最初是中性的刺激，这意味着它通常不会引起强烈的反应(例如，节拍器的声音)，成为一种“条件刺激”(CS)，因为动物了解到它预测了美国，所以在回应CS时产生了CR。这些术语仍然用于描述经典条件作用实验(尽管更好的翻译应该是“有条件的”和“无条件的”，而不是条件的和无条件的)。美国被称为强化物，因为它强化了对CS的反应生成CR。

跟踪调节延时调节
CS
我们

CS

我们
三军情报局
在两种常见的经典条件反射实验中，刺激物的排列被显示在右侧。在延迟条件作用下，CS贯穿于刺激间期(ISI)，即CS开始和美国开始之间的时间间隔(CS结束时，美国以这里所示的公共版本结束)。在跟踪条件作用中，CS结束后美国开始，CS偏移与US开始的时间间隔称为跟踪间隔。
t
巴甫洛夫的狗对着节拍器的声音流口水只是经典条件作用的一个例子
在许多动物的反应系统中进行了深入的研究。在某些方面，URs往往是预先准备好的，比如巴甫洛夫的狗的唾液分泌，或者以某种方式的保护，比如眼睛对刺激眼睛的事物的反应而眨眼，或者看到捕食者而被冻住。在一系列实验中，动物体验到CS-US的预测关系后，会知道CS可以预测美国，这样动物就可以用CR对CS做出反应，为动物做好准备，或者保护它不受预测的美国影响。一些CRs与UR相似，但开始的时间较早，并且在提高效率的方式上有所不同。例如，在一项深入研究的实验中，一种声音CS可以可靠地预测一只兔子的眼睛会吸入一股空气(美国)，引发一种UR，这种UR包括一个被称为“镍钛膜”的保护性内眼睑的闭合。在一次或多次试验之后，这种音调就会触发一种由膜闭合组成的CR反应，这种反应从喷气之前开始，最终会被计时，因此，当可能发生喷气时，就会出现峰值闭合。这种CR，是在预期的空气膨胀和适当的时间，提供更好的保护，而不是简单地启动关闭作为对激怒我们的反应。通过学习刺激之间的预测关系来预测重要事件的能力是非常有益的，它在整个动物王国中广泛存在。

\subsection{阻塞和高阶条件作用}

实验中观察到许多经典条件作用的有趣性质。除了CRs的预期性质外，两种被广泛观察的特性在经典条件反射模型的发展中起着重要作用:阻塞和高阶条件作用。阻塞发生在一个动物不能学习CR时提出了一个潜在的CS连同另一个CS,一直以前条件的动物生产CR。例如,在第一阶段的实验涉及到兔子瞬膜条件反射,一只兔子是第一条件语气CS和CR的空气吹我们生产关闭瞬膜的空气泡芙。实验的第二阶段包括额外的实验，在这个实验中，第二种刺激，比如一盏灯，被添加到音调中，形成一个复合的音调/灯光CS，然后同样的空气吹散我们。在实验的第三阶段,第二个刺激,光给兔子兔子是否已经学会应对CR。事实证明,兔子生产很少,甚至没有,CRs的光:学习光已经被前面的学习了基调。这样的阻塞结果挑战了条件作用只依赖于简单的时间连续的观点，也就是说，条件作用的一个必要而充分的条件是一个美国经常在时间上紧跟着一个CS。在下一节中，我们将描述Rescorla - Wagner模型(Rescorla和Wagner, 1972)，它为阻塞提供了一个有影响力的解释。
高阶条件反射发生在一个先前条件下的CS扮演一个美国的角色来调节另一个最初的中性刺激。巴甫洛夫描述了一项实验，实验中他的助手首先让狗对着节拍器的声音垂涎三尺，这个节拍器可以预测我们的食物。在这一阶段的训练之后，进行了一系列的实验，在实验中，狗最初并不在意的一个黑色方块被放置在狗的视线中，接着是节拍器的声音——而这并不是食物。在仅仅十次试验中，这只狗仅仅在看到黑广场时就开始流口水，尽管它的出现并没有被食物所吸引。节拍器本身的声音就像一个美国，把唾液分泌的声音转化成黑色方块c。这是二阶条件。如果黑色方块被用来作为一个美国建立唾液分泌CRs到另一个中立的CS，它将是三级条件作用，等等。高阶条件作用很难证明，尤其是在高阶条件作用下，部分原因是高阶强化物由于在高阶条件作用试验中没有被原来的美国重复跟随而失去其强化价值。但在适当的条件下，如将一阶试验与高阶试验混合，或通过提供普遍的激励刺激，高阶条件作用可被证明超越二阶。正如我们下面所描述的，经典条件作用的TD模型使用了引导思想，这是我们的方法的核心，来扩展Rescorla-Wagner模型对阻塞的描述，包括CRs的预期性和高阶条件作用。
高阶的仪器条件反射也发生了。在这种情况下，刺激

与控制组的比较是必要的，以表明先前对音调的条件作用是阻碍对光线的学习。这是通过音调/灯光CS的试验来实现的，但是没有
之前的调理。在这种情况下，光的学习是没有损害的。摩尔和Schmajuk
(2008)对这一程序做一个完整的说明。

一致预测初级强化本身就是一种强化物，如果它的奖励或惩罚的质量是通过进化建立在动物身上的，那么强化就是第一种强化物。预测刺激变成了次级强化物，或者更一般地说，高阶或条件强化物——后者是一个更好的术语当预测的强化刺激本身是次级的，甚至是更高阶的强化物。条件强化物提供条件强化:条件奖励或条件惩罚。条件性强化就像一级强化一样，增加了动物产生条件性奖励行为的倾向，减少了动物产生条件性惩罚行为的倾向。(请参阅我们在本章末尾的评论，它解释了我们的术语有时是如何与心理学中使用的术语有所不同的。)
条件强化是一个关键的现象，它解释了为什么我们为条件强化金钱工作，它的价值仅仅来自于拥有它所预言的东西。在第13.5节所描述的演员-评论家方法中(在第15.7和15.8节的神经科学中进行了讨论)，评论家使用TD方法来评价演员的政策，其价值估计为演员提供条件强化，允许演员改进其政策。这种高阶工具条件作用的模拟有助于解决第1.7节中提到的信贷分配问题，因为当主奖励信号被延迟时，评论家会对参与者进行时时刻刻的强化。我们将在下面的14.4节中讨论这个问题。

\subsection{Rescorla-Wagner模型}

Rescorla和Wagner创建他们的模型主要是为了解释阻塞。Rescorla-Wagner模型的核心理念是，只有当动物感到意外时，动物才会学习，换句话说，只有当动物感到意外的时候(尽管没有必要暗示任何有意识的期望或情感)。我们首先用术语和符号表示Rescorla和Wagner的模型，然后转向我们用来描述TD模型的术语和符号。
这就是Rescorla和Wagner对他们的模型的描述。该模型调整了复合CS的每个成分刺激的“联想强度”，这个数字表示该成分预测美国的强度或可靠性。当CS复合构成的几个组件刺激呈现在经典条件作用试验中,每个组件的关联强度刺激变化的方式取决于一个关联强度与整个刺激化合物,称为“聚合关联强度,而不只是在每个组件的关联强度本身。
Rescorla和Wagner认为这是一种化合物CS AX，由a和X组成，动物可能已经经历过a刺激，刺激X对动物来说可能是新的。让VA、VX和VAX分别表示刺激A、X和化合物AX的结合力。假设在试验中，化合物CS AX后面跟着一个US，我们把它命名为刺激物y，然后是它的结合力

刺激成分变化根据这些表达式:ΔVA =αAβY(RY−VAX)
ΔVX =αXβY(RY−VAX),
αAβY和αXβY步长参数,取决于CS组件和美国的身份,和联想的渐近水平从总体实力,美国可以支持。(Rescorla和瓦格纳在这里λR的使用,但我们使用R来避免混淆使用λ和因为我们通常认为这是一个奖励信号的大小,与经典条件作用的警告,美国不一定是奖励或惩罚)。该模型的一个关键假设是，总联合强度VAX等于VA + VX。联想的优势,改变了这些Δs成为联想的优势开始下一个试验。
为了完成这个模型，模型需要一个响应生成机制，这是一种将V的值映射到CRs的方法。因为这个映射依赖于实验情况的细节，Rescorla和Wagner没有指定映射，只是假设更大的V s会产生更强或更可能的CRs，而负的V s意味着没有CRs。
Rescorla-Wagner模型解释了以一种解释阻塞的方式获取CRs。VAX,只要总关联强度的刺激化合物低于渐近水平的关联强度,,我们可以支持,预测误差的RY−VAX是正的。这意味着，在连续的试验中，组成刺激的结合强度VA和VX会增加，直到总结合强度VAX等于RY，这时，结合强度停止变化(除非美国发生变化)。当一个新的组件添加到复合CS的动物已经条件,进一步调节增强复合产生很少或没有添加CS组件的关联强度增加,因为错误已经减少到零,或一个较低的值。美国的出现几乎已经得到了完美的预测，因此新的CS组件很少或没有错误(或令人惊讶)。之前的学习妨碍了对新组件的学习。
为了从Rescorla和Wagner的模型过渡到经典条件作用的TD模型(我们称之为TD模型)，我们首先根据我们在整本书中使用的概念重新塑造他们的模型。具体地说,我们匹配的符号使用学习线性函数近似(9.4节),我们认为学习的调节过程作为一个预测的“我们的”试验的基础上提出的复合CS的审判,美国Y的大小在哪里上面给出的Rescorla-Wagner模型的变化。我们也介绍。因为Rescorla-Wagner模型trial-level模型,这意味着它处理关联优势如何改变从试验,试验不考虑任何细节和之间的试验中发生了什么,我们不需要考虑如何在试验状态改变时,直到我们在以下部分呈现完整的TD模型。相反，在这里，我们简单地认为状态是一种标记试验的方式，根据试验中出现的CSs组件的集合。
因此，假设试验类型或状态s由特征的实值向量x(s) = (x1(s)， x2(s)，…,xd(s))?如果CSi是a的第i个分量，xi(s) = 1

复方CS，在试验中出现，否则为0。然后，如果联合强度的d维矢量为w，则三型s的总联想强度为。

v̂(s,w)= w ? x(s)。 					(14.1)
这对应于强化学习的价值估计，我们把它看作是美国的预测。
现在暂时让t表示一个完整的试验的数量,而不是它通常意义作为一个时间步(我们回到t通常意味着当我们扩展到下面的TD模型),并假定圣是国家相应试验t。调节试验t更新关联强度矢量wt wt + 1如下:

wt + 1 = wt +αδtx(圣), 					(14.2)

α是步长参数,——因为在这里,我们描述Rescorla -瓦格纳model-δt预测误差

δt = Rt−v̂(St,wt)。 					(14.3)
Rt是试验t的预测目标，也就是美国的规模，或者用Rescorla和Wagner的话来说，试验中的美国能支持的结合力。注意，由于(14.2)中的因子x(St)，只有在试验中出现的CS组件的结合力作为试验结果进行了调整。你可以把预测误差看作是一种惊喜的度量，而总联想强度则是当动物的期望值与目标美国的大小不匹配时所违反的期望。
从机器学习的角度看，Rescorla-Wagner模型是一个纠错监督学习规则。它本质上和最小均方(LMS)或Widrow-Hoff，学习规则(Widrow and Hoff, 1960)一样，发现权值——这里的结合力——使所有误差的平方的平均值尽可能接近于零。它是一种“曲线拟合”或回归算法，在工程和科学应用中得到广泛应用(见第9.4节)。
Rescorla-Wagner模型非常有影响力的动物学习理论的历史,因为它表明,一个“机械”的理论可以解释关于阻塞的主要事实不需要采取更复杂的认知理论涉及,例如,动物的明确认识到另一个刺激组件被添加,然后向后扫描其短期记忆重新评估涉及美国的预测关系。Rescorla-Wagner模型展示了如何用一种简单的方式调整传统的条件连续理论(即刺激的时间连续是学习的必要和充分条件)来解释阻塞(Moore and Schmajuk, 2008)。
Rescorla-Wagner模型提供了一个简单的关于阻塞的描述和一些古典条件作用的其他特征，但是它不是一个完整的或完美的古典模型

LMS规则和Rescorla-Wagner模型的唯一区别是LMS the
输入向量xt可以拥有任意实数作为组件,并且至少在最简单的版本的LMS状态步长参数α不依赖于输入向量或刺激的身份背景预测的目标。


调节。不同的想法解释了各种观察到的影响，在理解古典条件作用的许多微妙之处方面仍在取得进展。我们接下来将描述的TD模型，虽然也不是一个完整或完美的经典条件作用模型，但它扩展了Rescorla-Wagner模型，以解决刺激之间的试验内和试验间的时间关系如何影响学习以及如何产生高阶条件作用。

\section{TD模型}

TD模型是一个实时模型，与Rescorla - Wagner模型不同。Rescorla-Wagner模型中的一个单步t代表了一个完整的条件作用试验。该模型并不适用于在试验进行期间发生的事情或试验之间可能发生的事情的细节。在每次试验中，动物可能会经历不同的刺激，这些刺激在特定的时间发生，并且持续的时间也不同。这些时间关系强烈影响学习。Rescorla-Wagner模型也不包含高阶条件作用机制，而对于TD模型，高阶条件作用是基于TD算法的自举思想的自然结果。
为了描述TD模型，我们从上面的Rescorla-Wagner模型的提法开始，但是t现在在试验中或试验之间标注时间步数，而不是完整的试验。认为时间t,t + 1之间的一个小的时间间隔,说幅第二,和认为审判是一个序列的州,一个与每个时间步,现在国家在一步t代表刺激是如何表示的细节在t CS组件,而不只是一个标签出现在一个审判。事实上，我们可以完全放弃试验的想法。从动物的角度来看，试验只是它与世界互动的持续经验的一个片段。按照我们通常看到的代理与环境相互作用的观点，想象一下，动物经历了无数的状态序列，每个状态序列都由特征向量x(s)表示。也就是说，把试验称为刺激模式在实验中重复的时间片段通常还是比较方便的。
状态特征不局限于描述动物所经历的外部刺激;它们可以描述外部刺激在动物大脑中产生的神经活动模式，这些模式可以依赖于历史，这意味着它们可以是外部刺激序列产生的持久模式。当然，我们并不确切地知道这些神经活动模式是什么，但是像TD模型这样的实时模型可以让我们探索学习关于外部刺激的内部表征的不同假设的后果。由于这些原因，TD模型不提交任何特定的状态表示。此外，由于TD模型包含了在刺激之间跨越时间间隔的折现和资格跟踪，因此该模型还可以探索折扣和资格跟踪如何与刺激表示交互，从而对经典条件反射实验的结果进行预测。
下面我们将描述一些与TD模型一起使用的状态表示，以及它们的一些含义，但是目前我们仍然不知道

表示并假设每个状态s都由特征向量x(s) = (x1(s)， x2(s)，…,xn)?。然后，状态s对应的总结合力由(14.1)给出，与Rescorla-Wgner模型相同，但TD模型更新的结合力矢量w不同。现在t标记了一个时间步骤而不是一个完整的试验，TD模型根据这个更新来支配学习:

wt + 1 = wt +αδtzt, 					(14.4)
Rescorla-Wagner取代xt(St)的更新和zt型(14.2),一个向量的资格痕迹,和(14.3),而不是δtδt是TD错误:
δt = Rt + 1 +γv̂(wt)圣+ 1−v̂(St,wt), 					(14.5)
γ是折现系数(在0和1之间),预测目标在时间t Rt,v̂(wt圣+ 1)和v̂(St,wt)聚合关联优势在t + 1和t(14.1)所定义的。
每个组件我eligibility-trace矢量zt型增量或根据组件的精神性xi(St)的特征向量x(St),和其他衰变率由γλ:

zt型+ 1 =γλzt + x(St)。 					(14.6)
λ是通常的资格跟踪衰减参数。
注意,如果γ= 0,TD模型减少Rescorla-Wagner模型的异常:


14.2.4 TD模型模拟

像TD模型这样的实时调节模型之所以有趣，主要是因为它们可以预测许多无法用试验级模型表示的情况。这些情况包括条件刺激的时间和持续时间，这些刺激的时间与美国的时间有关，以及CRs的时间和形状。例如，美国通常必须在一个中性刺激开始后开始进行条件反射，学习的速度和有效性取决于刺激间间隔(ISI)，即CS和美国的一组之间的间隔。当CRs出现时，它们通常在美国出现之前就开始了，在学习过程中它们的时间分布也会发生变化。在使用复合CSs的条件作用下，复合CSs的组件刺激可能不会同时开始和结束，有时会形成所谓的串行化合物，其中的组件刺激会随着时间的推移而发生。像这样的时间考虑使得重要的是考虑刺激是如何表示的，这些表征是如何在试验期间和试验之间随时间展开的，以及它们是如何与折扣和资格痕迹交互的。

图14.1:三个刺激表示(列)有时用于TD模型。每一行表示刺激表示的一个元素。三个代表不同
沿时间概化梯度，在全序列化合物(左列)中相邻时间点之间不进行概化，在相邻时间之间完全概化
存在表示中的点(右列)。microstimulus表示占据
一个中间立场。时间的泛化程度决定了时间的粒度。
通过这些，我们学会了预测。适应学习和行为的微小变化，评价经典条件作用的TD模型，第40卷，2012，第311页，E. A. Ludvig, R. S。
萨顿,e . j .凯赫。施普林格的许可。



图14.1显示了用于探究TD模型行为的三个刺激表示:完整的串行化合物(CSC)、微刺激(MS)和存在表示(Ludvig、Sutton和Kehoe, 2012)。这些表示形式的不同之处在于，它们在刺激出现的邻近时间点之间强制泛化的程度不同。
图14.1中最简单的表示形式是图右列中的表示形式。此表示对于在试验中出现的每个组件CS都有一个单独的特性，其中该特性在该组件出现时的值为1，否则为0。在场表征并不是关于刺激物如何在动物的大脑中被表现出来的现实假设，但是正如我们下面所描述的，带有这种表征的TD模型可以产生许多经典条件作用下看到的时间现象。
对于CSC表示(图14.1的左列)，每个外部刺激的出现都会引发一系列精确定时的短持续内部信号

4在我们的形式,有不同的状态,圣,为每个时间步t庭审中翻供,以及试验中,一个复合CS由n组件CSs各种时间发生在整个试验中,在不同时期有一个特性,xi,对于每个组件CSi,i = 1,。其中xi(St) = 1表示
当CSi存在时，所有的时间都是t，否则等于0。


持续到外部刺激结束。这就好比假设动物的神经系统有一个时钟，它可以在刺激时精确地记录时间;这就是工程师们所谓的“抽头延迟线”。与在场表征一样，CSC表征作为大脑内部如何表示刺激的假设是不现实的，但Ludvig等人(2012)称其为“有用的虚构”，因为它可以揭示TD模型在相对不受刺激表征约束的情况下是如何工作的细节。CSC表示也被用在大脑中产生多巴胺的大多数TD模型中，这是我们在第15章中讨论的话题。CSC表示经常被视为TD模型的一个重要部分，尽管这种观点是错误的。
MS表示法(图14.1的中心列)类似于CSC表示法，因为每个外部刺激都引发一系列内部刺激，但在这种情况下，内部刺激——微刺激——并不是如此有限和不重叠的形式;它们会随着时间和重叠而延长。随着时间从刺激开始，不同的微刺激组变得或多或少的活跃，每一个后续的微刺激逐渐扩大，并达到一个较低的最高水平。当然,有很多女士表示根据microstimuli的性质,和很多女士的例子表示研究文献中,在某些情况下还有建议动物的大脑如何产生(见文献和历史评论这一章结束时)。MS表示法比存在或CSC表示法更现实，它们是关于刺激物的神经表示法的假设，它们允许TD模型的行为与动物实验中观察到的更广泛的现象集合相关。特别是microstimuli的假设瀑布是由航空母舰以及CSs,并通过研究显著影响学习microstimuli之间的交互,资格痕迹,打折,TD模型帮助框架假设占经典条件作用的许多微妙的现象和动物的大脑如何产生。我们将在下面讨论更多，特别是在第15章，我们将讨论强化学习和神经科学。
然而，即使使用简单的存在表示，TD模型也产生了Rescorla-Wagner模型解释的所有经典条件作用的基本属性，以及超出试验级模型范围的条件作用的特性。例如，正如我们已经提到的，经典条件作用的一个显著特征是，美国通常必须在条件作用出现的中性刺激开始后开始，条件作用发生后，CR开始于美国出现之前。换句话说，条件反射通常需要一个积极的ISI，而CR通常会预期到美国。条件作用的强度(例如，CS引起的CRs百分比)取决于ISI在不同物种和反应系统中的差异，但它通常具有以下特性:对于零ISI或负ISI，它是可以忽略的，即。，当美国发病与CS发病同时发生，或比CS发病更早时(尽管研究发现，联想强度有时会略有增加，或与CS发生负相关

5 .在我们的形式主义中，对于每一个CS组件的CSi，在一个试验中，并且在每一个时间t在a。
试一下，有一个单独的特性，如果t = t，那么xt i(St?) = 1 ?对于任何t ?现场CSi和
否则等于0。这与Sutton和Barto(1990)中的CSC表示不同
每个时间步骤都有相同的特点，但没有对外界刺激的参考;因此,
完整的系列化合物名称。

前
在第一个CS (CSA)和美国之间的空跟踪间隔中填充第二个CS (CSB)，形成一个串行复合刺激，然后方便条件作用于CSA。右边显示的是TD模型的行为，该模型在模拟这样一个实验中的存在表示，其时间细节如上所示。与实验结果相一致(Kehoe, 1982)，该模型显示了第一个CS在第二次CS出现时的调节速率和第一个CS的渐近水平。
这是一个著名的例子。

圣e年代
的


道明模型中的埃格-米勒效应
在一个试验中，对刺激之间的时间关系的调节作用是由埃格和米勒(1962)所做的一个实验，该实验涉及两个重叠的CSs在延迟配置中的作用，如图右上方所示。尽管CSB与美国的时间关系更好，但CSA的存在大大降低了CSB与CSA不存在的对照组的条件反射。在这个实验的模拟中，直接向右显示了TD模型产生的相同结果。
TD模型解释了阻塞，因为它是一个错误校正的学习规则，就像Rescorla-Wagner模型一样。除了计算基本的阻塞再

354年	第14章:心理学
然而，sults, TD模型预测(包括存在表示和更复杂的表示)，如果阻塞的刺激被提前移动，那么阻塞就会被逆转，从而在阻塞发生之前发生

图14.2:TD模型中的时间优先级覆盖阻塞。
阻断刺激的开始(如右图中的CSA)。TD模型行为的这一特征值得关注，因为在模型引入时没有观察到它。回想一下，在闭锁过程中，如果一个动物已经知道一个CS可以预测一个美国人，那么这个新增加的第二个CS也可以预测美国人的减少。,被阻塞。但是，如果新添加的第二个CS比预先训练的CS开始得早，那么——根据TD模型——对新添加CS的学习不会被阻塞。事实上，随着训练的继续，新增加的CS增加了联想力，训练前的CS失去了联想力。在这些条件下，TD模型的行为如图14.2的下半部分所示。
这个模拟实验不同于Egger-Miller实验(上一页的底部)，即较短的CS在与美国完全相关之前进行了预先训练。这一惊人的预测使得Kehoe、Schreurs和Graham(1987)使用经过充分研究的兔子显微膜制剂进行了实验。他们的研究结果证实了该模型的预测，他们指出，非td模型在解释其数据时存在相当大的困难。
与TD模型,早期预测刺激优先于后预测刺激,因为在这本书中描述的所有预测方法、TD模型基于倒车或引导理念:更新关联优势优势在特定状态转向的力量之后。自举的另一个结果是，TD模型提供了高阶条件作用的描述，这是古典条件作用的一个特性，超出了Rescoral-Wagner和类似模型的范围。正如我们上面所描述的，高阶条件反射是一种现象，在这种现象中，先前条件反射的CS可以作为一个美国来调节另一个最初的中性刺激。图14.3显示了TD模型(同样是存在表示)在一个高阶条件反射实验中的行为——在这种情况下，它是二级条件反射。在第一阶段(图中没有显示)，CSB被训练去预测一个美国，使它的结合力增加，这里是1.65。在第二阶段，CSA在没有美国的情况下与CSB配对，按照图顶部所示的顺序排列。尽管CSA从未与美国配对，但它仍具有结合力。通过持续的训练，CSA的结合力达到峰值，然后下降，因为第二强化CSB的结合力降低，失去了提供二次强化的能力。CSB的联想

图14.3:TD模型的二阶条件反射。
因为在这些高阶条件作用试验中，美国没有出现，所以强度会降低。这些是CSB的灭绝试验，因为它与美国的预测关系被破坏了，所以它作为强化物的能力下降了。在动物实验中也可以看到同样的模式。在高阶条件反射试验中，条件强化的消失使得演示高阶条件反射变得困难，除非原始的预测关系周期性地通过插入一阶试验来恢复。
TD模型产生第二和高阶条件的模拟,因为γv̂(wt)圣+ 1−v̂(St,wt)出现在TD错误δt(14.5)。这意味着,由于之前的学习,γv̂(wt圣+ 1)
可以不同于v̂(St,wt),使δt零(时间不同)。这种差异与(14.5)中Rt+1的状态相同，这意味着，在学习方面，时间差异和发生在美国之间没有区别。实际上，TD算法的这一特性是其发展的主要原因之一，我们现在通过它与动态编程的联系理解了这一点，如第6章所述。自举值与二阶、高阶条件作用密切相关。

在上面描述的TD模型行为的例子中，我们只研究了CS组件的关联强度的变化;我们没有研究模型对动物条件反射(CRs)特性的预测:它们的时间、形状，以及它们在条件反射试验中是如何发展的。这些特性取决于物种、观察到的反应系统和条件作用试验的参数，但在许多不同动物和不同反应系统的实验中，CR的大小或CR的概率会随着美国的预期时间的增加而增加。例如,在经典条件作用的一只兔子的瞬膜反应,我们上面提到的,在条件反射实验中延迟从CS出现当瞬膜开始跨越眼随试验,和这个先行关闭的振幅逐渐增加在CS和美国之间的间隔,直至膜达到最大关闭预计美国时间。CR的时间和形状对其适应性意义至关重要——过早地覆盖眼睛会降低视力(即使硅胶膜是半透明的)，而过晚覆盖则没有什么保护价值。对于经典条件作用的模型来说，获取这样的CR特征是一个挑战。
TD模型不包括其定义任何机制来翻译我们预测的时间进程,v̂(St,wt),到一个配置文件,可以与动物的CR。最简单的属性选择的时间进程

模拟的CR等于美国预测的时间过程。在这种情况下,模拟CRs的特点以及它们如何改变试验只取决于刺激表示选择和模型的参数的值α、γ,λ。
图14.4显示了我们在学习过程中不同时间点的预测时间过程，如图14.1所示。这些模拟美国发生25次步骤发病后的CS,α= . 05,λ= .95和γ= .97点。通过CSC表示(图14.4左)，TD模型形成的美国预测曲线在CS和美国之间的区间内呈指数增长，直到达到美国发生时的最大值(在第25步)。这种指数增长是TD模型学习规则折现的结果。由于存在表示法(图14.4中)，在刺激出现时，美国的预测几乎是不变的，因为只有一种重量，或联想强度，用于每一种刺激的学习。因此，具有存在表示的TD模型不能重新创建CR计时的许多特性。使用MS表示(图14.4右侧)，TD模型的美国预测的开发就更加复杂了。经过200次试验，预测的轮廓是用CSC表示生成的美国预测曲线的合理近似。

图14.4:三种不同刺激表征的TD模型在获取过程中的时间过程预测。左:有了完整的串行化合物(CSC)，美国的预测在时间间隔中呈指数增长，在美国达到顶峰。在渐近线(试验200)，美国的预测在美国强度上达到顶峰(在这些模拟中为1)。中间:在存在表示的情况下，美国的预测几乎是不变的。
的水平。这个恒定的水平是由美国强度和CS-US间隔的长度决定的。
右:微刺激表示，在渐近点，TD模型接近
用CSC通过线性组合表示的指数增长的时间过程
microstimuli不同。适应学习和行为的微小变化，评价经典条件作用的TD模型，第40卷，2012,E. A. Ludvig, R. S. Sutton, E. J. Kehoe。施普林格的许可。



图14.4所示的美国预测曲线并不是为了精确地匹配在任何特定动物实验条件作用下的CRs曲线，但它们说明了刺激表现对从TD模型得出的预测的强大影响。此外，尽管我们只能在这里提到，刺激方案如何与折扣和资格跟踪相互作用，这一点很重要。

在确定由TD模型产生的美国预测剖面的性质时。另一个超越我们在这里讨论的维度是不同的反应生成机制的影响，这些机制将我们的预测转化为CR档案;图14.4所示的剖面图是“原始”的美国预测剖面图。即使没有任何特殊的假设关于动物的大脑可能会产生明显的反应我们的预测,然而,CSC的概要文件在图14.4和女士表示增加的时间我们方法和达到最大时的我们,就像在许多动物条件反射实验。
TD模型，结合特定的刺激表示和反应生成机制，能够解释在动物经典条件反射实验中观察到的异常广泛的现象，但它远不是一个完美的模型。为了生成经典条件作用的其他细节，需要对模型进行扩展，可能需要添加基于模型的元素和机制，以便自适应地修改其某些参数。其他建模经典条件反射的方法明显地偏离了rescorla - wagner风格的错误修正过程。例如，贝叶斯模型在经验修正概率估计的概率框架内工作。所有这些模型都有助于我们理解经典条件作用。
TD模型的最显著特点是它是基于一个理论的理论我们已经描述了在这个书,表明的是什么动物的神经系统要做而进行调节:它试图形成准确的长期预测,符合所强加的限制表示刺激的方式以及神经系统是如何运作的。换句话说，它提出了一个经典条件作用的规范性解释，在这个解释中，长期而非直接的预测是一个关键特征。
经典条件作用的TD模型的发展就是一个明确的目标是对动物学习行为的一些细节进行建模的实例。除了作为一种算法的地位，TD学习也是生物学习这一模型的基础。正如我们在第15章中所讨论的，TD学习也证实了一个有影响的模型，即产生多巴胺的神经元的活动。多巴胺是哺乳动物大脑中的一种化学物质，与奖赏过程密切相关。在这些例子中，强化学习理论与动物行为和神经数据进行了详细的联系。
我们现在转向考虑工具条件作用实验中强化学习和动物行为之间的对应关系，这是动物学习心理学家研究的另一种主要的实验类型。

\section{工具性条件作用}

在工具性条件反射实验中，学习依赖于be-havior的后果:一种强化刺激的传递取决于动物的行为。相比之下，在经典的条件反射实验中，强化刺激——美国——是独立于动物行为的。器质性条件作用通常被认为和操作性条件作用是一样的，b·f·斯金纳(1938,1963)在行为-或有强化实验中引入的术语

使用这两个术语的人的观点和理论有很多不同之处，其中一些我们将在下文中提及。我们将专门用“工具条件作用”这个词来描述实验，在实验中，强化取决于行为。工具性条件作用的根源可以追溯到美国心理学家爱德华·桑代克(Edward Thorndike)所做的实验。
桑代克观察了猫的行为。

桑代克的益智盒之一。
《动物智力:动物联想过程的实验研究》，《心理学评论》，第二(4)专著系列，纽约麦克米伦出版社，1898年版。
当他们被放置在“拼图盒子”中，例如右边的那个，他们可以通过适当的行动逃跑。例如，一只猫可以通过三个不同的动作来打开一个盒子的门:把盒子后面的一个平台压下去，抓着一根绳子拉绳子，把一根棍子向上或向下推。当它们第一次被放在一个益智盒里，外面可以看到食物时，除了桑代克的一些猫外，其他的猫都表现出“明显的不适迹象”和异常剧烈的活动，“本能地想要逃离监禁”(桑代克，1898)。
用不同的猫和
有不同逃生机制的盒子里，桑代克记录了每只猫在每个盒子里经历多次逃生所花费的时间。他观察到时间几乎总是随着连续的经历而减少，例如，从300秒到6秒或7秒。他在一个益智盒里这样描述猫的行为:

那只猫在她的冲动挣扎中在盒子上抓来抓去的猫很可能会用爪子抓绳子、圈或按钮来开门。渐渐地，所有其他的不成功的冲动都会被消除，而导致成功行为的特定冲动会被由此带来的快乐所压制，直到，经过多次尝试，猫在被放进盒子后，会立即以一种确定的方式抓住按钮或循环。(桑代克1898年,p . 13)
这些实验和其他的实验(有些是用狗、小鸡、猴子甚至鱼做的)让桑代克制定了一些学习的“法则”，其中最具影响力的是“效果法则”，我们在第1章(第15页)中引用了它的一个版本。这条定律描述了人们通常所说的“试错法”。正如第一章中所提到的，影响定律的许多方面都引起了争议，其细节也在多年来不断修改。然而，法律——无论哪种形式——仍然表达了一种持久的学习原则。
强化学习算法的本质特征与动物学习的特征相对应。首先，强化学习算法是可选择的，这意味着他们会尝试不同的选择，并通过比较它们的结果进行选择。第二，强化学习算法是关联的，这意味着通过选择找到的替代方法与特定的情况或状态相关联，从而形成代理策略。就像学习用效应定律来描述，强化

学习不仅是寻找能产生大量回报的行为的过程，而且是将这些行为与情景或状态联系起来的过程。桑代克通过“选择和连接”(Hilgard, 1956)使用了这个短语学习。进化中的自然选择是选择过程的一个主要例子，但它不是关联的(至少正如人们普遍理解的那样);监督学习是有关联的，但它不是选择性的，因为它依赖指令直接告诉代理如何改变它的行为。
在计算术语中，效果定律描述了一种将搜索和记忆结合在一起的基本方法:在每种情况下尝试和选择许多操作的形式中进行搜索，在将情况与发现的操作联系起来的形式中进行记忆——到目前为止——在这些情况下效果最佳。搜索和记忆是所有强化学习算法的基本组成部分，无论记忆是采用代理策略、值函数还是环境模型的形式。
强化学习算法的搜索需求意味着它必须以某种方式进行探索。动物们显然也在探索，早期的动物学习研究人员对一种动物在类似桑代克的益智盒的情况下选择其行为的指导程度并不认同。行为是“绝对随机、盲目摸索”的结果(伍德沃斯，1938年，第777页)，还是有某种程度的指导，要么是通过事先学习、推理，要么是其他方式?尽管包括桑代克在内的一些思想家似乎已经采取了前者的立场，但其他人则倾向于更深思熟虑的探索。增强学习算法为代理在选择行为时可以使用多少指导提供了很大的自由度。形式的探索我们使用在这本书中给出的算法,如ε-greedy和upper-confidence-bound行动选择,仅仅是最简单的。更复杂的方法是可能的，唯一的规定是必须对算法进行某种形式的探索才能有效地工作。
我们处理强化学习的特性允许一组行动在任何时候都依赖于环境的当前状态，这与Thorndike在他的猫的益智盒行为中观察到的一些东西相呼应。这些猫从它们本能地在当前环境下的行为中选择动作，桑戴克称之为它们的“本能冲动”。首先，猫被放在一个益智盒里，它本能地用巨大的能量抓挠、抓爪和咬东西:猫在有限的空间中找到自己时的本能反应。成功的行动是从这些行动中选择的，而不是从每个可能的行动或活动中选择。这就像我们形式主义的特征，从一个国家中选择的行为属于一组可接受的行为，a (s)。指定这些集合是增强学习的一个重要方面，因为它可以从根本上简化学习。它们就像动物的本能冲动。另一方面，桑代克的猫可能是根据本能的特定情境而不是行动的顺序来探索，而不是仅仅从一组本能的冲动中进行选择。这是另一种使强化学习更容易的方法。
受影响的最著名的动物学习研究者包括克拉克·赫尔(如赫尔，1943)和b·f·斯金纳(如斯金纳，1938)。他们研究的中心是基于行为的后果来选择行为。强化学习具有与赫尔理论相同的特点，包括类似于精英的机制和二次强化，以解释当行为与随后的强化之间存在显著的时间间隔时的学习能力

刺激(见14.4节)。随机性也在赫尔的理论中发挥了作用，通过他所谓的“行为振荡”来引入探索性行为。
斯金纳并没有完全认同记忆方面的影响定律。他反对联想联系的概念，相反，他强调从自发的行为中选择。他引入了“操作性”一词来强调行动对动物环境的影响的关键作用。与桑代克和其他实验不同的是，斯金纳的操作条件作用实验允许动物实验在长时间内不间断地进行。他发明了操作性条件反射室，现在被称为“斯金纳箱”，其中最基本的版本包含一个杠杆或钥匙，动物可以按这个杠杆或钥匙来获得奖励，比如食物或水，这些奖励将根据一个定义明确的规则(称为强化计划)交付。通过记录杠杆按压的累积次数作为时间的函数，斯金纳和他的追随者可以研究不同的强化时间表对动物杠杆按压速度的影响。使用我们在本书中介绍的强化学习原理来模拟这些实验的结果并不是很完善，但是我们在本章末尾的书目和历史评论部分中提到了一些例外。
斯金纳的另一项贡献来自于他对训练动物的有效性的认识，他通过不断强化对所期望行为的近似，他称之为“塑造”过程。虽然这种方法已经被包括斯金纳在内的其他人使用过，但是当他和他的同事们试图用鸽子的嘴敲击一个木球来训练鸽子打碗的时候，这种方法的重要性却给他留下了深刻的印象。在等待了很长一段时间后，他们没有看到任何可以加强的迹象
…决定要加强任何与“swipea”有细微相似之处的反应——也许，起初仅仅是观察球的行为，然后选择更接近最终形式的反应。结果让我们感到吃惊。不出几分钟，球就从箱子的墙壁上滚下来，好像鸽子是壁球冠军似的。(斯金纳,1958,p . 94)
鸽子不仅学会了一种对鸽子来说不寻常的行为，它还通过一种相互作用的过程迅速地学会了这种行为，在这个过程中，鸽子的行为和强化事件会因彼此的反应而发生变化。斯金纳将加固过程与雕刻家将黏土塑造成理想的形状进行了比较。整形是计算强化学习系统的一种强有力的技术。当一个代理很难收到任何非零的奖励信号时，无论是由于奖励情况的稀疏性，还是由于初始行为的不可接近性，从一个更容易的问题开始，随着代理学习逐渐增加难度，都可以是一种有效的、有时是不可缺少的策略。
来自心理学的一个概念在工具条件作用的背景下特别相关，那就是动机，它是指影响行为的方向和力量或活力的过程。例如，桑代克的猫被驱使着逃离益智盒，因为它们想要的是外面的食物。获得这个目标对他们是有益的，并加强了允许他们逃跑的行动。很难把动机的概念精确地联系起来，因为它有很多维度

强化学习的计算视角，但与它的一些维度有明显的联系。
从某种意义上说，强化学习行为者的奖励信号是其动机的基础:行为者的动机是使其长期获得的总奖励最大化。因此，动机的一个关键方面是，是什么让一个代理人的经验值得。在强化学习中，奖励信号取决于强化学习主体的环境和行为状态。此外，正如第1章所指出的，代理环境的状态不仅包括关于机器外部的信息，如存放代理的有机体或机器人，还包括机器内部的信息。一些内部状态成分对应于心理学家所说的动物的动机状态，它影响着对动物的奖励。例如，动物在饥饿的时候吃比刚吃完一顿令人满意的饭时吃得更多。国家依赖的概念足够广泛，可以允许多种类型的调制对奖励信号产生的影响。
价值函数为心理学家的动机概念提供了进一步的联系。如果选择一个行动的最基本的动机是为了获得尽可能多的回报,为强化学习代理选择操作使用价值函数,一个更近的动机是提升其价值函数的梯度,即选择行为将导致大多数高估值的下一个国家(或本质上是一样的,选择行动最大的action-values)。对于这些代理，价值函数是决定其行为方向的主要驱动力。
动机的另一个维度是，动物的动机状态不仅影响学习，而且也影响动物学习后行为的强度或活力。例如，在学会在迷宫的目标盒子里找到食物后，饥饿的老鼠比不饿的老鼠跑得更快。动机的这个方面并没有如此清晰地与我们在这里展示的强化学习框架联系起来，但是在这一章末尾的书目和历史评论部分，我们引用了几篇论文，提出了基于强化学习的行为活力理论。
当强化刺激在强化事件后发生时，我们现在转向学习的主题。强化学习算法用于延迟强化学习的机制——合格跟踪和TD学习——与心理学家关于动物如何在这些条件下学习的假设非常相似。


\section{延迟强化}

影响定律要求对关系产生一种落后的影响，一些早期的法律批评家无法想象现在会如何影响过去的事物。当一个行为与随后的奖励或惩罚之间有相当大的延迟时，学习甚至可能发生，这一事实进一步加剧了这种担忧。类似地，在经典条件作用下，当我们的开始与CS相抵消时，学习也会发生。我们称之为延迟强化问题，这与明斯基(1961)所说的“学习系统的信贷分配问题”有关:how do you

将成功的荣誉分配到许多可能涉及到的决策中?本书介绍的强化学习算法包括两个基本的机制来解决这个问题。第一个是使用资格跟踪，第二个是使用TD方法来学习价值函数，这些函数提供几乎即时的行为评估(在工具条件作用实验中)或者提供即时预测目标(在经典条件作用实验中)。这两种方法都对应于动物学习理论中提出的类似机制。
Pavlov(1927)指出，每一种刺激都必须在神经系统中留下一个在刺激结束后持续一段时间的痕迹，他提出，当CS抵消和美国发作之间存在时间间隔时，刺激痕迹使学习成为可能。直到今天，在这些条件下的条件作用被称为跟踪条件作用(第344页)。假设当美国到达时CS的踪迹仍然存在，那么通过跟踪和美国同时存在，就可以进行学习。我们在第十五章讨论了神经系统中微量机制的一些建议。
刺激痕迹也被提出作为一种手段，以弥合行动之间的时间间隔和结果奖励或惩罚在工具性条件作用。例如，在赫尔影响深远的学习理论中，“磨牙刺激痕迹”解释了他所说的动物的目标梯度，描述了工具条件反应的最大强度是如何随着强化延迟的增加而降低的(赫尔，1932,1943)。赫尔假设，动物的行为留下了内部刺激，其痕迹随着时间的作用而呈指数衰减。通过观察当时的动物学习数据，他假设在30到40秒后，这些痕迹会有效地达到零。
本书中描述的算法中使用的资格跟踪类似于赫尔的跟踪:它们是过去状态访问或过去状态操作对的衰减跟踪。Klopf(1972)在他的神经元理论中引入了资格追踪，在这一理论中，它们是突触间过去活动的时间延伸，神经元之间的连接。Klopf的轨迹比我们的算法使用的指数衰减轨迹更复杂，当我们在第15.9节讨论他的理论时，我们会进一步讨论这个问题。
赫尔(Hull)(1943)提出，由于条件强化与磨牙刺激痕迹的共同作用，使目标梯度从目标向后传递，从而产生了更长的梯度。动物实验表明，如果条件有利于条件强化在延迟期的发展，那么学习就不会像在阻碍二次强化的条件下那样随着延迟的增加而减少。如果在延迟间隔期间有经常发生的刺激物，则条件强化更受欢迎。然后就好像奖励并没有因为有更直接的条件强化而被延迟。赫尔因此设想有一个主要的梯度，基于由刺激痕迹介导的初级强化的延迟，这是逐步修改和延长，由条件强化。
在这本书中提出的算法使用资格跟踪和价值函数使学习延迟强化对应于赫尔的假设如何

动物能够在这些条件下学习。在第13.5节、第15.7节和第15.8节中讨论的演员-批评家架构最清楚地说明了这种对应关系。批评家使用TD算法来学习与系统当前行为相关联的值函数，即预测当前策略的返回值。演员根据评论家的预测更新当前的政策，或者更准确地说，基于评论家的预测的变化。评论家产生的TD错误作为一个条件强化信号，为参与者提供即时的绩效评估，即使主要奖励信号本身被严重延迟。评估动作价值函数的算法，如Q-learning和Sarsa，同样使用TD学习原理，通过条件强化的方法使学习延迟强化。我们在第15章讨论的TD学习和多巴胺产生神经元的活动之间的紧密联系为强化学习算法和赫尔学习理论的这一方面提供了额外的支持。


\section{认知地图}

基于模型的强化学习算法使用环境模型，这些模型与心理学家所说的认知地图有共同的元素。回忆的规划和学习我们所讨论的在第8章,通过一个环境模型我们意味着任何一个代理可以用来预测其环境将如何应对其行为状态转换和奖励,我们所指的规划过程,计算一个政策从这样一个模型。环境模型由两部分组成:状态转换部分编码关于行为对状态变化的影响的知识，奖励模型部分编码关于每个状态或每个状态-动作对期望的奖励信号的知识。基于模型的算法通过使用一个模型来选择行为，根据未来的状态和预期的奖赏信号来预测可能的行为过程的结果。最简单的计划是比较“想象”的一系列决策集合的预测结果。
关于动物是否使用环境模型的问题，如果使用环境模型，模型是什么样子的以及它们是如何学习的，在动物学习研究的历史中发挥了重要的作用。一些研究人员挑战了当时流行的学习和行为的刺激反应(S-R)观点，这与最简单的无模型的学习策略相对应，通过展示潜在的学习。在最早的潜在性学习实验中，两组大鼠在迷宫中奔跑。实验组在实验的第一阶段没有奖励，但是在第二阶段开始的时候，食物突然被引入了迷宫的目标盒中。对于对照组，在两个阶段，食物都在目标框中。问题是，在没有食物奖励的情况下，实验组的老鼠在第一阶段是否会学到任何东西。虽然实验大鼠在第一次实验中似乎没有学到很多东西，但当他们发现第二阶段引入的食物后，他们很快就发现了对照组的老鼠。结论是“在非奖励期，[实验组]的大鼠发展出一种对迷宫的潜在性学习，一旦奖励被引入，他们就能利用这些潜在性学习”(Blodgett, 1929)。

潜伏学习是最密切相关的心理学家爱德华•杜尔曼解释这个结果,和其他类似,表明动物可以学习环境的“认知地图”没有奖励或处罚,,他们可以使用地图后,他们有动机去实现一个目标(杜尔曼,1948)。一份认知地图也可以让老鼠计划一条通往目标的路线，而这条路线与老鼠在最初的探索中所使用的路线不同。对这些结果的解释导致了长期存在的争议，这是行为主义者/认知心理学二分法的核心。现代意义上，认知地图并不局限于空间布局的模型，而是更普遍的环境模型，或者是动物“任务空间”的模型(如Wilson, Takahashi, Schoenbaum, and Niv, 2014)。潜在学习实验的认知地图解释类似于动物使用基于模型的算法，环境模型即使没有明确的奖励或惩罚也可以学习。当动物被奖励或惩罚的表象所激励时，模型被用于计划。
托尔曼对动物如何学习认知地图的描述是，当动物探索环境时，它们通过经历一连串的刺激来学习刺激-刺激或S-S联系。在心理学中，这被称为期望理论:考虑到S-S关联，刺激的出现会产生下一个刺激的预期。这很像控制工程师所说的系统识别，在系统识别中，一个不知道动态的系统模型是从标记的训练例子中学到的。在最简单的离散时间版本中，训练例子是S-S ?对，S是状态，S是什么?，其后的状态是标签。当观察到S时，模型产生了S的“期望”。将观察到的下一个。对计划更有用的模型也包括操作，因此示例看起来像SA-S ?,年代在哪里?当行为A在状态s中执行时，它是被期望的。在本例中，例子是形式S - R或SA - R，其中R是与S或SA对相关联的奖励信号。这些都是监督学习的形式，通过这些形式，agent可以获得认知地图，无论它在探索它的环境时是否接收到任何非零的奖励信号。



\section{习惯性和目标导向的行为}

无模型强化学习算法和基于模型的强化学习算法之间的区别与心理学家对习得行为模式的习惯控制和目标导向控制之间的区别是一致的。习惯是由适当的刺激触发的行为模式，然后或多或少地自动执行。根据心理学家使用这个短语的方式，目标导向行为是有目的性的，因为目标导向行为是由目标的价值以及行为与后果之间的关系所决定的。习惯有时被认为是由先行刺激控制的，而目标导向的行为则被认为是由结果控制的(Dickinson, 1980, 1985)。目标导向控制的优势在于，当环境改变动物对其行为的反应方式时，它能迅速改变动物的行为。当习惯行为对来自于习惯环境的输入做出快速反应时，它却无法快速地适应环境的变化。目标导向的发展

行为控制可能是动物智力进化的一个重大进步。
图14.5展示了假设任务中无模型和基于模型的决策策略之间的区别，在这个假设任务中，老鼠必须在一个有不同目标框的迷宫中行走，每一个目标框都提供相应的大小奖励(图14.5顶部)。从S1开始，老鼠必须首先选择左(L)或右(R)，然后在S2或S3再次选择L或R，以达到目标框之一。目标框是每一集老鼠情景任务的最终状态。无模型策略(图14.5左下)依赖于状态操作对的存储值。这些操作值是对每个(非终端)状态下的每个操作的最高回报的估计。他们是从开始到结束运行迷宫的许多试验中获得的。当行为值已足够好地估计最优回报时，大鼠只需在每个状态中选择动作值最大的动作，以实现最优
 

模范自由 					基于模型的

图14.5:基于模型和无模型的策略，以解决假设的顺序动作选择问题。上图:一只老鼠用不同的目标盒子导航迷宫，每一个盒子都有一个奖励。左下:无模型策略依赖于在许多学习试验中获得的所有状态-动作对的存储操作值。来做决定
rat只需要在每个状态中选择动作值最大的动作。较低的
正确示例:在基于模型的策略中，老鼠学习了一个由知识组成的环境模型
状态-行动-下一个状态的转换和一个由奖励知识组成的奖励模型
与每个不同的目标框相关联。大鼠可以通过使用模型来模拟一系列的动作选择来决定在每个状态下转向的方式，从而找到一条产生最高的路径
回报。改编自认知科学的趋势，第10卷，第8期，y[和合]，D. Joel和P。
《动机的规范性视角》，2006年第376页，获得爱思唯尔的许可。

决策。在这种情况下，当动作值估计值变得足够精确时，大鼠从S1和R中选择L，以获得4的最大返回值。不同的无模型策略可能仅仅依赖于缓存的策略而不是动作值，从S1到L、从S2到r进行直接链接。不需要参考状态转换模型，目标框的特性和它们所提供的回报之间不需要任何联系。
图14.5(右下)演示了一个基于模型的策略。它使用由状态转换模型和奖励模型组成的环境模型。状态转换模型显示为一个决策树，而奖励模型将目标框的不同特征与每个目标框中的奖励联系起来。(与状态S1、S2和S3相关的奖励也是奖励模型的一部分，但这里它们是零，没有显示出来。)基于模型的代理可以通过使用模型来模拟操作选择序列来确定在每个状态下转向的方式，从而找到一条产生最高回报的路径。在这种情况下，返回是在路径的最后得到的结果。在这里，有一个足够精确的模型，大鼠会选择L和R来获得4的奖励。比较模拟路径的预测收益是一种简单的规划形式，可以通过各种方式进行，如第8章所述。
当无模型代理的环境改变了它对代理行为的反应方式时，代理必须在更改的环境中获得新的经验，在此期间它可以更新策略和/或值函数。模范自由策略如图14.5(左下),例如,如果一个目标框以某种方式转向提供不同的奖励,河鼠必须遍历迷宫,可能很多时候,体验新的奖励到达这一目标框,同时更新其政策或其行为价值功能(或两者)在此基础上体验。关键的一点是，对于一个无模型的代理来说，要更改其策略为一个状态指定的动作，或者要更改与一个状态关联的动作值，它必须移动到那个状态，可能多次从那个状态开始操作，并体验其动作的后果。
一个基于模型的代理可以适应其环境的变化，而不需要这种“个人体验”与受变化影响的状态和行为。模型的变更(通过计划)自动地改变它的策略。计划可以确定环境变化的后果，这些变化在代理自己的经验中从来没有联系在一起。例如，再一次提到图14.5的迷宫任务，想象一下，有一个先前学习过的转换和奖励模型的老鼠被直接放在S2的右边的目标框中，以发现现在可用的奖励值是1而不是4。老鼠的奖励模式将会改变，即使在迷宫中寻找目标框所需要的行动选择没有被涉及。规划过程将为迷宫的运行带来新的奖励知识，无需在迷宫中增加额外的经验;在本例中，将策略更改为右转，即S1和S3，以获得3的返回值。
这种逻辑正是动物的结果贬值实验的基础。这些实验的结果提供了动物是否已经学会了一种习惯，或者它的行为是否处于目标导向的控制之下。结果贬值实验就像后学实验，奖励会从一个阶段变化到下一个阶段。后

在学习的最初奖励阶段，结果的奖励值会发生变化，包括变成零，甚至变成负值。
这一类型的早期重要实验是由Adams和Dickinson(1981)进行的。他们通过仪器调节训练大鼠，直到大鼠在训练室内用力按压蔗糖颗粒的杠杆。然后，老鼠被放置在同一个房间里，用操纵杆收回并允许非偶然的食物，这意味着球团可以独立于他们的行动提供给他们。在15分钟的自由接触后，一组老鼠被注射了令人作呕的氯化锂。这一实验重复了三次，在最后一次实验中，注射的老鼠都没有消耗任何非偶发颗粒，这表明小球的奖励价值已经降低——小球被贬值了。在一天后进行的下一阶段中，老鼠再次被放置在实验室内，进行了一段灭种训练，这意味着反应杆已恢复正常，但与颗粒分发器断开连接，这样按压它就不会释放出颗粒。问题是，具有小球奖励值降低的大鼠是否会比没有小球奖励值的大鼠的杠杆作用更小，即使没有由于杠杆作用而导致的奖励值降低。结果显示，从物种灭绝试验开始，注射的老鼠的反应速度明显低于未注射的老鼠。
亚当斯和狄金森得出结论，注射的大鼠通过认知图谱将杠杆按压与小球相联系，而小球则与恶心相关联。因此，在物种灭绝试验中，老鼠“知道”按下杠杆的后果是它们不想要的，因此它们从一开始就减少了杠杆的压力。重要的一点是，他们减少了杠杆的压力，而没有经历过直接的压力，然后是生病:当他们生病的时候没有杠杆。他们似乎能够将行为选择的结果(按下杠杆就会得到一个小球)与结果的回报价值(小球是要避免的)结合起来，因此可以相应地改变他们的行为。并不是每个心理学家都同意这种“认知”的实验解释，它不是解释这些结果的唯一可能的方法，但是基于模型的规划解释被广泛接受。
没有什么可以阻止代理同时使用无模型和基于模型的算法，并且有充分的理由同时使用这两种算法。我们从自己的经验中知道，只要有足够的重复，目标导向的行为就会变成习惯性的行为。实验表明，这种情况也发生在老鼠身上。Adams(1982)进行了一项实验，看看扩展训练是否能将目标导向的行为转化为习惯性行为。他通过比较结果贬值对经历不同程度训练的老鼠的影响来做这个实验。如果与接受较少训练的大鼠相比，长期训练使大鼠对货币贬值更不敏感，这将是长期训练使行为更习惯性的证据。Adams的实验紧跟着Adams和Dickinson(1981)的实验。稍微简化一下，一组老鼠被训练到100个奖励的杠杆，另一组老鼠被训练到500个奖励的杠杆。经过这次训练，小白鼠的球团的奖励值降低(使用氯化锂注射)。

在两组。然后两组大鼠都接受了灭绝训练。亚当斯的问题是，贬值会不会对训练过度的老鼠产生比没有训练过度的老鼠更少的杠杆作用，这将是延长训练减少对结果贬值的敏感性的证据。结果证明，贬值大大降低了未受过过度训练的大鼠的压杠杆率。相反，对于训练过度的老鼠来说，贬值对它们的杠杆作用不大;事实上，如果说有什么不同的话，那就是它更有活力。(完整的实验包括对照组，显示不同程度的训练本身并没有显著影响学习后的杠杆率。)这一结果表明，尽管未受过过度训练的大鼠对自己行为的结果非常敏感，但训练过度的大鼠却养成了一种施加压力的习惯。
从计算的角度来看这个和其他类似的结果，就可以理解为什么人们可能会期望动物在某些情况下习惯性地行动，在其他情况下以目标导向的方式行动，以及为什么它们在继续学习时从一种控制模式转向另一种控制模式。毫无疑问，动物使用的算法与我们在本书中展示的算法并不完全匹配，但人们可以通过考虑各种强化学习算法所暗示的权衡来深入了解动物的行为。计算神经学家Daw、Niv和Dayan(2005)提出了一个想法，即动物使用无模型和基于模型的过程。每个过程都提出一个动作，而选择执行的动作是由被认为是两个过程中更值得信赖的过程提出的，这是由贯穿整个学习过程的信心度量所决定的。早期学习基于模型的系统的计划过程更加值得信赖，因为它将短期的预测联系在一起，而这些短期的预测比没有模型的过程的长期预测更准确。但是，随着经验的不断积累，无模型的过程变得更加值得信赖，因为规划很容易出错，因为模型的不准确性和使规划可行所必需的捷径，例如各种形式的“树修剪”:删除无前景的搜索树分支。根据这个观点，随着经验的积累，人们会期望从目标导向的行为转向习惯行为。关于动物如何在目标导向和习惯性控制之间进行仲裁，人们提出了其他的观点，行为和神经科学研究都在继续研究这个问题和相关的问题。
无模型和基于模型的算法之间的区别被证明对这项研究是有用的。我们可以在抽象设置中检查这些类型的算法的计算含义，以揭示每种类型的基本优点和局限性。这既可以提出建议，也可以使问题变得更加尖锐，这些问题可以指导实验的设计，以增加心理学家对习惯性和目标导向行为控制的理解。


\section{总结}

本章的目的是探讨强化学习与动物学习心理学实验研究之间的对应关系。我们在一开始就强调，本书所描述的强化学习并不是有意的为动物行为的细节建模。它是一个抽象的计算框架，从人工智能和工程的角度探索理想化的情况。但是，许多基本的强化学习算法都是受到心理学理论的启发，在某些情况下，这些算法促进了新的动物学习模式的发展。这一章描述了这些通信中最明显的一个。

预测算法和控制算法之间的强化学习的区别与动物学习理论在经典条件作用和工具条件作用之间的区别类似。工具性和经典条件作用实验的关键区别在于，前者的强化刺激取决于动物的行为，而后者则不然。通过TD算法学习预测与经典条件作用相对应，我们将经典条件作用的TD模型描述为一个例子，强化学习原理解释了动物学习行为的一些细节。这个模型概括了有影响的Rescorla-Wagner模型，它包含了在个体试验中事件影响学习的时间维度，并且它提供了二阶条件作用的解释，在二阶条件作用中，强化刺激的预测因子会自我强化。这也是对大脑中多巴胺神经元活性有影响的观点的基础，这是我们在第15章中所讨论的。
试错学习是强化学习控制的基础。我们在这里和第1章(第15页)中讨论了索恩代克用猫和其他动物做实验的一些细节。指出在强化学习中，探索并不局限于“盲目摸索”;只要有一些探索，试验就可以通过使用先天和以前学到的知识的复杂方法产生。我们讨论了训练方法b·f·斯金纳(B. F. Skinner)所称的“塑形”，在这个过程中，奖励或突发事件被逐步改变，以训练一种动物连续地接近所期望的行为。塑形不仅是动物训练中不可缺少的，也是训练强化学习因子的有效工具。这也与动物的动机状态有关，它会影响动物接近或避免什么，以及什么事件会对动物产生奖励或惩罚。
本书介绍的强化学习算法包括两种基本机制来解决延迟强化问题:通过TD算法学习的资格跟踪和价值函数。这两种机制在动物学习理论中都有先例。资格追溯类似于早期理论的刺激痕迹，价值函数对应于次要强化作用，提供几乎即时的评价反馈。
下一章的内容是在强化学习的环境模型和心理学家所说的认知地图之间。20世纪中期进行的实验旨在展示动物学习认知地图的能力，将其作为国家行动协会的替代品，或作为补充，然后利用它们指导行为，特别是当环境发生意外变化时。强化学习中的环境模型就像认知地图，它们可以通过监督学习方法学习而不依赖奖励信号，然后它们就可以

后来用于计划行为。
强化学习在无模型和基于模型的算法之间的区别与习惯性和目标导向行为之间的心理学区别。无模型算法通过访问存储在策略或动作值函数中的信息来做出决策，而基于模型的方法则通过使用代理环境的模型提前规划来选择操作。结果-贬值实验提供了动物行为是习惯性还是被目标控制的信息。强化学习理论有助于澄清对这些问题的思考。
动物学习清楚地告知强化学习，但作为一种机器学习，强化学习的目的是设计和理解有效的学习算法，而不是复制或解释动物行为的细节。我们专注于动物学习相关方面的明确方法解决预测和控制问题的方法,强调了丰硕的强化学习和心理学之间的双向流动的思想没有冒险深入的许多行为细节和争议占据了动物学习研究人员的注意。强化学习理论和算法的未来发展很可能会利用与动物学习的许多其他特性的联系，因为这些特性的计算效用会得到更好的理解。我们期望在强化学习和心理学之间的思想交流将继续为这两个学科带来成果。
强化学习与心理学和其他行为科学领域之间的许多联系超出了本章的范围。我们在很大程度上讨论了与决策心理学的联系，决策心理学关注的是在学习之后如何选择行动或如何做出决策。我们也不讨论生态学家和行为生态学家所研究的行为的生态和进化方面的联系:动物如何相互联系和它们的物理环境，以及它们的行为如何有助于进化适应。优化、MDPs和动态编程在这些领域中占据着重要地位，我们强调与动态环境的代理交互，这与研究复杂的“生态论”中的代理行为有关。多主体强化学习，在这本书中被省略了，与行为的社会方面有联系。尽管缺乏治疗，强化学习绝不应该被解释为忽视进化的观点。强化学习并不意味着学习和行为有一张白板。事实上，工程应用的经验强调了构建强化学习系统知识的重要性，这与进化为动物提供的知识类似。

\section{书目的和历史的言论}

Ludvig, Bellemare和Pearson(2011)和Shah(2012)回顾了心理学和神经科学语境下的强化学习。这些出版物是本章和下一章关于强化学习和神经科学的有用的伙伴。

14.1 Dayan, Niv, Seymour和Daw(2006)专注于clas-之间的交互
物理条件作用和工具条件作用，特别是当分类条件作用和工具反应发生冲突时。他们提出了一个Q-learning框架来建模这种交互的各个方面。Modayil和Sutton(2014)利用一个移动机器人展示了将固定响应与在线预测学习相结合的控制方法的有效性。他们把这种方法称为“pavlovian控制”，强调它与通常的强化学习控制方法不同，这种方法是建立在正确执行固定反应的基础上，而不是基于奖励最大化。罗斯(1933)的机电机器，特别是沃尔特的海龟(沃尔特，1951年)的学习版本是巴甫洛夫控制的早期插图。

14.2.1 Kamin(1968)首先报道了阻塞，现在通常被称为Kamin阻塞，
在经典条件作用。Moore和Schmajuk(2008)对阻塞现象、它所刺激的研究及其对动物学习理论的持久影响进行了很好的总结。Gibbs, Cool, Land, Kehoe和Gormezano(1991)描述了兔子的瞬态膜反应的二级条件反射，以及它与连续复合刺激作用的关系。芬奇(Finch and Culler)(1934)报告说，“当动物的动机通过不同的指令得到维持时”，狗的前肢退缩得到了五阶条件反射。

14.2.2 Rescorla-Wagner模型的想法是，学习发生在动物身上
《惊奇》改编自Kamin(1969)。除了Rescorla和Wagner之外的古典条件作用模型包括Klopf(1988)、Grossberg(1975)、Mackintosh(1975)、Moore and Stickney(1980)、Pearce and Hall(1980)、Courville、Daw和Touretzky(2006)。Schmajuk(2008)回顾经典条件作用模型。Wagner(2008)对Rescorla-Wagner模型和类似的学习基本理论提供了现代心理学视角。

14.2.3经典条件作用的TD模型的早期版本出现在萨顿和
Barto (1981a)，也包括了早期模型的预测，即时间主导压倒了阻塞，后来Kehoe、Schreurs和Graham(1987)等人证明了这一点，即在兔裂膜制备过程中发生。萨顿和巴托(1981a)对Rescorla - Wagner模型和最小均方(LMS)或Widrow-Hoff，学习规则(Widrow和Hoff, 1960)之间的近同一性进行了最早的认识。早期的模型在Sutton的TD算法(Sutton, 1984, 1988)之后进行了修订，并在Sutton和Barto(1987)中首次提出TD模型，在Sutton和Barto(1990)中更全面，这部分主要基于此部分。额外的探索

TD模型及其可能的神经实现由Moore和他的同事(Moore, Desmond, bertier, Blazis, Sutton, and Barto, 1986;摩尔和Blazis,1989;Moore, Choi, and Brunzell, 1998;摩尔，马克斯，卡斯塔尼亚，波勒万，2001)。Klopf(1988)经典条件作用的驱动强化理论扩展了TD模型以解决额外的实验细节，如采集曲线的s型。在这些出版物中，TD被认为是时间导数，而不是时间差异。

14.2.4 Ludvig、Sutton和Kehoe(2012)对TD模型的性能进行了评价
在先前未探索过的涉及古典条件作用的任务中，研究了各种刺激表现形式的影响，包括他们早先引入的微刺激表现形式(Ludvig, Sutton, and Kehoe, 2008)。在TD模型的背景下，关于各种刺激表示的影响及其可能的神经实现对响应时间和地形的影响的早期研究是摩尔和他的同事在上面提到的。虽然不是在TD模型的背景下，但是像Ludvig等人(2012)的微刺激表征已经被Grossberg和Schmajuk(1989)、Brown、Bullock和Grossberg(1999)、Buhusi和Schmajuk(1999)以及Machado(1997)提出和研究。第353-355页的数字改编自萨顿和巴托(1990)。

14.3第1.7节包括关于试错学习历史的评论
的法律效果。索恩代克的猫可能是根据直觉的特定环境而不是行动的顺序来探索的，而不是仅仅从一组本能的冲动中选择。Selfridge, Sutton, and Barto(1985)展示了在杆平衡强化学习任务中塑造的有效性。在强化学习的其他例子塑造Gullapalli和Barto(1992),马哈和康奈尔大学(1992),马塔里奇(1994),民宿和Colombette(1994),Saksida,雷蒙德,和Touretzky(1997),和Randløv Alstrøm(1998)。Ng(2003)、Ng、Harada和Russell(1999)在某种意义上使用了与Skinner不同的“塑造”一词，关注的是如何在不改变最优策略的情况下改变奖励信号的问题。
Dickinson和Balleine(2002)讨论了学习和动机之间相互作用的复杂性。Wise(2004)概述了强化学习及其与动机的关系。Daw和Shohamy(2008)将动机和学习联系到强化学习理论的各个方面。参见《麦克卢尔》、《杜》、《蒙太古》(2003)、《和合》、《乔尔》、《大安》(2006)、《兰格尔》、《卡默尔》、《蒙太古》(2008)、《大安》、《贝里奇》(2014)。McClure等人(2003)，Niv, Daw, Daw, Dayan (2006)， and Niv, Daw, Joel, Dayan(2007)提出了与强化学习框架相关的行为活力理论。

14.4斯宾塞，赫尔的学生和耶鲁大学的合作者，阐述了更高的角色。
在解决延迟强化问题(Spence, 1947)的问题上加强了秩序。学习很长时间的延迟，如在厌恶的条件下延迟数小时，导致干涉理论取代衰减跟踪理论(例如，Revusky和Garcia, 1970;伯克和科斯塔,2014)。在延迟强化下学习的其他观点引用了意识和工作记忆的角色(例如Clark和Squire, 1998;Seo, Barraclough，和Lee, 2007)。

14.5 Thistlethwaite(1951)对潜在学习实验进行了广泛的回顾
直到它出版的时候。Ljung(1998)提供了模型学习的概述，或系统识别，工程技术。Gopnik, Glymour, Sobel, Schulz, Kushnir和Danks(2004)提出了一个关于儿童如何学习模型的贝叶斯理论。

14.6习惯行为和目标导向行为以及无模型行为之间的联系
基于模型的强化学习最早由Daw、Niv和Dayan(2005)提出。用来解释习惯性和目标导向行为控制的假设迷宫任务是基于对Niv, Joel，和Dayan(2006)的解释。Dolan和Dayan(2013)回顾了与这个问题相关的四代实验研究，并讨论了如何在强化学习无模型/基于模型的区别的基础上继续前进。Dickinson(1980, 1985)和Dick-inson和Balleine(2002)讨论了与这一区别有关的实验证据。Donahoe和Burgos(2000)也认为，无模型过程可以解释收益-贬值实验的结果。Dayan和Berridge(2014)认为经典条件反射涉及基于模型的过程。Rangel、Camerer和Montague(2008)回顾了许多未解决的问题，包括习惯性、目标导向和Pavlovian控制模式。

评论心理学术语——传统意义上的钢筋的强化的行为模式(通过增加其强度或频率)由于动物接收刺激(或体验刺激的遗漏)在一个合适的时间与另一个刺激和反应之间的关系。强化产生的变化留在未来的行为中。在心理学中，强化有时是指在行为上产生持久变化的过程，无论这些变化是加强还是削弱了一种行为模式(Mackintosh, 1983)。强化指的是强化之外的弱化，这与强化的日常意义，以及它在心理学中的传统用法不一致，但它是我们在这里采用的一个有用的扩展。在任何一种情况下，被认为是行为改变的原因的刺激被称为强化物。
心理学家通常不像我们一样使用特定的短语强化学习。动物学习先驱者可能把强化和学习视为同义词，因此使用这两个词是多余的。我们在计算和工程研究中使用这个短语，主要受明斯基(Minsky, 1961)的影响。但这个短语最近在心理学和神经科学中越来越流行，很可能是因为强化学习算法和动物学习算法之间出现了强烈的相似之处——这一章和下一章中所描述的相似之处。
根据通常的用法，奖励是动物接近和工作的对象或事件。动物因其“好”而受到奖励

为了使动物的行为更好而给予的行为。类似地，惩罚是动物通常避免的对象或事件，是“不良”行为的结果，通常是为了改变这种行为。主要奖励奖励是由于机器内置一个动物神经系统的进化,以改善其生存和繁殖的机会,例如,奖励由营养食物的味道,性接触、成功逃脱,和许多其他的刺激和事件,预测在动物繁殖成功率的祖先历史。如第14.2.1节所解释的，高阶奖励是通过预测初级奖励的刺激物提供的奖励，直接或间接地通过预测其他预测初级奖励的刺激物。如果奖励质量是直接预测主要奖励的结果，那么奖励是次要的。
在这本书中，我们称Rt为“t时刻的奖励信号”，有时仅仅是“t时刻的奖励信号”，但我们不认为它是agent环境中的对象或事件。因为Rt是一个数字——不是物体或事件——它更像是神经科学中的奖赏信号，它是大脑内部的信号，就像神经元的活动一样，影响决策和学习。当动物感知到一个有吸引力的(或令人厌恶的)物体时，这个信号可能会被触发，但它也可能被一些在动物外部环境中不存在的东西触发，比如记忆、想法或幻觉。因为我们的Rt可以是正的，负的，或零的，所以我们最好把负的Rt叫做惩罚，Rt等于零是一个中性信号，但为了简单起见，我们通常避免这些项。
在强化学习中，生成所有Rts的过程定义了代理试图解决的问题。代理人的目标是在一段时间内尽可能地保持Rt的大小。Rt是在这方面,主要奖励一个动物,如果我们认为动物面临的问题的问题在其生命周期中获得尽可能多的主要奖励(,因此,通过进化的未来的“智慧”,提高解决实际问题的机会,这是其基因传递给后代)。然而，正如我们在第15章中所指出的，在动物的大脑中不太可能存在一个像Rt这样的“主人”奖励信号。
不是所有的强化物都是奖励或惩罚。有时，强化不是动物接受刺激的结果，刺激是通过给行为贴上好或坏的标签来评估其行为。一种行为模式可以通过刺激得到强化，无论动物的行为如何。如第14.1节所述，强化物的提供是否取决于，或不取决于，前一行为是工具的，或操作的，条件作用实验和经典的，或巴甫洛夫的，条件作用实验的决定性区别。在这两种实验中，强化都是有效的，但只有前者是对过去行为进行评估的反馈。(尽管它经常被指出,即使在加强我们在经典条件反射实验中不是取决于主体的前行为,强化价值可以通过这种行为,影响的一个例子是,一个封闭的眼睛使眼睛不那么厌恶空气膨胀。)
当我们在下一章讨论这些信号的神经关联时，奖励信号和强化信号之间的区别是一个关键点。就像奖励信号，对我们来说，任何特定时间的强化信号都是正数或负数，或零。强化信号是指导学习算法变化的主要因素

在代理的策略、价值评估或环境模型中生成。对我们最有意义的定义是，增强信号在任何时候都是一个数字，它乘以一个向量(可能还有一些常数)来确定某些学习算法中的参数更新。
对于某些算法来说，奖励信号本身就是参数更新方程的关键乘法器。对于这些算法，增强信号与奖励信号相同。但对大多数的算法我们讨论在本书中,强化信号包括条款除了奖励信号,一个例子是TD错误δt = Rt + 1 +γV(圣+ 1)−V(St),即强化信号道明州值学习(和类似的TD错误行为价值学习)。在这种强化信号,Rt + 1是主要的强化的贡献,和颞预测值的差异,γV(圣+ 1)−V(St)(或类似的行动时间差异值),条件是钢筋的贡献。因此,每当γV(圣+ 1)−V(St)= 0,δt信号“纯”主要强化;当Rt+1 = 0时，它表示“纯”条件强化，但它通常表示这些的混合。注意我们6.1节中提到的,这个δt才可用时间t + 1。因此我们认为δt强化信号在时间t + 1,这是合适的,因为它增强了预测和/或行为使t前一步。
一个可能的混淆来源是著名心理学家b·f·斯金纳及其追随者使用的术语。对于斯金纳来说，当动物行为的结果增加了这种行为的频率时，就会产生积极的强化作用;当行为的后果降低了行为的频率时，惩罚就会发生。当行为导致厌恶刺激(即动物不喜欢的刺激)被移除时，负面强化就会发生，从而增加这种行为的频率。另一方面，当行为导致食欲刺激(即动物喜欢的刺激)消失时，就会出现消极惩罚，从而降低这种行为的频率。我们发现没有必要对这些区别进行严格的区分，因为我们的方法比这更抽象，奖励和强化信号都允许同时具有积极和消极的价值。(但特别要注意，当我们的强化信号为负时，它与斯金纳的负强化并不相同。)
另一方面，人们也经常指出，仅仅依靠一个数字作为奖励或惩罚信号，这与动物的食欲和厌恶系统具有不同的性质和涉及不同的大脑机制这一事实是不一致的。这指向了一个方向，在这个方向上，未来的强化学习框架可能会被开发出来，以利用不同的欲望和厌恶系统的计算优势，但现在我们正在传递这些可能性。
另一个术语上的差异是我们如何使用行动这个词。对于许多认知科学家来说，一种行为是有目的的，因为它是动物对问题行为和行为后果之间关系的认知的结果。行动是目标导向的，是决定的结果，而不是由刺激引发的反应;反射或习惯的结果。我们使用“行动”这个词，而不区分其他人所谓的“行动”、“决定”和“反应”。这些是重要的区别，但对我们来说，它们是由不同的区别所包围的

无模型和基于模型的强化学习算法，我们在上面的14.6节中讨论了它们与习惯行为和目标导向行为的关系。Dickinson(1985)讨论了反应和行动之间的区别。
这本书中经常用到的一个词是“控制”。我们所说的控制与动物学习心理学家所说的完全不同。所谓控制，我们指的是代理影响其环境，从而导致代理喜欢的状态或事件:代理对其环境施加控制。这是控制工程师使用的控制感。另一方面，在心理学中，控制通常意味着动物的行为受到动物受到的刺激(刺激控制)或它所经历的强化计划的控制。在这里，环境控制代理。在这种意义上的控制是行为矫正治疗的基础。当然，当代理与环境交互时，这两个控制方向都起作用，但是我们的关注点是代理作为控制器;不是环境作为控制器。与我们的观点相对应，也许更有启发性的是，代理实际上控制了从其环境接收的输入(Powers, 1973)。这不是心理学家所说的刺激控制。
有时，强化学习被理解为仅仅从奖励(和惩罚)中直接学习策略，而不涉及价值函数或环境模型。这就是心理学家所说的刺激-反应或S-R学习。但对我们来说，和今天的大多数心理学家一样，强化学习远不止于此，除了S-R学习之外，还包括价值函数、环境模型、计划和其他通常被认为属于心理功能认知方面的方法。

