\chapter{第四章 动态规划}
\begin{summary}

术语动态规划(DP)指的是一组算法，它们可以用来计算最优策略，给出了一个完美的环境模型，作为Markov决策过程(MDP)。传统的DP算法在强化学习中的效用是有限的，这一方面是因为它们假设了一个完美的模型，另一方面是因为它们的计算开销很大，但它们在理论上仍然很重要。DP为理解本书其余部分的方法提供了基本的基础。事实上，所有这些方法都可以被看作是试图达到与DP几乎相同的效果，只需要较少的计算和不假设环境的完美模型。
我们通常假设环境是一个有限的MDP。也就是说，我们假设它的状态，作用和奖赏集S A R是有限的，它的动力学是由一组概率p(S ?,r | s)∈年代,∈(s),r∈r和s ?∈S +(S + S +终端状态如果问题情景)。尽管DP思想可以应用于具有连续状态和操作空间的问题，但只有在特殊情况下才可能有精确的解决方案。获得具有连续状态和操作的任务的近似解的一种常见方法是量化状态和操作空间，然后应用有限状态DP方法。我们在第9章中探讨的方法适用于连续问题，是该方法的重要扩展。
DP和强化学习的关键思想是利用价值函数来组织和组织寻找好的策略。在本章中，我们将展示如何使用DP来计算第3章定义的值函数。如前所述,我们可以很容易地获得最优政策一旦我们找到了最优值函数,v∗或者q∗,满足贝尔曼最优方程:
v∗(s)= max
一个
E(Rt + 1 +γv∗(圣+ 1)|圣=,=)

=最大
一个

r s ?,
p(s ?r | s,)

r +γv∗(?)

,或 					(4.1)

问∗(,)= E

Rt + 1 +γmax
一个?
问∗(圣+ 1 ?)
吗?吗?吗?圣=,=

=

r s ?,
p(s ?r | s,)

r +γmax
一个?
问∗(s ?,一个?)

, 					(4.2)


∈年代,∈(s),和s ?∈S +。正如我们将看到的，DP算法是通过将诸如此类的Bellman方程转换为赋值，也就是说，转换为更新规则以改进期望值函数的近似。
\end{summary}

\section{政策评估(预测)}
首先,我们考虑如何计算一个任意的州值函数vπ政策π。这在DP文献中被称为政策评估。我们也把它称为预测问题。从第三章,∈年代,

vπ(年代)。= Eπ[Gt |圣= s]
= Eπ[Rt + 1 +γGt + 1 |圣= s] 					(从(3.9))
= Eπ[Rt + 1 +γvπ(圣+ 1)|圣= s] 					(4.3)

=

一个
π(|)?
r s ?,
p(s ?r | s,)

r +γvπ(?)

, 					(4.4)


在π(|)的概率是采取行动在国家年代政策π,和期望是下标在ππ来表示他们是有条件的被跟踪。的存在唯一性vπ保证只要γ< 1或最终终止与所有国家在政策保证π。
如果环境的动力是完全已知的,那么(4.4)是一个系统的| | |年代线性方程组|未知(的vπ(S)∈年代),原则上,它的解决方案是一种简单,如果繁琐,计算。对于我们的目的，迭代解决方法是最合适的。考虑一个近似函数序列v0, v1, v2…，每个映射S+到R(实数)。初始近似,v0,任意选择(除了终端状态,如果有的话,必须给定值0),并且每个逐次逼近获得通过使用vπ贝尔曼方程(4.4)作为一个更新规则:

vk + 1(s)。= Eπ[Rt + 1 +γvk(圣+ 1)|圣= s]
=

一个
π(|)?
r s ?,
p(s ?r | s,)

r +γvk(?)

, 					(4.5)
所有年代∈s .显然,vk = vπ定点这个更新规则,因为贝尔曼方程vπ保证我们平等的在这种情况下。事实上,序列{ vk }可以显示一般收敛于vπk→∞在相同条件下保证vπ的存在。这种算法称为迭代策略评估。
生产每一个逐次逼近,从vk vk + 1,迭代政策评估相同的操作适用于每个州史:它取代年代的旧值与一个新值从旧值获得的继任者的年代,和预期的即时回报,以及所有可能一步过渡政策的评估。我们将这种操作称为预期更新。迭代策略评估的每次迭代更新每个状态的值一次，生成新的近似值函数

vk + 1。有几种不同类型的预期更新，这取决于一个状态(如这里)或状态-操作对是否正在更新，并且取决于后续状态的估计值的精确方式。DP算法中完成的所有更新都被称为预期更新，因为它们基于对所有可能的下一个状态的期望，而不是基于一个示例下一个状态。更新的性质可以用公式表示，如上面所示，也可以用备份图表示，如第3章所介绍的。例如，与迭代策略评估中使用的预期更新相对应的备份图表显示在第59页。
要编写一个连续的计算机程序来实现(4.5)所给出的迭代策略评估，您必须使用两个数组，一个用于旧值，一个用于新值，一个用于新的值，vk+1(s)。使用两个数组，可以从旧值中逐个计算新值，而不需要更改旧值。当然，更容易使用一个数组并更新值“到位”，也就是说，每个新值立即覆盖旧值。然后，根据状态更新的顺序，有时在(4.5)的右边使用新值而不是旧值。这也就地算法收敛于vπ;实际上，它的收敛速度通常比双数组版本要快，正如您所期望的那样，因为它在新数据可用时就会使用它们。我们认为更新是在状态空间中执行的。对于就地算法，状态值在扫描过程中更新的顺序对收敛速度有显著影响。当我们想到DP算法时，我们通常会想到合适的版本。
在下面的框中，一个完整的就地版本的迭代策略评估显示在伪代码中。注意它是如何处理终止的。在形式上，迭代策略评估只在极限上收敛，但在实践中，它必须在极限下停止。伪代码测试数量马克斯∈年代| vk + 1(S)−vk(S)|每次扫描和停止时足够小。


示例4.1考虑如下所示的4×4 gridworld。

					r =
Rt	1 1
−1
	8	9	10	11		
行动	12	13	14			

非终端状态为S ={1,2，…14 }。在每个状态中都可能有4个动作，A = {up, down, right, left}，它们决定导致相应的状态转换，除了那些将代理从网格中移除的动作实际上保持状态不变。因此,例如,p(6−1 | 5右)= 1,p(7−1 | 7右)= 1,p(10 r | 5右)= 0 r∈r .这是一个尚未完全,情景任务。奖励是−1在所有转换到终端状态。终端状态在图中被阴影化(虽然它在两个地方显示，但它在形式上是一个状态)。因此预期回报函数r(s,s)=−1对所有国家s,s ?假设代理遵循等概率随机策略(所有行为的概率相等)。图4.1的左侧显示了由迭代策略评估计算的值函数{vk}的序列。最后估计实际上是vπ,在这种情况下,给每个州的否定预期数量的步骤的状态,直到终止。
练习4.1在例4.1中,如果π是等概率的随机策略,什么是qπ(11日)?
什么是qπ(7)? 					?
在示例4.1中，假设一个新的状态15被添加到状态13下面的gridworld中，它的操作，左，右，右，向下，分别将代理发送到状态12、13、14和15。假设来自原始状态的转换没有改变。那么,什么是vπ(15)等概率的随机的政策?现在假设状态13的动力学也发生了变化，比如从状态13向下的动作将代理带到新的状态15。什么是vπ(15)为等概率的随机政策在这种情况下??
4.3运动的方程是什么类似于(4.3),(4.4),(4.5)和行为价值函数qπ及其函数序列q0逐次逼近,q1、q2,。。。?

\section{政策改进}

我们计算策略的值函数的原因是为了帮助找到更好的策略。假设我们有确定的值函数vππ任意确定的政策。对于一些国家年代我们想知道我们是否应该改变政策以确定性选择一个行动? =π(年代)。我们都知道它是多么好未嫁的现行政策是vπ(s),但会更好或更糟改变到新的政策?回答这个问题的一种方法是考虑在s和之后选择a

Vk的
随机策略f r t
该政策
v
 


-1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0
k = 1
0.0



-1.7 -2.0 -2.0 -1.7 -2.0 -2.0 -2.0 -2.0 -2.0 -2.0 -1.7 -2.0 -2.0 -1.7
k = 2
0.0




0.0




-2.4 -2.9 -3.0 -2.4 -2.9 -3.0 -2.9 -2.9 -3.0 -2.9 -2.4 -3.0 -2.9 -2.4
k = 3
0.0


0.0

-6.1 -8.4 -9.0 -6.1 -7.7 -8.4 -8.4 -8.4 -8.4 -7.7 -6.1 -9.0 -8.4 -6.1
k = 10
0.0

0.0

-14年。-20年。-22年。-14年。-18年。-20年。-20年。-20年。-20年。-18年。-14年。-22年。-20年。-14年。
k =
0.0

0.0
77年

贪心政策
g p李
   。v

随机的政策

图4.1:小网格环境下迭代策略评估的收敛性。左列
随机策略的状态-值函数的近似序列(所有操作的可能性相等)。右列是与值函数估计值相对应的贪婪策略序列(箭头表示达到最大值的所有操作，而数字则四舍五入为两位数)。最后的政策只保证是一个
对随机策略的改进，但是在这种情况下，以及第三次迭代之后的所有策略，
是最优的。


现有政策后,π。这种行为方式的价值是。

qπ(年代)。= E[Rt + 1 +γvπ(圣+ 1)|圣=,=) 					(4.6)

=

r s ?,
p(s ?r | s,)

r +γvπ(?)

.电

标准的关键是是否大于或小于vπ(年代)。如果是大的,最好是选择一个在年代,此后跟随比它会遵循ππ时间一个期望它会更好的选择每次遇到年代,事实上,新政策将是一个更好的一个整体。
这是一个被称为政策改进定理的一般结果的特例。让ππ?是任何对确定性的政策,对于所有∈年代,

qπ(年代,π?(s))≥vπ(年代)。 					(4.7)
那么政策π?必须一样好,或者比,π。也就是说,它必须获得大于或等于预期收益从所有国家∈年代:

vπ?(s)≥vπ(年代)。 					(4.8)
而且，如果在任何状态下都有严格的(4.7)不等式，那么在那个状态下一定有严格的(4.8)不等式。这个结果尤其适用于这两个政策,我们认为在前款规定的,最初确定的政策,π,以及政策的改变,π吗?π是相同的,除了π?(s)= a ? =π(年代)。显然,(4.7)在以外的所有国家。因此,如果qπ(,)> vπ(s),那么改变政策确实比π。
政策改进定理证明背后的思想很容易理解。从(4.7),我们不断扩大qπ端(4.6)和(4.7),直到我们上重新得到vπ?(s):

vπ(s)≤qπ(年代,π?(s))
= E[Rt + 1 +γvπ(圣+ 1)|圣=,=π?(s)] 					(通过(4.6))
= Eπ?(Rt + 1 +γvπ(圣+ 1)|圣= s]
≤Eπ吗?(Rt + 1 +γqπ(圣+ 1,π?(圣+ 1)|圣= s] 					(通过(4.7))
= Eπ?(Rt + 1 +γEπ?(Rt + 2 +γvπ(圣+ 2)|圣+ 1 + 1 =π?(圣+ 1)]|圣= s]= Eπ吗?

Rt + 1 +γRt + 2 +γ2vπ(圣+ 2)
吗?吗?圣=年代

≤Eπ吗?

Rt + 1 +γRt + 2 +γ2Rt + 3 +γ3vπ(圣+ 3)?吗?圣= s ?
.电

≤Eπ吗?

Rt + 1 +γRt + 2 +γ2Rt + 3 +γ3Rt + 4 +···
吗?吗?圣=年代

= vπ?(年代)。
到目前为止，我们已经看到，在给定策略及其价值函数的情况下，如何轻松地评估单个状态下的策略更改为特定的操作。这是一个自然的延伸

考虑改变在所有国家和所有可能的行动,选择在每个州的行动似乎最好根据qπ(年代)。换句话说,考虑新的贪婪的政策,π吗?,由

π?(s)。= argmax
一个
qπ(年代)

= argmax
一个
E(Rt + 1 +γvπ(圣+ 1)|圣=,=) 					(4.9)

= argmax
一个

r s ?,
p(s ?r | s,)

r +γvπ(?)

其中argmaxa表示a的值，在这个值下，后面的表达式最大化(带任意断开的连接)。贪婪的政策的行动看起来最好在短任期时的lookahead-according vπ一步。通过构造，贪婪策略满足政策改进定理(4.7)的条件，因此我们知道它与原始策略一样好，或者比原始策略更好。在原有政策的基础上制定新政策，使其对原有政策的价值功能产生贪欲，这一过程称为政策改进。
假设新的贪婪的政策,π吗?一样好,但并不比,老政策π。然后vπ= vπ?,从(4.9),对所有∈年代:
vπ?(s)= max
一个
E(Rt + 1 +γvπ?(圣+ 1)|圣=,=)

=最大
一个

r s ?,
p(s ?r | s,)

r +γvπ?(?)

.电

但这是一样的贝尔曼最优性方程(4.1),因此,vπ吗?必须v∗,ππ?必须优化政策。因此，除了最初的政策已经是最优的时候之外，政策改进必须给我们一个严格更好的政策。
到目前为止，在这一节中，我们已经讨论了确定性策略的特殊情况。在一般情况下,一个随机策略π指定概率,π(|),在每一个行动,一个,在每个州,s。我们不会经历的细节,但事实上所有的思想这部分很容易扩展到随机的政策。特别地，政策改进定理在随机情况下进行。此外，如果在政策改进步骤(4.9)中有联系——也就是说，如果有几个动作达到了最大值——那么在随机情况下，我们不需要从中选择一个动作。相反，每个最大化操作都可以得到在新的贪婪策略中被选择的概率的一部分。任何分配方案都是允许的，只要所有的次最大值动作都是零概率的。
图4.1的最后一行显示了随机策略的策略改进示例。π,这里原来的政策是等概率的随机的政策,新政策,π吗?关于vπ贪婪。价值函数vπ左下图所示的设置可能的π?在右下角的图表中显示。美国有多个箭头在π吗?图是指几个动作在(4.9)中达到最大值的动作;这些行为之间的概率分配是允许的。任何此类政策的价值函数,vπ?(s),可以被检验是−1−2,或者−3州,∈年代,而vπ(s)是最多−14。因此,vπ?(s)≥vπ(年代)

∈年代,说明政策改进。尽管在这种情况下,新政策π吗?恰好是最优的，一般来说只有改进是有保证的。


\section{政策迭代}
π,一旦一个策略,改进了使用vπ产生更好的政策,π吗?我们可以计算vπ呢?和改善再取得一个更好的π? ?。因此，我们可以得到一系列单调改进的政策和价值函数:


π0
子邮件
−→vπ0
我
−→π1
子邮件
−→vπ1
我
−→π2
子邮件
−→···
我
−→π∗
子邮件
−→v∗

在E−→代表一个政策评估和我−→代表一个政策改进。每个策略都保证比前一个策略有严格的改进(除非它已经是最优的)。由于有限的MDP只有有限数量的策略，因此这个过程必须在有限的迭代中收敛到最优策略和最优值函数。
这种寻找最优策略的方法称为策略迭代。完整的算法在下面的框中给出。注意，每个策略评估(本身就是一个迭代计算)都是从前一个策略的值函数开始的。这通常会导致政策评估的收敛速度大幅提高(大概是因为从一个政策到下一个政策的价值函数变化不大)。

例4.2:杰克的汽车租赁杰克为一家全国性的汽车租赁公司管理两个地点。每天都有一定数量的顾客到每个地方租车。如果杰克有一辆车的话，他就把车租出去，并被国家公司记入10美元。如果他在那个地方没有车，那生意就完了。汽车在归还后的第二天就可以出租了。为了确保车辆可以在需要的地方使用，Jack可以在两个地点之间过夜，每辆车的费用是2美元。我们假设汽车的数量要求在每个位置并返回泊松随机变量,也就是说,的概率数字n =λ
N
n !e
−λ
,λ是预期的数量。假设λ3和4租赁请求
第一个和第二个位置，返回3和2。略来简化问题,我们假设可以有不超过20辆汽车在每个位置(全国任何额外的车回到了公司,因此从问题消失),最高5辆车可以从一个位置移动到另一个晚上。我们把贴现率γ= 0.9,制定持续有限MDP,几天的时间步骤,国家汽车的数量在每个位置在一天结束的时候,和操作的两个地点之间的净数量的汽车移动过夜。图4.2显示了从从不移动任何汽车的策略开始的策略迭代发现的策略序列。


图4.2:策略迭代对Jack的汽车租赁问题发现的策略序列，以及最终的状态值函数。前五幅图显示，对于一天结束时每个地点的汽车数量，将从第一个位置移到第二个位置的汽车数量(负数表示从第二个位置转到第一个位置)。每一个
连续政策是对以前政策的严格改进，最后一个政策是最优的。

策略迭代常常以令人惊讶的很少的迭代收敛，例如杰克的汽车租赁的例子，并且正如图4.1中的例子所示。图4.1左下图显示了等概率随机策略的值函数，右下图显示了该值函数的贪心策略。政策改进定理告诉我们，这些政策优于原来的随机政策。然而，在这种情况下，这些策略不仅是更好的，而且是最优的，在最小的步骤中向终端状态前进。在本例中，策略迭代将在一次迭代之后找到最优策略。
第80页的策略迭代算法有一个微妙的缺陷，即如果策略在两个或两个以上的策略之间持续切换，它可能永远不会终止。这对于教育学来说是可以的，但对于实际应用来说就不行了。修改的伪代码
保证收敛。 					?
练习4.5如何为动作值定义策略迭代?给一个完整的算法计算q∗,类似于为计算v∗80页。请特别注意这个练习，因为所涉及的思想将贯穿整个练习
剩下的书。 					?

练习4.6假设你只局限于考虑ε-soft政策,这意味着选择每个动作每个状态的概率,年代,至少是ε/ | |。定性地描述在每个步骤3、2和1中需要的更改，
策略迭代算法的秩序,v∗80页。 					?
练习4.7(编程)为策略迭代编写一个程序，并通过以下更改重新解决Jack的汽车租赁问题。杰克的一个员工住在第一个地方，每天晚上乘公共汽车回家，住在第二个地方附近。她很乐意免费把一辆车送到第二个地方。每增加一辆车的价格仍然是2美元，所有的车都朝相反的方向行驶。此外，杰克在每个地点都有有限的停车位。如果超过10辆车在一个地点过夜(在任何车辆移动后)，那么使用第二个停车场的额外费用必须为4美元(与保留的车辆数量无关)。这些非线性和任意的动力学经常出现在实际的问题中，不能简单地用动态规划以外的优化方法来处理。要检查您的程序，首先复制给出的结果
原始的问题。 					?

\section{值迭代}

策略迭代的一个缺点是,它的每个迭代包括政策评估,这本身就可能是一个旷日持久的迭代计算需要多个横扫状态集。如果政策评估是迭代完成的,然后完全收敛vπ只发生在极限。我们必须等待精确的汇合，还是我们能不能就此止步?图4.1中的示例显然表明，可以截断政策评估。在这个示例中，超过前三个的策略评估迭代对相应的贪婪策略没有影响。
实际上，策略迭代的策略评估步骤可以通过多种方式被截断，而不会失去策略迭代的收敛性保证。一个重要的特殊

case是在一次扫描(每个状态的一次更新)之后停止策略评估的时候。这种算法称为值迭代。它可以写成一种特别简单的更新操作，结合了策略改进和截断的策略评估步骤:

vk + 1(s)。=最大
一个
E(Rt + 1 +γvk(圣+ 1)|圣=,=)

=最大
一个

r s ?,
p(s ?r | s,)

r +γvk(?)

, 					(4.10)


为所有的s∈s任意v0,序列{ vk }可以显示在相同的条件下收敛于v∗保证v∗的存在。
另一种理解值迭代的方法是参考Bellman最优方程(4.1)。注意，只需将Bellman最优方程转换为更新规则，就可以获得值迭代。还要注意，值迭代更新与策略评估更新(4.5)是相同的，除非它要求对所有操作都采取最大的操作。查看这种密切关系的另一种方式是在第59页(策略评估)和图3.4左边(值迭代)比较这些算法的备份图。这两个是自然计算vπ和v∗备份操作。
最后，让我们考虑值迭代是如何终止的。政策评估,价值迭代正式要求无限的迭代收敛到v∗。在实践中，当值函数在一次扫描中仅发生少量更改时，我们将停止。下面的框显示了一个具有这种终止条件的完整算法。
 

价值迭代有效地结合了它的每一个清扫，一扫政策评估和一扫政策改进。更快的收敛性通常是通过在每个策略改进扫描之间插入多个策略评估来实现的。总的来说，完整的截断策略迭代算法可以被看作是扫描序列，其中一些使用策略评估更新，其中一些使用值迭代更新。因为(4.10)中的最大操作是两者之间唯一的区别

这些更新，这仅仅意味着最大操作被添加到策略评估的一些扫描中。所有这些算法都收敛到一个最优策略的折现有限MDPs。

例4.3:赌徒的问题，赌徒有机会在抛硬币序列的结果上下注。如果硬币是正面朝上，他赢的钱和他掷硬币所得的钱一样多;如果是反面，他就会失去赌注。游戏结束时，赌徒以达到100美元的目标而获胜，或因钱用光而失败。在每次抛硬币时，赌徒必须以整数的美元来决定自己的资本份额。这个问题可以被表述为一个不打折扣的，偶然的，有限的

99 75 50 25 1
10 20 30 40 50
1 0
0.2 0.4 0.6 0.8
1

资本的资本
价值估计

最后的政策(股份)
扫1扫2扫3
扫描32
终值函数

图4.3:ph = 0.4时赌徒问题的解决方案。上面的图显示了func的值。
通过连续的值迭代扫描发现。的
下图显示了最终的策略。
MDP。状态是赌徒首都年代∈{ 1,2,。99 }和操作风险,∈{ 0 1。分钟(100−年代)}。除了那些赌徒达到他的目标(当目标是+1时)，所有转变的奖励都是零。然后状态值函数给出从每个状态中获胜的概率。政策是从资本水平到股权水平的映射。最优政策使达到目标的可能性最大化。让ph表示硬币朝上的概率。如果已知ph值，那么整个问题就是已知的，可以通过值迭代来解决。图4.3显示了在ph = 0.4的情况下，值函数在连续的值迭代扫描中发生的变化，以及最终找到的策略。这种政策是最优的，但不是唯一的。实际上，有一个完整的家庭的最优策略，都对应于对最优值函数的argmax操作选择的关系。你能猜到整个家庭是什么样子吗?

练习4.8为什么赌徒问题的最优策略有如此奇怪的形式?特别地，对于50的资本，它把所有的赌注都押在一次抛掷上，但是对于51的资本，它就这样做了
不是。为什么这是一个好的政策? 					?

练习4.9(编程)实现赌徒问题的值迭代，并求解ph = 0.25和ph = 0.55。在编程中，您可能会发现引入两个与终止对应的虚拟状态(大写0和100)，分别为0和1。图形化地显示结果，如图4.3所示。
你的结果是稳定θ→0 ? 					?
练习4.10操作值的值迭代更新(4.10)的模拟值是什么，
qk + 1(,)? 					?
\section{异步动态规划}
DP方法的主要缺点,到目前为止,我们已经讨论了它们涉及业务在整个国家的MDP,也就是说,他们需要清洁工的状态。如果国家设置非常大,甚至一个单一的扫描可以是非常昂贵的。例如，西洋双陆棋有超过1020个州。即使我们可以在每秒100万个状态上执行值迭代更新，完成一次扫描也需要花费超过1000年的时间。
异步DP算法是现有的迭代DP算法，它们不是按照对状态集的系统扫描来组织的。在更新其他状态的值之前，可以更新一些状态的值几次。然而，为了正确地收敛，异步算法必须继续更新所有状态的值:它不能在计算中某个点之后忽略任何状态。异步DP算法允许在选择状态更新时具有很大的灵活性。
例如，异步值迭代的一个版本使用值迭代更新(4.10)更新每个步骤上只有一个状态sk的值k。如果0≤γ< 1,渐近收敛到v∗保证考虑到所有国家只发生在序列{ sk }无限次数(甚至可以随机序列)。(在不打折扣的章节案例中，可能有一些更新顺序不会导致趋同，但相对而言，避免这些更新比较容易。)类似地，可以混合策略评估和值迭代更新，以生成一种异步截断策略迭代。尽管这个和其他更不常见的DP算法的细节超出了这本书的范围，但是很明显，一些不同的更新形成了构建块，可以在各种各样的无甜味DP算法中灵活地使用。
当然，避免扫描并不一定意味着我们可以减少计算量。这仅仅意味着，算法在改进策略之前，不需要陷入任何无望的长扫描。我们可以尝试利用这种灵活性，选择我们应用更新的状态，以提高算法的进展速度。我们可以尝试对更新进行排序，让价值信息以一种有效的方式从一个状态传播到另一个状态。有些国家可能不需要像其他国家那样经常更新它们的价值观。如果它们与最佳行为无关，我们甚至可以尝试跳过更新一些状态。第8章讨论了实现这一点的一些想法。
异步算法还使计算与实时交互更容易混合。为了解决给定的MDP，我们可以在代理实际体验MDP的同时运行迭代DP算法。代理的经验可以使用


确定DP算法应用其更新的状态。同时，DP算法的最新值和策略信息可以指导代理的决策。例如，我们可以在代理访问状态时对状态应用更新。这使得将DP算法的更新集中到与代理最相关的状态集的某些部分成为可能。这种聚焦是强化学习的重复主题。


\section{广义政策迭代}

策略迭代包括两个同步的、相互作用的过程，一个使值函数与当前策略(策略评估)一致，另一个使策略对当前值函数(策略改进)贪婪。在策略迭代中，这两个过程交替进行，每个过程在另一个过程开始之前完成，但这并不是真正必要的。例如，在值迭代中，每次策略改进之间只执行一次策略评估迭代。在异步DP方法中，评估和改进过程以更细的粒度进行交叉。在某些情况下，一个进程在返回到另一个进程之前更新一个状态。只要这两个进程继续更新所有状态，最终的结果通常是对最优值函数和最优策略的相同收敛。


评价




改进
π吗?贪婪(V)
Vπ
V ?vπ




v∗π∗
我们使用广义策略迭代(GPI)一词来表示让策略评估和策略改进过程相互作用的总体思想，而不依赖于这两个过程的粒度和其他细节。几乎所有的强化学习方法都被很好地描述为GPI。也就是说，所有的策略和值函数都具有可识别性，策略总是相对于值函数进行改进，而值函数总是被驱动到策略的值函数，如右边的图所示。如果评价过程和改进过程都稳定，即不再产生变化，那么价值函数和政策必须是最优的。只有当它与当前政策一致时，价值函数才会稳定，并且只有当它对当前值函数贪婪时，才会稳定。
因此，只有当发现一个策略对其自身的评估函数是贪婪的时，这两个进程才会稳定。这意味着Bellman最优性方程(4.1)成立，从而使策略和值函数是最优的。
GPI中的评估和改进过程可以看作是竞争和合作的过程。它们竞争的意义在于它们向相反的方向拉。使策略对值函数变得贪婪通常会使值函数对更改的策略不正确，使值函数与策略保持一致通常会导致策略不再贪婪。然而，从长期来看，这两个过程相互作用，以找到一个共同的解决方案:最优值函数和最优策略。



v∗,π∗
v,π
我们也可以将GPI中评估和改进过程之间的相互作用考虑为两个约束或目标——例如，如右边的图表所建议的二维空间中的两条线。虽然实际的几何图形要比这个复杂得多，但图表显示了真实情况下会发生什么。每个进程都将值函数或策略驱动到一个表示解决方案的行中。
两个目标中的一个。目标相互作用是因为这两条线不是正交的。直接向一个目标前进会导致一些运动偏离另一个目标。但不可避免的是，联合过程更接近最优性的总体目标。这个图中的箭头与策略迭代的行为相对应，每个箭头都带着系统完成两个目标中的一个。在GPI中，你也可以对每个目标采取更小、不完全的步骤。在任何一种情况下，这两个过程共同实现了最优性的总体目标，尽管两者都没有试图直接实现这一目标。

\section{动态编程效率}

DP可能并不适用于非常大的问题，但是与其他解决MDPs的方法相比，DP方法实际上是非常有效的。如果我们忽略了一些技术细节，那么(最坏的情况)DP方法找到最优策略的时间是状态数和动作数的多项式。如果n和k表示状态和动作的数量,这意味着DP方法需要大量的计算操作,小于n和k的多项式函数。DP方法保证在多项式时间内找到最优政策尽管(确定性)政策是kn的总数。从这个意义上说，DP比任何直接的政策空间搜索都要快得多，因为直接搜索必须详尽地检查每一个政策才能提供同样的保证。线性规划方法也可用于解MDPs，在某些情况下，它们的最坏情况收敛保证比DP方法更好。但是线性规划方法在比DP方法更少的状态下变得不切实际(大约是100倍)。对于最大的问题，只有DP方法是可行的。
由于维度的诅咒，有时人们认为DP的适用性是有限的，因为状态的数量常常随状态变量的数量呈指数增长。大型的状态集确实会造成困难，但是这些都是问题的固有困难，而不是开发计划署作为解决方案的方法。事实上，相对于直接搜索和线性规划等竞争方式，DP更适合处理大型状态空间。
在实践中，DP方法可以与当今的计算机一起使用，来解决具有数百万个状态的MDPs。策略迭代和值迭代都得到了广泛的应用，而且不清楚在一般情况下，哪种方法更好。在实践中，这些方法的收敛速度通常比理论的最坏情况运行时间快得多，尤其是当它们启动时

具有良好的初始值函数或策略。
对于状态空间较大的问题，通常首选异步DP方法。要完成同步方法的一次扫描，需要对每个状态进行计算和内存。对于某些问题，即使如此多的内存和计算是不切实际的，但是这个问题仍然是可以解决的，因为沿着最优解轨迹出现的状态相对较少。在这种情况下可以应用异步方法和GPI的其他变体，并且可以比同步方法更快地找到好的或最优的策略。


\section{总结}

在本章中，我们已经熟悉了动态规划的基本思想和算法，因为它们与求解有限的MDPs有关。策略评估是指(典型地)对给定策略的值函数进行迭代计算。策略改进是指根据策略的价值函数计算改进的策略。将这两个计算放在一起，我们得到了策略迭代和值迭代，这是两种最流行的DP方法。其中任何一种都可以用于可靠地计算有限MDPs的最优策略和值函数，并提供MDP的完整知识。
经典的DP方法在状态集中执行扫描操作，对每个状态执行预期的更新操作。每个这样的操作根据所有可能的继承状态的值及其发生的可能性更新一个状态的值。预期的更新与Bellman方程密切相关:它们只不过是将这些方程转化为赋值语句。当更新不再导致值的任何变化时，满足相应Bellman方程的值出现收敛。就像有四个主要价值函数(vπ,v∗、qπ和q∗),有四个相应的贝尔曼方程和四个预期相应更新。DP更新操作的直观视图由它们的备份图提供。
通过将DP方法视为通用策略迭代(GPI)，可以深入了解几乎所有的增强学习方法。GPI是围绕一个近似策略和一个近似值函数的两个相互作用过程的一般思想。一个过程接受给定的策略并执行某种形式的策略评估，将值函数更改为更类似于策略的真正值函数。另一个过程接受给定的值函数并执行某种形式的策略改进，更改策略以使其更好，假设值函数是其值函数。虽然每个过程都改变了另一个过程的基础，但总的来说，它们共同努力找到一个共同的解决方案:策略和价值函数，这两个过程都没有改变，因此是最优的。在某些情况下，GPI可以被证明为收敛的，尤其是我们在本章中介绍的传统DP方法。在其他情况下，收敛性还没有得到证明，但是GPI的思想仍然提高了我们对这些方法的理解。
不需要完全遍历状态集来执行DP方法。异步DP方法是一种就地的迭代方法，以任意顺序更新状态，可能是随机确定的，并使用过时的信息。

