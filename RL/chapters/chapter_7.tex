\chapter{第七章 n-step引导}
在本章中，我们统一了前面两章提出的蒙特卡罗(MC)方法和一步时间差(TD)方法。无论是MC方法还是单步TD方法都不是最好的。在本章中，我们介绍了n-step TD方法，这些方法概括了这两种方法，以便在需要时能够顺利地从一种方法转换到另一种方法，以满足特定任务的需求。n步方法跨越一个光谱，一端是MC方法，另一端是一步TD方法。最好的方法往往介于两个极端之间。
另一种看待n步方法的好处的方式是，它们将您从时间步的专制中解放出来。通过一个步骤的TD方法，相同的时间步骤决定了操作被更改的频率以及引导完成的时间间隔。在许多应用程序中，我们希望能够非常快速地更新操作，以考虑任何已经更改的内容，但是如果是在发生重大且可识别的状态更改的一段时间内，bootstrapping工作得最好。使用一步TD方法，这些时间间隔是相同的，因此必须做出妥协。n步方法允许在多个步骤上进行引导，使我们摆脱了单时间步的专制。
n步方法的思想通常被用于介绍资格跟踪的算法思想(第12章)，它允许同时在多个时间间隔内进行引导。在这里，我们转而考虑n步自举的想法，将对eligibilitytrace机制的处理推迟到稍后。这允许我们更好地分离问题，在更简单的n步设置中尽可能多地处理它们。
和往常一样，我们首先考虑预测问题，然后是控制问题。也就是说，我们首先要考虑的是n步方法如何能够帮助预测作为一个固定策略的状态函数的回报。在估算vπ)。然后我们将思想扩展到行为值和控制方法。

\section{n-step TD预测}

蒙特卡罗和TD方法之间的方法空间是什么?考虑估算vπ从样本集生成使用π。蒙特卡罗方法根据从该状态观察到的整个奖励序列，对每个状态执行更新，直到事件结束。另一方面，单步TD方法的更新只基于下一个奖励，一步后从状态值开始作为剩余奖励的代理。因此，一种中间方法将基于中间奖励数执行更新:多于一个，但在终止之前少于所有的。例如，两步更新将基于前两步奖励和两步后状态的估计值。类似地，我们可以有三步更新、四步更新等等。图7.1显示了备份图的频谱n-step vπ更新,一步TD更新左边和右边的up-until-termination蒙特卡罗更新。


互译TD
以及TD(0) 2-step 3-step TD n-step TD。
∞一步一步TD和蒙特卡洛
 

图7.1:n步方法的备份图。这些方法构成了光谱范围
从单步TD方法到蒙特卡罗方法。


使用n步更新的方法仍然是TD方法，因为它们仍然根据先前的估计与后来的估计的不同而改变先前的估计。之后的估计不是一步后的，而是n步后的。时间差异超过n步的方法称为n步TD方法。上一章介绍的TD方法都使用单步更新，这就是为什么我们称它们为单步TD方法。
更正式地说，考虑状态St的估计值的更新，由于状态奖赏序列St, Rt+1, St+1, Rt+2…, RT, ST(省略动作)我们知道在蒙特卡洛更新估计vπ(St)更新的方向

完整的回报:
Gt。= Rt + 1 +γRt + 2 +γ2Rt + 3 +···+γT−−1 t Rt,
其中T是这一集的最后一步。让我们将这个量称为更新的目标。而在蒙特卡洛更新目标是回报，一步更新目标是第一个回报加上下一个状态的折现估计值，我们称之为一步回报:

Gt:t + 1。= Rt + 1 +γVt(圣+ 1),
Vt的地方:S→R估计在时间t的vπ就在这里。Gt的下标:t + 1表明,这是一个截断换取时间t使用奖励直到时间t + 1,与贴现估计γVt(圣+ 1)取代其他的条款γRt + 2 +γ2Rt + 3 +···+γT−−1 t rt的回报,正如我们在前一章讨论。我们现在的观点是，这个想法在两个步骤之后和一个步骤之后一样有意义。两步更新的目标是两步返回:
Gt:t + 2。= Rt + 1 +γRt + 2 +γ2Vt + 1(圣+ 2),
现在γ2Vt + 1(圣+ 2)纠正缺失的条款γ2Rt + 3 +γ3Rt + 4 +···+γT−−1 t rt。同样，任意n步更新的目标是n步返回:
Gt:t + n。= Rt + 1 +γRt + 2 +···+γn−1 Rt + n +γnVt + n−1(圣+ n)(7.1)
所有n,t(n≥1和0≤t < t−n。所有n-step回报可以被认为是全部返回近似,截断后n步,然后纠正其余失踪条款Vt + n−1(圣+ n)。如果t + n≥(如果n-step返回延伸到或超过终止),那么所有缺失的条款视为零,等于定义和n-step返回普通全部返回(Gt:t + n。如果t + n≥t = Gt)。
注意，n> 1的n步返回包含了从t到t+ 1转换时不可用的未来奖励和状态。没有真正的算法可以使用n-step返回直到它Rt + n和v + n计算−1。第一次出现这种情况是t+ n，使用n步返回的自然状态值学习算法是这样的

Vt + n(St)。= Vt + n−1(St)+α?Gt:t + n−Vt + n−1(St)?,0≤t < t(7.2),而其他州的值保持不变:Vt + n(s)= Vt + n−1(s),所有的s ? =圣。我们称这个算法为n步TD。注意任何变化都是在每集的前n−1步骤。为了弥补这一点，在这一集的结尾、结束后和开始下一集之前都要进行同样多的更新。完整的伪代码在下一页的框中给出。
练习7.1在第6章中，我们注意到如果值估计值不随步骤变化，那么蒙特卡罗误差可以写成TD误差之和(6.6)。显示在(7.2)中使用的n步错误也可以作为一个和TD错误(同样，如果是值的话
估计不会改变)概括先前的结果。 					?
练习7.2(编程)使用n步方法，值估计值会随着步骤的不同而变化，因此使用TD错误和的算法(参见前面的练习)

(7.2)中的错误位置实际上是一个稍微不同的算法。是更好的算法还是更糟糕的算法?设计并编写一个小实验来回答这个问题
问题经验。 					?
n-step返回使用价值函数Vt + n−1纠正丢失的奖励之外Rt + n。n-step返回的一个重要属性是他们的期望是保证更好的估计比Vt vπ+ n−1,在一个最坏的状态。也就是最糟糕的错误预期n-step返回保证小于或等于γn乘以最严重的错误在Vt + n−1:

马克斯
s
吗?吗?? Eπ[Gt:t + n |圣= s]−vπ(s)
吗?吗?吗?≤γn马克斯
s
吗?吗?? Vt + n(s)−−1 vπ(s)
吗?吗?,(7.3)
对于所有n≥1。这被称为n步返回的错误减少属性。由于误差减小的性质，我们可以正式地证明，在适当的技术条件下，所有n阶TD方法都收敛于正确的预测。因此，n步TD方法形成了一组声音方法，其中一步TD方法和蒙特卡罗方法是极端成员。
示例7.1:在示例6.2中描述的5状态随机漫步任务中，考虑使用n-step TD方法。假设第一集直接从中心状态C到右边，经过D和E，然后在右边结束，返回1。回想一下，所有状态的估计值都是从中间值V (s) = 0.5开始的。由于这种经验，一步法只会改变最后一种状态的估计，

V (E)增加到1，观察到的返回值。另一方面，两步方法将增加前两种状态的值:V (D)和V (E)都将递增为1。对于n >2，一个三步法，或者任何一个n步法，会将所有三个访问状态的值都增加到1，所有的值都是相同的。
n的哪个值更好?图7.2显示了一个简单的实证测试的结果为一个较大的随机游走过程,19个州,而不是5(−1左边的结果,所有的值初始化为0),我们在本章使用作为一个运行的例子。结果显示n-step TD方法n和α的值范围。显示的性能测量每个参数设置,在纵轴上,是√(之间的平均平方误差预测最后一集的19个州和自己的真实值,然后平均在第十集,100年整个实验的重复(此句组走都是用于所有参数设置)。注意，中间值为n的方法效果最好。这说明了将TD和蒙特卡罗方法推广到n步方法，可能比这两种极端方法中的任何一种都要好。
 
0.5



平均均方根误差
0.45



超过19个州0.4和前10个州
0.35事件
0.3



0.25

0.2 0
0.4 					0.8 - 0.6 					1

α

图7.2:性能n-step TD方法作为α的函数,针对各种n值,19个随机漫步的任务(例如7.1)。


练习7.3为什么你认为在本章的例子中使用了一个更大的随机行走任务(19个州而不是5个州)?走小一点的路会把优势转移到n的不同值吗?左侧的变化结果如何从0到−1在大走路?你认为这对n的最大值有影响吗??


\section{ n-step撒尔沙}

n步方法如何不仅用于预测，还用于控制?在本节中，我们将展示n步方法如何以一种简单的方式与Sarsa结合

146年 					第七章:n-step引导



生成一种策略TD控制方法。我们称Sarsa的n阶版本为n阶Sarsa，而在上一章中出现的原始版本我们将其命名为one-step Sarsa(0)
主要的思想是简单的开关状态的行为(政府行动对),然后使用一个ε-greedy政策。n步Sarsa的备份图(如图7.3所示)与n步TD的备份图(图7.1)一样，是交替状态和动作的字符串，除了Sarsa的备份图都以动作而不是状态开始和结束。我们根据估计的行动值重新定义n步回报(更新目标):

Gt:t + n。= Rt + 1 +γRt + 2 +···+γn−1 Rt + n +γnQt + n−1(圣+ n + n),n≥1,0≤t < t−n,
(7.4)

Gt:t + n。如果t + n≥t = Gt。自然算法是

Qt + n(St)。= Qt + n−1(St)+α(Gt:t + n n−−Qt + 1(圣,)],0≤t < t(7.5),而其他州的值保持不变:Qt + n(,)= Qt + n−1(,),为所有的年代,这样一个年代吗? =圣或? =。我们称之为n步萨尔萨算法。伪代码显示在下一页的框中，图7.4给出了一个与单步方法相比，它可以加速学习的例子。


互译撒尔沙
Sarsa(0) 2-step Sarsa 3-step Sarsa n-step Sarsa。
∞要撒尔沙即蒙特卡洛
n-step
预计撒尔沙
 

图7.3:状态操作值的n步方法频谱的备份图。它们的范围从Sarsa(0)的一步更新到蒙特卡罗方法的上至终止更新。在这两者之间是n步更新，基于n步的实际奖励和第n个下一个状态-行动对的估计价值，所有的都适当地打折。在最右边是n阶期望Sarsa的备份图。
路线
动作值增加一步萨尔萨
动作值增加
由所述的撒尔沙
图7.4:使用n步方法加速政策学习的Gridworld示例。第一个面板显示了代理在单个事件中所采取的路径，以a结尾。
高回报的位置，由g标记。在这个例子中，所有的值最初都是0，并且都是0
除了g点的正奖励外，奖励为零。另外两个面板上的箭头显示了通过一步和n步Sarsa方法，哪些行为值因这条路径而增强。一步法只强化动作序列的最后一个动作
导致高回报，而n阶方法加强了序列的最后n个动作，
从这一集中学到了很多。

练习7.4证明Sarsa(7.4)的n阶返回可以精确地用一个新的TD错误来写，as


Gt:t + n = Qt−1(圣,)+
最小值(t + n t)−1 ?k = t
γk−t[Rk + 1 +γQk(Sk + 1,正义与发展党+ 1)−Qk−1(Sk,Ak)]。

(7.6)

预期的撒尔沙呢?预期Sarsa的n步版本的备份图显示在图7.3的最右侧。它由一个样本的线性字符串操作和状态,就像在n-step撒尔沙,除了它的最后一个元素是一个分支在所有行动可能性加权,一如既往,概率在π。这个算法可以用与n步Sarsa(上)相同的方程来描述，除了n阶返回重新定义为。

Gt:t + n。= Rt + 1 +···+γn−1 Rt + n +γnV̄t + n−1(圣+ n),t + n < t)(7.7)(Gt:t + n。= Gt t + n≥t)V̄t(s)是预期的近似值的年代,使用估计的行动值在时间t,根据目标政策:

V̄t(年代)。=

一个
π(|)Qt(,)∈年代。 					(7.8)

期望近似值用于开发本书其余部分的许多动作-值方法。如果s是终端，则其期望值定义为0。


\section{n-step Off-policy学习}

回想一下,off-policy学习是学习一个政策的价值函数,π,而另一项政策后,b。通常,π的贪婪策略目前action-value-function估计,和b是一个探索性的政策,也许ε-greedy。为了使用来自b的数据，我们必须考虑到两个策略之间的差异，使用它们采取所采取行动的相对概率(见第5.5节)。在n步方法中，返回是在n步之上构建的，所以我们对这些n个动作的相对概率感兴趣。例如，要制作一个简单的n步TD的脱机版本，只需对时间t(实际上是在时间t+ n)进行加权
ρt:t + n−1:

Vt + n(St)。= Vt + n−1(St)+αρt:t + n−1(Gt:t + n−Vt + n−1(St)],0≤t < t(7.9)ρt:t + n−1,称为重要性抽样比率,相对概率的两种策略下的n行为从+ n−1(cf Eq。5.3):

例如,如果任何一个永远不会采取行动的π(即。π(Ak | Sk)= 0)然后n-step返回应给予零重量和完全忽略。另一方面,如果偶然一个π的行动将有更大概率比b,那么这将增加重量,否则会返回。这是有意义的,因为操作的特点是π(因此我们想了解它),但选择b,因此很少出现很少的数据。为了弥补这一点，我们必须在这种情况发生时对其进行超重处理。注意，如果两个策略实际上是相同的(on-policy的情况)，那么重要性抽样比率总是1。因此，我们的新更新(7.9)概括并可以完全取代我们之前的n步TD更新。同样，我们之前的n步Sarsa更新可以被一个简单的off-policy表单完全替代:

Qt + n(St)。= Qt + n−1(St)+αρt + 1:t + n(Gt:t + n n−−Qt + 1(圣,)],(7.11)为0≤t < t。注意，这里的重要性抽样比n步TD(7.9)晚一步开始和结束。这是因为我们正在更新状态-动作对。我们不需要关心我们选择行动的可能性;既然我们已经选择了它，我们想要充分地从发生的事情中学习，并且只对后续操作进行重要的抽样。完整算法的伪代码如下所示。

非策略版本的n-step预期Sarsa将使用与上面对n-step Sarsa相同的更新，除了重要性抽样比率将减少一个因素。就是上面的方程使用ρt + 1:t + n−1代替ρt + 1:t + n,当然它会使用预期的撒尔沙版本的n-step返回(7.7)。这是因为在预期的Sarsa中，所有可能的行为都在最后的状态中被考虑;实际上服用的那一种没有效果，也不需要纠正。


\section{*每一决策方法与对照不同}

上一节中介绍的多步离策略方法很简单，概念上也很清楚，但可能不是最有效的方法。一种更复杂的方法将使用每个决策的重要性抽样思想，如第5.9节所介绍的。要理解这种方法，首先要注意，普通的n步返回(7.1)和所有返回一样，都可以递归地编写。对于在视界h处结束的n步，可以写入n步返回

Gt:h = Rt + 1 +γGt + 1:h t < h < t, 					(7.12)
“大酒店”:h。= Vh−1(Sh)。(回想一下，这个返回在时间h时使用，之前表示t + n。)现在考虑行为策略b后的效果是不一样的目标政策π。所有产生的经验,包括第一个奖励Rt + 1和下一个状态圣+ 1必须由重要性抽样比例加权时间t,ρt =π(圣)| b(| St)。人们可能会简单地用上面等式的右边来衡量，但我们可以做得更好。假设行动在时间t永远不会选择通过π,所以ρt是零。然后，一个简单的加权将导致n步返回为零，这将导致作为目标时的高方差。相反，在这种更复杂的方法中，我们使用另一种策略定义，即n步返回结束于horizon h, as
Gt:h。=ρt(Rt + 1 +γGt + 1:h)+(1−ρt)Vh−1(St),t < h < t(7.13),再次Gh:h。= Vh−1(Sh)。在这种方法中,如果ρt是零,那么目标而不是零,导致估计缩水,目标是一样的估计,导致没有变化。重要性抽样比率为零意味着我们应该忽略样本，因此保持估计不变似乎是合适的。第二项，附加项(7.13)称为控制变量(由于不明确的原因)。注意，控件变量不会更改预期的更新;重要性抽样比率的期望值为1(第5.9节)，与估计值不相关，因此控制变量的期望值为零。还要注意,off-policy定义(7.13)是一个严格的泛化的早些时候在政策的定义n-step返回(7.1),这两个相同的政策情况下,ρt总是1。
对于传统的n阶方法来说，与(7.13)结合使用的学习规则是n阶TD update(7.2)，它除了嵌入在返回中的抽样比率外没有显式的重要性抽样比率。
练习7.5编写策略外状态值预测算法的伪代码
上面所描述的。 					?

对于动作值，n步返回的off-policy定义有点不同，因为第一个动作在重要性抽样中没有作用。第一个动作是学习到的;在目标政策下，它是否不可能甚至不可能是不重要的，它已经被采取了，现在必须给它的奖励和状态提供全部的单位重量。重要性抽样只适用于其后的行为。
首先要注意的是，对于操作，以horizon h结尾的n步策略返回值，期望形式(7.7)可以像在(7.12)中一样递归地写，但是对于操作，递归以Gh:h结尾。= V̄h−1(Sh)如(7.8)。带有控件变量的off-policy表单是

Gt:h。= Rt + 1 +γ

ρt + 1 gt + 1:h + V̄h−1(圣+ 1)−ρt qh−1 + 1(圣+ 1,+ 1)

= Rt + 1 +γρt + 1

Gt + 1:h Qh−−1(圣+ 1,+ 1)

+γV̄h−1(圣+ 1),t < h≤t。
(7.14)

如果h < T，那么递归以Gh:h结束。= Qh−1(Sh啊),但是,如果h T≥,递归结束和GT−1:h。= RT。合成预测算法(与(7.5)结合后)类似于预期的Sarsa。
练习7.6证明上述方程中的控制变量不改变
收益的期望值。 					?
∗练习7.7编写的伪代码off-policy行为价值预测算法描述上方。特别注意终止条件
到达视界或情节结束时的递归。 					?
练习7.8显示了n阶跃返回(7.13)的一般(非策略)版本仍然可以准确、紧凑地写成基于状态的TD错误(6.5)的总和
近似状态值函数不变。 					?
练习7.9重复上述练习，得到偏离策略的n步返回(7.14)和预期的Sarsa TD错误(公式中括号中的数量)。


练习7.10(编程)设计一个小的离策略预测问题，并使用它来表明使用(7.13)和(7.2)的离策略学习算法更有效
与使用(7.1)和(7.9)的简单算法相比。 					?
我们在本节、前一节和第5章中使用的重要性抽样使我们能够进行良好的偏离策略的学习，但也会导致高方差更新，迫使使用一个小的步长参数，从而导致学习速度变慢。很可能不可避免的是，非政策培训要比政策培训慢——毕竟，数据与正在学习的东西没有多大关系。然而，这些方法也可以改进。控制变量是减少方差的一种方法。另一种方法是根据观察到的方差快速调整步长，如Autostep方法(Mahmood、Sutton、Degris和Pilarski, 2012)。另一种有希望的方法是田(为准备)将Karampatziakis和Langford(2010)的不变更新扩展到TD。Mahmood的使用技术(2017);马哈茂德

Sutton(2015)也可能是解决方案的一部分。在下一节中，我们将考虑一个不使用重要性抽样的非策略学习方法。


\section{无重要抽样的脱机学习:n步树备份算法}

在没有重要抽样的情况下，偏离政策的学习是可能的吗?Q-learning和来自第六章的期望Sarsa在单步情况下可以做到这一点，但是有相应的多步算法吗?在本节中，我们介绍了这样一个n步方法，称为树备份算法。

在Rt + 1 + 1
圣+ 1



圣+ 2
Rt + 2



在Rt + 3 + 2 + 3


的3步tree-backup
更新
该算法的思想由右图所示的三步树备份备份图提出。沿着中心脊柱，在图中标注有三个样本状态和奖励，以及两个样本动作。这些是表示初始状态-动作对St之后发生的事件的随机变量。挂在每个状态边的是未被选择的操作。(对于最后一个状态，所有的动作都被认为还没有被选中。)因为我们没有未选中操作的示例数据，所以我们引导并使用它们的值估计值来形成更新的目标。这稍微扩展了备份图的概念。到目前为止，我们总是将图顶部节点的估计值更新为目标，并结合沿途的奖励(适当的折扣)和底部节点的估计值。在树备份更新中，目标包含所有这些东西，加上悬挂在各个级别上的悬空操作节点的估计值。这就是为什么它被称为树备份更新;它是对整个估计动作值树的更新。
更准确地说，更新是来自树的叶节点的估计操作值。在内部的动作节点，对应于
实际采取的行动，不要参与。每个叶节点对目标与体重成正比的概率发生在目标政策π。因此每个一级操作造成的体重π(|圣+ 1),除了真正采取行动,在+ 1,不贡献。其概率,π(+ 1 |圣+ 1),用于重量所有二级动作值。因此，每一个非选定的二级动作a?贡献与体重π(+ 1 |圣+ 1)π(一个? |圣+ 2)。每个第三级行动贡献与体重π(+ 1 |圣+ 1)π(圣+ 2)+ 2 |π(? ? |圣+ 3),等等。它就像图中的一个动作节点的每个箭头都是根据目标策略中被选择的操作的概率来加权的，如果在操作下面有树，那么这个权重就适用于树中的所有叶节点。

我们可以将三步树备份更新看作是由6个半步组成的，在一个动作到一个后续状态的半步之间交替进行，并且从这个状态考虑所有可能的动作以及它们在策略下发生的概率。
现在让我们为n步树备份算法开发详细的方程。一步返回(目标)与预期的Sarsa相同，

Gt:t + 1。= Rt + 1 +γ

一个
π(圣+ 1)| Qt(圣+ 1), 					(7.15)

t < t−1,两步tree-backup返回

Gt:t + 2。= Rt + 1 +γ

在+ 1 ? =
π(圣+ 1)| Qt + 1(圣+ 1)

在+ 1 |圣+ +γπ(1)

Rt + 2 +γ

一个
圣+ 2π(|)Qt + 1(圣+ 2)

= Rt + 1 +γ

在+ 1 ? =
π(圣+ 1)| Qt + 1(圣+ 1 a)+γπ(+ 1 |圣+ 1)Gt + 1:t + 2,


t < t−2。后一种形式给出了树备份n步返回的一般递归定义:

Gt:t + n。= Rt + 1 +γ

在+ 1 ? =
π(圣+ 1)| Qt + n−1(圣+ 1)+γπ(+ 1 |圣+ 1)Gt + 1:t + n(7.16)
t < t−1,n≥2,n = 1例由(7.15)除了GT−1:t + n。= RT。然后将此目标与通常的n步Sarsa动作-值更新规则一起使用:

Qt + n(St)。= Qt + n−1(St)+α(Gt:t + n n−−Qt + 1(圣,),

0≤t < t,而所有其他政府行动对的值保持不变:Qt + n(,)= Qt + n−1(,),为所有的年代,这样一个年代吗? =圣或? =。该算法的伪代码显示在下一页的框中。
练习7.11表明，如果近似的动作值不变，则树备份返回(7.16)可以写成基于预期的TD错误之和:


Gt:t+n = Q(St, At) +。
最小值(t + n−1 t−1)?k = t
δk
k ?我= t + 1
γπ(Ai | Si),

δt的地方。= Rt + 1 +γV̄t(圣+ 1)−Q(圣,)和V̄t由(7.8)给出。?

\section{7.6 *统一算法:n-step Q(σ)}
到目前为止，在本章中，我们已经考虑了三种不同类型的动作-值算法，对应于图7.5所示的前三个备份图。n阶Sarsa有所有的样本跃迁，树备份算法有所有的状态到动作的跃迁完全分支，没有采样，n阶期望Sarsa有所有的样本跃迁，除了最后一个状态到动作的跃迁，它以一个期望值完全分支。这些算法能统一到什么程度?
图7.5中的第四个备份图建议了一种统一的思想。这是这样一种想法，即可以一步一步地决定是否要将操作作为示例(如在Sarsa中)，或者考虑对所有操作的期望(如在树备份更新中)。然后，如果选择总是抽样，就会得到Sarsa，而如果选择从不抽样，就会得到树备份算法。预期Sarsa将是一个案例，其中一个选择除了最后一个步骤之外的所有步骤。

四步撒尔沙
四步
树备份
四步
预计撒尔沙
四步
问(σ)
 
图7.5:本章迄今为止考虑的三种n步动作-值更新的备份图(4步情况)，加上第四种结合的更新的备份图
他们所有人。一半的ρ表示转换的重要性抽样中需要off-policy情况。第四种更新结合其他各州的基础上通过选择是否样品(σt = 1)(σt = 0)。


当然还有很多其他的可能性，正如图中最后一张图所示。为了进一步增加可能性，我们可以考虑抽样和期望之间的连续变化。让σt∈[0,1]表示取样的程度上一步t,用σ= 1表示完整的抽样和σ= 0表示一个纯粹的期望没有抽样。随机变量σt可能被设定为一个函数的状态,行动,或政府行动在时间t。我们称之为n-step Q(σ)拟议的新算法。
现在让我们开发n-step Q(σ)的方程。首先我们写tree-backup n-step返回(7.16)的地平线h = t + n,然后在预期的近似值V̄(7.8):

Gt:h = Rt + 1 +γ

在+ 1 ? =
Qh−π(|圣+ 1)1(圣+ 1 a)+γπ(+ 1 |圣+ 1)Gt + 1:h

= Rt + 1 +γV̄h−1(圣+ 1)−γπ(+ 1 |圣+ 1)Qh−1(圣+ 1,+ 1)+γπ(+ 1 |圣+ 1)Gt + 1:h
= Rt + 1 +γπ(+ 1 |圣+ 1)

Gt + 1:h Qh−−1(圣+ 1,+ 1)

+γV̄h−1(圣+ 1),
之后,这正是像n-step换取撒尔沙与控制变量(7.14)除了操作概率π(+ 1 |圣+ 1)代替重要性抽样比率ρt + 1。问(σ),我们这两种情况之间的滑动线性:

Gt:h。= Rt + 1 +γ

σt + 1ρt + 1 +(1−σt + 1)π(+ 1 |圣+ 1)

Gt + 1:h Qh−−1(圣+ 1,+ 1)

+γV̄h−1(圣+ 1), 					(7.17)

156年 					第七章:n-step引导



t < h≤t。递归以Gh:h结束。= Qh−1(Sh啊)如果h < T,或与GT−1:T。如果h = T。然后我们使用n-step Sarsa(7.11)的通用(off-policy)更新。在方框中给出了一个完整的算法。
 \section{总结}

在本章中，我们开发了一系列时间差异学习方法，它们介于前一章的一步TD方法和前一章的蒙特卡罗方法之间。包含中间引导量的方法很重要，因为它们通常比任何一种极端都要好。



四步TD
本章的重点是n步方法，它展望下一个n个奖励、状态和行为。右边的两个四步备份图总结了介绍的大多数方法。州值更新显示是n-step TD重要性抽样,和行为价值更新n-step Q(σ),给出预期撒尔沙和Q学习的。所有n步方法都需要在更新之前延迟n个时间步骤，因为只有这样才能知道所有需要的未来事件。另一个缺点是，与以前的方法相比，它们涉及更多的计算时间步长。与单步方法相比，n步方法还需要更多的内存来记录最后n步的状态、动作、奖励，有时还需要更多的内存。最后，在第12章中，我们将看到多步TD方法是如何通过使用资格跟踪以最小的内存和计算复杂度实现的，但是除了一步方法之外，总会有一些额外的计算。这样的代价很值得为摆脱单一时间的暴政而付出代价。
虽然n-step方法比那些使用资格跟踪的方法要复杂得多，但是它们在概念上是清晰的。我们试图利用这一点，在n-step案例中开发出两种脱机学习的方法。1、基于
重要性抽样概念上很简单，但方差很大。如果目标和行为策略非常不同，那么在实现效率和实用性之前，可能需要一些新的算法思想。另一种是基于树备份更新的Q-learning自然扩展到具有随机目标策略的多步情况。它不涉及重要性抽样，但是，如果目标和行为策略本质上不同，即使n很大，引导也可能只跨越几个步骤。

书目的和历史的言论

n阶返回的概念是由Watkins(1989)提出的，他也首先讨论了他们的错误减少属性。这本书的第一版探讨了n步算法，在这本书中，它们被视为概念性兴趣，但在实践中不可行。Cichosz(1995)和van Seijen(2016)的研究表明，它们实际上是完全实用的算法。鉴于此，以及它们在概念上的清晰性和简洁性，我们选择在第二版中突出它们。特别是，我们现在将所有关于逆向观点和资格追溯的讨论推迟到第12章。

7.1-2基于工作对本文进行了随机游动示例中的结果
萨顿(1988)，辛格和萨顿(1996)。在本章中使用备份图来描述这些和其他算法是新的。

7.3-5这些部分的开发是基于Precup, Sutton的工作，
还有Singh (2000)， Precup, Sutton, Dasgupta(2001)，还有Sutton, Mahmood, Precup, van Hasselt(2014)。
树备份算法是由于Precup, Sutton和Singh(2000)，但它的呈现是新的。

7.6 Q(σ)算法是新到这个文本,但算法密切相关
De Asis, Hernandez-Garcia, Holland和Sutton(2017)进一步探索。
