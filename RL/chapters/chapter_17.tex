\section{第十七章 前沿}
\begin{summary}
	在最后一章中，我们谈到了一些超出本书范围的话题，但我们认为这些话题对于强化学习的未来尤其重要。这些主题中的许多超出了可靠的已知范围，有些超出了MDP框架
\end{summary}。


\section{一般价值函数和辅助任务}

在这本书的过程中，我们的价值函数的概念变得相当普遍。使用脱机策略学习，我们允许值函数以任意目标策略为条件。在12.8节我们广义打折终止函数γ:S ?→[0,1],这样不同的贴现率可以应用在每个时间步在决定返回(12.17)。这让我们能够预测出，在一个任意的、依赖于国家的视界上，我们能得到多少回报。下一步，或许也是最后一步，是超越奖励的泛化，允许对任意信号的预测。与预测未来奖励的总和不同，我们可以预测声音或颜色感觉的未来值的总和，或者预测内部高度处理的信号，比如另一个预测。无论以这种方式在一个值函数式的预测中加入什么信号，我们都称它为该预测的累积量。我们形式化累积量信号Ct∈r .使用这种一般的价值函数,或养狐业,写

vπ,γ,C(s)= E

∞吗?k = t
吗?k ?
我= t + 1
γ(Si)

Ck + 1

圣= s:∞∼π

。(17.1)

与传统价值函数(如vπ或q∗)这就是我们寻求一个理想函数近似的参数化形式,我们可能会继续表示v̂(s,w),当然就会有不同的w为每个预测,也就是说,对于每个选择π,γ,c,因为奖励养狐业没有必然联系,这也许是一个误称称之为价值函数。人们可以简单地称之为预测，或者更有特色地称之为预测(圈，在准备中)。不管叫什么，它

是一个值函数的形式，因此可以用本书中介绍的学习近似值函数的方法来学习。除了习得的预测之外，我们还可以学习一些策略，以通常的方式，通过广义的策略迭代(第4.6节)或由专家-批评家的方法来最大化预测。这样，一个代理就可以学会预测和控制大量的信号，而不仅仅是长期的奖励。
为什么预测和控制信号比长期奖励有用呢?这些都是辅助任务，因为它们是额外的、附加的，是最大化回报的主要任务。一个答案是，预测和控制多种多样的信号的能力可以构成一种强有力的环境模型。正如我们在第8章中看到的，一个好的模型可以使代理更有效地获得报酬。需要进一步的概念才能清楚地开发这个答案，因此我们将其推迟到下一节。首先，让我们考虑两种更简单的方法，在这两种方法中，大量不同的预测可以帮助增强学习代理。
辅助任务可以帮助完成主任务的一种简单方法是，它们可能需要一些与主任务相同的表示。一些辅助任务可能更容易，更少的延迟和行动与结果之间更清晰的联系。如果在容易的辅助任务中可以及早发现好的特性，那么这些特性可能会显著地加快对主要任务的学习。没有必要的理由证明这是真的，但在很多情况下这似乎是合理的。例如，如果你学会了在短时间尺度上预测和控制你的传感器，比如秒，那么你可能会合理地提出物体的部分概念，这将极大地帮助预测和控制长期奖励。
我们可以想象一个人工神经网络(ANN)，其中最后一层被分割成多个部分，或者头部，每个人都在处理不同的任务。一个人可能为主要任务生成近似的值函数(以奖励为累积量)，而另一个人可能为各种辅助任务生成解。所有的正面都可以通过随机梯度下降传播到同一个身体——网络的共享前一部分，然后在它的下至上一层试图形成表象，以支持所有正面。研究人员已经尝试了辅助任务，比如预测像素的变化，预测下一次的奖励，并预测回报的分布。在许多情况下，这种方法已经被证明可以极大地加速学习的主要任务(Jaderberg等人，2017)。类似地，多次提出多个预测作为指导国家估计的一种方法(见第17.3节)。
另一种学习辅助任务的简单方法是通过类比经典条件作用的心理现象(第14.2节)来最好地解释。理解经典条件作用的一种方式是，进化建立在一个反射(非习得的)关联上，与一个特定信号的预测的特定行为相关联。例如，人类和其他许多动物似乎都有一种天生的反射，每当它们预测被戳到眼睛时，就会眨眼。这个预测是学来的，但是从预测到闭眼之间的联系是建立在这个基础上的，因此这个动物在它的眼睛里省下了许多没有保护的戳。同样，从恐惧到心率的增加，或者到冰冻的联系也可以建立起来。代理

设计师可以做一些类似的事情，通过设计(不学习)将特定事件的预测与预定的行为联系起来。例如，一辆自动驾驶汽车如果学会了预测未来是否会发生碰撞，那么当预测超过某个阈值时，就可以给它一个内置的反射，让它停下来，或者转开。或者考虑一个真空清洁机器人，它学会了在返回充电器之前预测它是否会耗尽电池电量，并且当预测变为非零的时候，它会本能地返回到充电器上。正确的预测将取决于房子的大小、机器人所在的房间以及电池的年龄，这些都是机器人设计者很难知道的。设计人员很难建立一个可靠的算法来决定是否返回到充电器的感官，但是从学习到的预测来看，这很容易做到。我们预见到许多可能的方法，如这种方法，学习的预测可以有效地结合内在的控制行为的算法。
最后，也许辅助任务最重要的作用是超越我们在本书中所做的假设，即状态表示是固定的，并且给了代理。要解释这一作用，我们首先必须后退几步，以认识到这一假设的重要性以及消除它的意义。我们在第17.3节中这样做。


\section{通过选项的时间抽象}

MDP形式主义的一个吸引人的方面是，它可以在许多不同的时间尺度上有效地应用于任务。一个人可以用它来正式地完成这样的任务:决定用哪块肌肉去抓住一个物体，用哪架飞机去方便地到达一个遥远的城市，用哪份工作去过一个令人满意的生活。这些任务在它们的时间尺度上有很大的不同，但是每个任务都可以被有效地表述为MDP，可以通过计划或学习过程来解决，如本书所述。所有这些都涉及到与世界的互动、顺序决策，以及一个有用的目标，即随着时间的推移积累回报，因此所有这些都可以写成MDPs。
虽然所有这些任务都可以用MDPs来表示，但是人们可能认为它们不能用一个MDP来表示。它们涉及如此不同的时间尺度，如此不同的选择和行动概念!例如，在肌肉抽搐的水平上计划一次穿越大陆的飞行将是没有好处的。然而，对于其他任务，抓握、投掷飞镖或打棒球，低水平的肌肉抽搐可能正好合适。人们做所有这些事情都天衣无缝，似乎没有在不同的层次之间切换。MDP框架可以同时扩展到所有级别吗?
也许可以。一种流行的想法是在一个详细的级别上，使用一个小的时间步骤，使MDP形式化，但是使用与许多基本的时间步骤相对应的扩展的行动过程，使计划能够在更高的级别上进行。要做到这一点，我们需要一个行动过程的概念，它可以扩展到多个时间步骤，并包含终止的概念。一般方法是政策制定这两个想法,π,与依赖政府的终止功能,γ,如养狐业。我们把它们中的一对定义为一种被称为选项的广义行为概念。执行一个选项ω= ?πωγω吗?在时间t是获取动作,,从πω(圣·|),然后在时间t + 1终止概率γω(圣+ 1)。如果选择

不终止t + 1,然后从πω选择+ 1(圣·| + 1),并选择终止在t + 2的概率γω(圣+ 2),等等,直到最终终止。方便考虑低级操作options-each行动的特殊情况对应于一个选项?πω,γω吗?其政策选择行动(πω(s)= a∈年代),其终止函数为零(γω所有∈年代(s)= 0 +)。选项有效地扩展了动作空间。代理可以选择一个低级操作/选项，在一个时间步骤之后终止，或者选择一个扩展选项，在终止之前执行多个时间步骤。
选项被设计成可以与低级操作互换。例如,一个行为价值函数的概念qπ自然地推广到一个选项值函数,它接受一个状态和选择作为输入,并返回预期的返回状态,从选择终止执行,并且按照政策,此后π。我们还可以将策略的概念概括为从选项而不是操作中选择的分层策略，在选择时，在选择时，执行到终止。有了这些想法，本书中的许多算法都可以推广到学习近似的选项-值函数和层次策略。在最简单的情况下，学习过程从选项启动“跳转”到选项终止，只有当选项终止时才会发生更新。更微妙的是，可以使用“内部选择”学习算法在每个时间步骤上进行更新，这通常需要策略外的学习。
也许最重要的概括性观点是环境模型，如第3章，第4章和第8章所阐述的。行为的传统模型是状态转移的概率和在每个状态下采取行为的预期立即奖励。传统的行为模型是如何推广到期权模型的?对于选项，适当的模型是由两个部分组成的，一个对应于执行该选项导致的状态转换，另一个对应于沿途的预期累积回报。期权模型的奖励部分，类似于国家行动对(3.5)的预期回报，是。
r(s,ω)。= e r1 +γR2 +γ2R3 +···+γτ−1 rτ?S0 = s A0:τ−1∼πω,τ∼γω(17.2),所有选项ω和州∈年代,τ是随机时间步的选择根据γω终止。注意整体折扣参数γ的角色在这个equation-discounting根据γ,但根据γω终止选项。选择模型的状态转换部分稍微有点微妙。模型的这一部分描述了每个可能的结果状态的概率(如(3.4))，但是现在这个状态可能会在不同的时间步长之后产生，每个时间步长都必须以不同的方式折现。ω模型选项指定为每个州年代ω可能开始执行,和每个国家年代?ω可能终止,

p(s ?|年代,ω)。=
∞吗?k = 1
γkPr { Sk = s ?τ= k | S0 =年代,A0:k−1∼πω,τ∼γω}。(17.3)

注意,因为γk的因素,这p(s ?|年代,ω)不再是一个转移概率和不再总结1 / s的所有值吗?。(尽管如此，我们仍然使用p中的|符号)

上面对选项模型转换部分的定义允许我们制定Bellman方程和动态编程算法，这些算法适用于所有选项，包括作为特殊情况的低级操作。例如,一般传达员状态方程的分层策略π的值


vπ=

ω∈Ω(s)
π(ω|)

r(年代,ω)+ ?
s?
p(s ?|年代,ω)vπ(?)

, 					(17.4)


Ω(s)表示一组选项的状态。如果Ω(s)只包括底层操作,那么这个方程可以减少平时的贝尔曼方程(3.14)的一个版本,当然除了γ是包含在新的p(17.3),因此没有出现。同样,也没有γ相应的规划算法。例如，带有选项的值迭代算法，类似于(4.10)


vk + 1(s)。=最大
ω∈Ω(s)

r(年代,ω)+ ?
s?
p(s ?|年代,ω)vk(?)

所有∈年代。


如果Ω(s)包括所有可用的低级的行为在每个年代,然后这个算法收敛到传统v∗,最优策略可以计算。然而,它计划选项尤其有用,当只有一个子集的可能的选择被认为是(Ω(s))在每一个状态。值迭代将收敛到受限制的选项集的最佳层次策略。虽然这个策略可能不是最优的，但是收敛速度会快得多，因为考虑的选项较少，而且每个选项都可以跳过许多时间步骤。
要计划有选择，必须给一个选择模型，或者学习它们。学习选项模型的一种自然方法是将其表示为GVFs的集合(如前面部分所定义)，然后使用本书中介绍的方法学习GVFs。不难看出，对于期权模型的奖励部分，如何做到这一点。一个仅仅选择一个养狐业的累积量奖励(Ct = Rt),其政策选择的政策(π=πω)及其终止函数贴现率倍选择的终止功能(γ(s)=γ·γω(s))。获得真正的细胞核然后等于奖励选择模型的一部分,vπ,γ,C(s)= r(年代,ω),和这本书中描述的学习方法可以用来近似。选项模型的状态转换部分只是稍微复杂一点。需要为选项可能终止的每个状态分配一个GVF。我们不希望这些GVFs累积任何东西，除了在选项终止时，以及在终止处于适当状态时。这可以通过选择预测向状态s转变的GVF的累积量来实现。Ct =γ(St)·圣= s ?。选择GVF的策略和终止函数与选择模型的奖励部分相同。真正的GVF等于s?部分选项的状态转换关系模型、vπγ,C(s)= p(s ?|年代,再次ω),这本书的方法可以用来学习。虽然这些步骤看起来都很自然，但是将它们组合在一起(包括函数逼近和其他基本组件)是非常具有挑战性的，并且超出了当前的技术水平。


练习17.1本节给出了折现情况的选项，但是当使用函数逼近时，折现可能不适用于控制(第10.4节)。类似于(17.4)的层次策略的自然Bellman方程是什么，但对于平均奖励设置(第10.3节)?与(17.2)和(17.3)类似的选择模型的两个部分是什么?吗?


\section{观察和状态}

在这本书中，我们将学到的近似值函数(以及第13章中的策略)作为环境状态的函数。这是第一部分中所介绍的方法的一个重大限制，其中所学习的值函数被实现为一个表，以便任何值函数都可以被精确地逼近;这种情况相当于假设环境的状态完全被代理观察到。但在许多有趣的情况下，当然在所有自然智能的生命中，感官输入只提供有关世界状况的部分信息。有些物体可能被其他物体遮挡，或在代理后面，或在数英里之外。在这些情况下，潜在的环境状态的重要方面是不能直接观察到的，假设所学习的值函数是作为一个表在环境的状态空间中实现的，这是一个强烈的、不现实的、限制的假设。
我们在第二部分中开发的参数函数逼近框架的限制要小得多，而且可以说，根本没有限制。在第2部分中，我们假设所学习的值函数(和策略)是环境状态的函数，但是允许这些函数被参数化任意地限制。函数近似值包含了部分可观测性的重要方面，这有点令人吃惊，并没有得到广泛的承认。例如，如果有一个状态变量不可观测，那么可以选择参数化，使近似值不依赖于该状态变量。其效果就像状态变量不可观测一样。因此，参数化情况下得到的所有结果都适用于不改变的局部可观测性。在这个意义上，参数化函数逼近的情况包括局部可观测的情况。
然而，如果不更明确地对待局部可观测性，就无法研究许多问题。虽然我们不能在这里对它们进行全面的处理，但我们可以概述这样做所需的变化。有四个步骤。
首先，我们要改变这个问题。环境不会发出它的状态，而只会发出观察信号——这些信号取决于它的状态，但就像机器人的传感器一样，只能提供有关它的部分信息。为了方便起见，在不丧失通用性的前提下，我们假定奖励是观察的直接的、已知的功能(也许观察是一个矢量，而奖励是is的组成部分之一)。环境交互将没有显式状态或奖励,但只会是一个交变序列∈的操作和观察Ot∈O:
A0 O1 A1 O2 A2 O3 A3 O4…,
每一个结尾都有一个特殊的终端观察。

其次，我们可以从观察和行动的序列中恢复本书中使用的状态概念。让我们用“历史”这个词，加上符号“Ht”，来表示轨迹的初始部分，直到观察到:Ht。= A0, O1，…−1,不。历史代表了我们可以知道的关于过去的大部分内容，而不需要查看数据流之外的内容(因为历史是整个过去的数据流)。当然，历史会随着t而发展，会变得庞大而笨拙。国家的概念是对历史的一些简洁的总结，它和实际的历史预测未来一样有用。让我们弄清楚这到底意味着什么。作为历史的总结，国家必须是历史的函数，St = f(Ht)，并且要像预测整个历史一样对未来有用，它必须具有所谓的马尔可夫性质。形式上，这是函数f的性质。函数f有马尔可夫性质当且仅当h和h两个历史?它被f映射到相同的状态(f(h)=f(h?)下一个观测的概率也相同，

(h)= f(h ?)⇒公关{ Ot + 1 = o | Ht = h,在} = =公关{ Ot + 1 = o | Ht = h ?=一个},(17.5)∈o和∈如果f是马尔可夫,然后圣= f(Ht)是一个国家用这个词在这本书。因此，我们把它称为马尔可夫状态，以区别于那些虽是历史总结但却没有马尔可夫性质的状态(我们稍后将考虑)。
马尔可夫状态是预测下一次观测的良好基础(17.5)，但更重要的是，它也是预测或控制任何事物的良好基础。例如，让测试是未来可能发生的任何交替动作和观察的特定序列。例如,一个三步测试τ= a1o1a2来标示,o2,a3,o3。给定特定历史h的这个测试的概率定义为

p(τ| h)。=公关{ Ot + 1 = o1,Ot + 2 = o2,Ot + 3 = o3 | Ht = h = a1,a2 + 1 =,+ 2 = a3 }。
(17.6)

如果f是马尔可夫h?任何两个历史映射到相同的状态下,然后对任何测试τ的长度,其概率给定两个历史也必须是相同的:

(h)= f(h ?)⇒p(τ| h)= p(τ| h ?)。 					(17.7)

换句话说，马尔可夫状态总结了历史上决定任何测试概率所需的所有信息。事实上,它总结了所有必要的做任何预测,包括任何养狐业,表现最佳(如果f是马尔可夫,那么总有一个确定性的函数π这样选择
。=π(f(Ht))
最优)。
将强化学习扩展到局部可观测性的第三步是处理某些计算上的考虑。特别是，我们希望国家是历史的紧凑总结。例如，恒等函数完全满足马尔可夫f的条件，但仍然没有什么用处，因为相应的状态St=Ht会随着时间增长，变得难以处理，如前所述，但更根本的原因是它永远不会重现;代理不会


遇到相同的状态两次(在一个持续的任务中)，因此不能从表格学习方法中获益。我们希望我们的状态和马尔可夫一样紧凑。关于如何获得和更新状态也有类似的问题。我们不想要一个包含整个历史的函数f。相反，由于计算上的原因，我们更喜欢使用增量的、递归更新来获得与f相同的效果，该更新从St计算St+1，包含下一个数据增量，At和Ot+1:

圣+ 1。= u(圣,在Ot + 1),对所有t≥0, 					(17.8)
对于给定的第一状态S0。函数u被称为状态更新函数。例如，如果f是恒等式(St=Ht)，那么u仅仅通过在其上追加At和Ot+1来扩展St。给定f，总是有可能构造一个对应的u，但它可能不方便计算，而且，就像恒等例子中那样，它可能不会生成一个紧态。状态更新函数是处理部分可观测性的任何代理体系结构的中心部分。它必须是有效的可计算的，因为在状态可用之前不能进行任何操作或预测。图17.1给出了这样一个代理体系结构的总体图。
通过状态更新函数获得马尔可夫状态的一个例子是由流行的贝叶斯方法(称为部分可观测的MDPs)提供的。在这种方法中，假定环境具有一个定义良好的潜伏状态Xt，它作为环境观察的基础并产生环境观察，但是代理永远都不能使用它(不要与代理用来进行预测和决策的状态St混淆)。自然的马可夫州，St，对于一个POMDP是在历史的潜在状态下的分布，称为信念状态。具体性,假设通常的情况下,有一个有限数量的隐藏状态,Xt∈{ 1,2,。d }。那么信念状态就是向量St。=圣∈Rd和组件

圣[我]。=公关{ Xt =我| Ht },所有可能的潜在的国家我∈{ 1,2,。d }。

信念状态保持相同的大小(相同数量的组件)，但是不会增长。它还可以通过贝叶斯规则进行增量更新，假设一个人完全了解环境的内部工作。具体来说，belief-state更新函数的第i个组件是


u(年代,o)[我]。=
d ?
x=1 s[x]p(i, o|x, a) d。
x = 1
d ?
x ?= 1 s[x]p(x ?o | x,)
, 					(17.9)


阿,对所有一个∈∈啊,和信念状态与组件∈Rd年代[x],four-argument p的函数通常不是一个mdp(第三章),但类似POMDPs,潜伏的状态:p(x ?o | x,)。x =公关{ Xt = ?不= o | Xt−1 = x,−1 = }。这种方法在理论工作中很流行，并且有很多重要的应用，但是它的假设和计算复杂度很差，我们不推荐它作为人工智能的一种方法。
另一个Markov状态的例子是由预测状态表示，或PSRs提供的。PSRs解决了POMDP方法的不足，即其代理状态St的语义基于环境状态Xt，而这是从未被观察到的


一个


图17.1:概念代理体系结构，包括模型、计划器和状态更新函数。在这个案例中，世界接收行动A并发出观测结果。观察和行动的副本被国家更新函数u用来产生新的状态。新状态是策略和值函数的输入，生成下一个操作，也是输入
对计划者(和u)来说，最负责学习的信息流由虚线表示，虚线穿过它们所改变的方框。奖励R直接改变策略和价值函数。行为、奖励和状态改变了模型，它与计划者紧密合作，也改变了策略和价值功能。请注意,
规划器的操作可以与代理-环境交互解耦，而
其他进程应该按照这个交互的锁步骤操作，以跟上到达的时间
新数据。还要注意，模型和计划器并不直接处理观测，而是只处理观测
与u生成的状态一起，可以作为模型学习的目标。

因此很难了解。在PSRs和相关的方法中，代理状态的语义是建立在对未来的观察和行为的预测上的，这是很容易观察到的。在PSRs中，马尔可夫状态定义为d特别选择的“核心”测试的概率的d向量(17.6)。然后，通过状态更新函数u更新矢量，该函数类似于贝叶斯规则，但基于可观测数据的语义，这无疑使学习变得更容易。这种方法在许多方面都得到了扩展，包括末端测试、成分测试、强大的“光谱”方法，以及TD方法学习的闭环和时间抽象测试。一些最好的理论发展是关于可观察的操作员模型(OOMs)和顺序系统(Thon, 2017)。
在我们简要概述如何处理强化学习中的局部可观察性的第四个也是最后一个步骤是重新引入逼近。正如在第二部分的介绍中所讨论的，要雄心勃勃地接近人工智能，就必须接受近似。这对状态和值函数都是一样的。我们必须接受并使用一种近似的状态概念。近似状态在我们的算法中所起的作用和以前一样，所以我们继续使用记号St表示代理所使用的状态，尽管它可能不是Markov。

也许关于近似状态最简单的例子就是最近的观测，St。=不。当然，这种方法不能处理任何隐藏的状态信息。最好使用最后的k次观测和行动，St。=不,在−−1。。。−k,k≥1,这可以通过状态更新功能,只是变化的新数据和最古老的数据。这种k阶历史方法仍然非常简单，但是与试图直接使用单个即时观察作为状态相比，可以极大地提高代理的能力。
当马尔可夫特性(17.5)仅近似满足时会发生什么?不幸的是，当定义马尔可夫特性的一步预测变得更不准确时，长期预测性能会显著下降。较长期的测试、GVFs和状态更新函数可能都不太接近。短期和长期的近似目标是不同的，目前没有有效的理论保证。
尽管如此，仍然有理由认为本节概述的一般思想适用于近似情况。一般的想法是，对某些预测有利的状态对其他预测也有利(特别是，对一步预测足够的马尔可夫状态对所有其他预测也足够)。如果我们从马尔可夫案例的特定结果退回去，一般的想法与我们在第17.1节中讨论的多头脑学习和辅助任务相似。我们讨论了对辅助任务有利的表示方法如何也对主任务有利。综上所述，这些建议了一种对局部可观测性和表示学习的方法，在这种方法中，需要进行多个预测，并用于指导状态特征的构建。完全但不实用的马尔可夫性质所提供的保证被启发式取代，启发式认为，对某些预测有利的东西可能对其他预测有利。这种方法可以很好地利用计算资源。有了一台大型机器，人们就可以对大量的预测进行实验，可能更倾向于那些与最终感兴趣的或最容易可靠地学习的预测，或者根据其他一些标准。这里很重要的一点是，不要再手动选择预测了。代理应该这样做。这将需要一种预测的通用语言，以便代理能够系统地探索大量可能的预测，并从中筛选最有用的预测。
特别是，POMDP和PSR方法都可以应用于近似状态。状态的语义在生成状态更新函数时很有用，就像在这两种方法和k阶方法中一样。为了在状态中保留有用的信息，语义不需要是正确的。一些状态增强的方法，如Echo state networks (Jaeger, 2002)，保留了关于历史的几乎任意的信息，但仍然可以很好地执行。有很多可能性，我们期望在这个领域有更多的工作和想法。学习近似状态的状态更新函数是强化学习中出现的表现学习问题的主要部分。

\section{设计奖励信号}


强化学习相对于监督学习的一个主要优势是强化学习不依赖于详细的教学信息:产生奖励信号不依赖于知道行为者的正确行为是什么。但是，强化学习应用程序的成功与否，很大程度上取决于奖励信号对应用程序设计者的目标的框架程度，以及信号对实现目标进展的评估程度。基于这些原因，设计奖励信号是任何强化学习应用的关键部分。
通过设计一个奖励的信号我们指的是设计一个代理环境的一部分,负责计算每个标量奖励Rt,并将其发送给代理在t。每次我们讨论术语第14章的末尾,我们说Rt更像是一个信号生成在动物的大脑比像一个对象或事件在动物的外部环境。我们大脑中产生这些信号的部分经过了数百万年的进化，以很好地适应我们的祖先在努力向后代传播基因时所面临的挑战。因此，我们不应该认为设计一个好的奖励信号总是一件容易的事情!
一个挑战是设计一个奖励信号，这样当一个代理学习时，它的行为就会接近并最终达到应用程序设计者真正想要的。如果设计人员的目标简单且容易识别，比如找到一个定义明确的问题的解决方案，或者在定义明确的游戏中获得高分，那么这很容易实现。在这样的情况下，通常是根据代理在解决问题上的成功或改进分数的成功来奖励它。但有些问题涉及目标，这些目标很难转化为奖励信号。当问题需要代理熟练地执行复杂的任务或一组任务时，这一点尤为明显，比如需要一个有用的家用机器人助理。此外，强化学习代理可以发现意想不到的方法来让他们的环境产生回报，其中一些可能是不受欢迎的，甚至是危险的。对于任何基于优化的方法，如强化学习，这都是一个长期且关键的挑战。我们将在本书的最后部分17.6节中更多地讨论这个问题。
即使有一个简单而容易识别的目标，稀疏奖励的问题也经常出现。频繁地提供不为零的奖励以使代理能够实现一次目标，更不用说学会如何在多个初始条件下有效地实现目标，这可能是一个艰巨的挑战。显然值得引发奖励的状态-行动对可能是很少的，而且在实现目标的过程中标记进展的奖励可能是很少的，因为进展是困难的，甚至是不可能被发现的。代理人可能会漫无目的地游荡很长一段时间(明斯基1961年将其称为“高原问题”)。
在实践中，设计奖励信号通常留给非正式的试错搜索，以产生可接受的结果。如果代理无法学习，学习太慢，或者学习错误的东西，那么设计师就会调整奖励信号，再试一次。为此，设计师根据他或她试图转化为奖励信号的标准来判断代理人的表现，以便代理人的目标与他或她自己的目标匹配。如果学习太慢，设计者可能会尝试设计一个非稀疏的奖励信号，有效地指导整个agent与环境的交互过程中的学习。

通过奖励实现子目标的代理来解决稀疏的奖励问题是很诱人的，设计者认为这是实现总体目标的重要途径。但是，用善意的补充奖励来增加奖励信号，可能会导致代理的行为与预期的不同;代理最终可能根本没有实现总体目标。提供这种指导的更好方法是不理会奖励信号，而是通过对其最终应该是什么的初步猜测来增强价值函数近似，或者通过对其特定部分的初步猜测来增强它。例如,假设人愿意提供v0:S→R作为初始猜测真正的最优值函数v∗,,一个是使用线性函数近似特征x:S→Rd。那么一个定义初始值函数逼近
v̂(s,w)。= w ? x(s)+ v0(s), 					(17.10)
和往常一样更新权重w。如果初始权值向量为0，则初始值函数为v0，而渐近解的质量则由特征向量决定。这种初始化可以对任意非线性逼近器和任意形式的v0进行，尽管不能保证总是能加速学习。
针对稀疏奖励问题的一种特别有效的方法是心理学家b·f·斯金纳(B. F. Skinner)介绍的整形技术，并在第14.3节中进行了描述。这种技术的有效性依赖于这样一个事实:稀疏奖励问题不仅仅是奖励信号的问题;它们也存在于代理的策略中，以防止代理经常遇到奖励状态。塑造包括在学习过程中改变奖励信号，从一个并不稀疏的奖励信号开始，考虑到代理的初始行为，并逐渐将其修改为适合于原始兴趣问题的奖励信号。每一次修改都要进行，以便在给定代理当前行为的情况下经常对其进行奖励。代理面临一系列越来越困难的强化学习问题，在每个阶段学到的东西使得下一个更难的问题相对容易，因为代理现在遇到奖励的频率比以前没有遇到更容易的问题的经验时要高。这种塑形是训练动物的一种基本技术，在计算强化学习中也很有效。
如果一个人不知道奖励应该是什么，但又有另一个人，也许是一个人，他已经是这个任务的专家，他的行为可以被观察到?在这种情况下，我们可以使用不同的方法，如“模仿学习”、“示范学习”和“学徒学习”。“这里的想法是从专家代理中获益，但保留最终表现更好的可能性。”从专家的行为中学习可以通过直接学习监督学习或者利用所谓的“反向强化学习”提取奖励信号，然后使用带有奖励信号的强化学习算法来学习策略。Ng和Russell(2000)研究的逆强化学习任务是尝试从专家的行为中恢复专家的奖励信号。这不能完全做到这一点，因为对于许多不同的奖励信号(例如给予所有国家和行动相同的奖励的任何奖励信号)，政策是最优的，但也有可能找到合理的奖励信号。不幸的是，需要强有力的假设，包括对环境动力学和特征向量的了解

奖励信号是线性的。该方法还需要多次完全解决问题(例如，通过动态编程方法)。尽管存在这些困难，Abbeel和Ng(2004)认为逆强化学习方法有时可以比监督学习更有效地从专家的行为中获益。
找到一个好的奖励信号的另一种方法是自动地尝试和错误地搜索我们上面提到的一个好的信号。从应用的角度来看，奖励信号是学习算法的一个参数。与其他算法参数一样，通过定义一个可行候选空间并应用优化算法，可以自动搜索一个好的奖励信号。优化算法通过运行带有该信号的强化学习系统，对每个候选奖励信号进行若干步的评估，然后通过一个“高级”目标函数对总体结果进行评分，该目标函数旨在编码设计者的真实目标，而忽略了agent的局限性。奖励信号甚至可以通过在线梯度上升得到改善，其中梯度是高级目标函数的梯度(Sorg, Lewis, and Singh, 2010)。将这种方法与自然世界联系起来，优化高级目标函数的算法类似于进化，高级目标函数是由动物后代的数量决定的进化适应性。
使用这种双层优化方法的计算实验——一种类似于进化，另一种类似于个体的强化学习——证实了仅仅凭直觉并不总是足以设计出好的奖励信号(Singh, Lewis，和Barto, 2009)。由高级目标函数评估的增强学习代理的性能可以非常敏感地关注代理的奖励信号的细节，这些细节是由代理的限制和它的行为和学习环境所决定的。这些实验也证明了agent的目标不应该总是与agent的设计者的目标一致。
乍一看，这似乎有悖常理，但代理商可能不可能实现设计师的目标，无论其奖励信号是什么。代理必须在各种各样的约束条件下学习，比如有限的计算能力，有限的环境信息，或者有限的学习时间。当存在这样的约束时，学习实现与设计师目标不同的目标有时会比直接追求目标更接近设计师的目标(Sorg, Singh, and Lewis, 2010;Sorg,2011)。在自然界中很容易找到这样的例子。因为我们不能直接评估大多数食物的营养价值，进化——我们的奖励信号的设计者——给了我们一个奖励信号，让我们去寻找特定的口味。虽然肯定不是绝对正确的(确实，在某些方面与祖先环境不同的环境中可能有害)，但这弥补了我们的许多局限性:我们的感官能力有限，我们可以学习的时间有限，以及通过个人实验找到健康饮食的风险。同样，因为动物不能观察自己的进化适应性，所以客观的功能不能作为学习的奖励信号。相反，进化提供了对可观察到的进化适应性预测者敏感的奖励信号。
最后，请记住，强化学习代理并不一定像一个完整的

生物体或机器人;它可以是一个更大行为系统的组成部分。这意味着奖励信号可能会受到行为主体内部的东西的影响，比如动机状态，记忆，想法，甚至是幻觉。奖励信号也可能取决于学习过程本身的属性，比如衡量学习进展的程度。使奖励信号敏感信息等内部因素这些可以让一个代理学习如何控制它的“认知结构”,以及掌握知识和技能,很难从一个奖励的信号,只取决于外部事件。类似这样的可能性导致了“内部激励强化学习”的概念，我们将在下一节的最后简要地讨论这个概念。


\section{剩余问题}

在这本书中，我们提出了人工智能强化学习方法的基础。粗略地说，这种方法是基于无模型和基于模型的方法共同工作的，如第8章的Dyna体系结构，结合第二部分中所开发的函数逼近。重点是在线和增量算法，我们认为它们甚至是基于模型的方法的基础，以及如何在非策略培训情况下应用这些算法。后者的全部基本原理仅在最后一章中提出。我们一直呈现off-policy学习作为一个吸引人的方式来处理探索/利用困境,但只有在这一章我们讨论了同时学习许多不同的辅助任务养狐业和学习世界等级的temporally-abstract选择模型,两者都涉及off-policy学习。正如我们在整本书中所指出的，还有许多有待解决的问题，本章讨论的更多研究方向也证明了这一点。但是假设我们很慷慨，对我们在书中所做的一切以及到目前为止在这一章中所概述的一切都给出了大致的轮廓。之后还剩下什么?当然我们不能确定需要什么，但我们可以做一些猜测。在这一节中，我们强调了另外六个问题，在我们看来，这些问题仍然需要在未来的研究中加以解决。
首先，我们仍然需要强大的参数函数逼近方法，这些方法在完全增量和在线设置中工作良好。基于深度学习和ANNs的方法是这一方向上的主要步骤，但是，仍然只能在大型数据集上进行批量训练，通过广泛的离线自我扮演进行训练，或者从多个代理在同一任务上的交叉经验中学习。这些设置和其他设置都是解决当今深度学习方法的基本限制的方法。深度学习方法很难在增量式的在线环境中快速学习，而这正是本书强调的强化学习算法最自然的地方。这个问题有时被描述为“灾难性干扰”或“相关数据”。“当新事物被发现时，它往往会取代以前学过的东西，而不是增加它，结果是旧的学习的益处消失了。”诸如“重播缓冲区”之类的技术通常用于保留和重播旧数据，以便其好处不会永久丧失。诚实的评价是，目前的深度学习方法不太适合在线学习。我们没有理由认为这种限制是不可克服的，而是认为是算法

这种方法，虽然同时保留了深度学习的优势，但还没有被设计出来。目前的大多数深度学习研究都是针对这一限制而不是消除它。
第二(也许是紧密相关的)，我们仍然需要学习特性的方法，以便后续学习能够很好地归纳。这个问题是一个普遍问题的实例，被称为“表示学习”，“建设性诱导”和“元学习”——我们如何使用经验，而不仅仅是学习一个给定的期望函数，而是学习归纳偏差，从而使未来的学习变得更好，从而更快?这是一个古老的问题，可以追溯到上世纪五六十年代人工智能和模式识别的起源。这样的年龄应该会让人踌躇一下。也许没有解决办法。但同样有可能的是，找到解决办法并证明其有效性的时机尚未到来。如今，机器学习的规模比过去大得多，良好的表示学习方法的潜在好处也变得更加明显。我们注意到，自2013年以来，每年都有一个新的年度会议——国际学习代表大会(International Conference on Learning representative)——探讨这一问题和相关话题。在强化学习环境中探索表现学习也不太常见。强化学习为这个旧问题带来了一些新的可能性，例如第17.1节讨论的辅助任务。在强化学习中，表征学习的问题可以通过学习第17.3节中讨论的状态更新函数来确定。
第三，我们仍然需要可伸缩的方法来使用学习的环境模型进行规划。在AlphaGo Zero和计算机国际象棋等应用程序中，规划方法已经被证明是非常有效的，在这些应用程序中，环境模型是根据游戏规则确定的，或者可以由人类设计师提供。但是，完全基于模型的强化学习(环境模型从数据中学习，然后用于规划)的情况非常少见。第8章中所描述的Dyna系统就是一个例子，但是正如这里所描述的，并且在后续的工作中，它使用了一个没有函数逼近的表格模型，这极大地限制了它的适用性。只有很少的研究包括学习过的线性模型，甚至更少的研究包括使用选项的时间抽象模型，如第17.2节所讨论的。
在使用已学习的模型进行规划之前，需要做更多的工作。例如，学习模型需要有选择性，因为模型的范围会强烈地影响规划效率。如果一个模型关注最重要的选项的关键后果，那么规划可以是高效和快速的，但是如果一个模型包含不太可能被选择的选项的不重要后果的细节，那么规划可能几乎是无用的。环境模型的建立应考虑其状态和动态，以优化规划过程为目标。模型的各个部分应该被不断地监测，以达到它们对计划效率的贡献或减损程度。这个领域还没有解决这个复杂的问题，也没有设计考虑到它们的影响的模型学习方法。

有些人会说深度学习解决了这个问题，例如，第16.5节中描述的DQN说明了一个解决方案，但我们并不信服。目前还没有什么证据能证明这一点。
学习单独解决代表学习问题是一种普遍而有效的方法。

在未来的研究中需要解决的第四个问题是自动选择代理工作的任务，并使用这些任务来构建其开发能力。在机器学习中，设计人员通常会设置学习代理希望掌握的任务。因为这些任务是预先知道的，并且是固定的，所以可以将它们构建到学习算法代码中。但是，展望未来，我们希望代理能够对它应该尝试掌握的任务做出自己的选择。这些可能是已经知道的特定的总体任务的子任务，或者它们可能是为了创建构建块，从而允许更高效地学习代理在将来可能面临的许多不同任务，但是目前还不知道。
这些任务可能类似于第17.1节中讨论的辅助任务或GVFs，或者按照第17.2节讨论的选项解决的任务。例如，在形成GVF时，累积量、策略和终止函数应该是什么?当前的艺术状态是手动选择这些任务，但更大的权力和一般性将来自于自动地做出这些任务选择，特别是当它们来自于代理先前构建的由表示学习或以前的子问题的经验所构建的结果时。如果GVF设计是自动化的，那么设计选择本身就必须显式地表示出来。与其让设计人员考虑并将任务选择嵌入到代码中，还不如让它们在机器本身中进行设置、更改、监视、过滤和自动搜索。然后，任务可以分层地构建在其他任务上，就像ANN中的特性一样。任务就是问题，ANN的内容就是这些问题的答案。我们期望有一个完整的问题层次来匹配现代深度学习方法提供的答案层次。
我们想强调的第五个问题是行为和学习之间的相互作用通过好奇心的计算模拟。在这一章中，我们设想了一个场景，在这个场景中，许多任务同时被学习，使用非策略方法，从相同的经验流中学习。所采取的行动当然会影响这一经验流，而这反过来又将决定发生了多少学习，以及学习了哪些任务。当奖励不存在，或者不受行为的强烈影响时，代理可以自由地选择在某种意义上最大限度地学习任务的行为，也就是说，使用学习进度的某种度量作为内部或“内在”的奖励，实现好奇心的计算形式。除了测量学习进度外，内在奖励除了其他可能性外，还可以表示收到了意外的、新奇的或其他有趣的输入，或者可以评估代理在其环境中引起变化的能力。在这些方法中产生的内在奖励信号可以由一个代理通过定义辅助任务、GVFs或选项来为自己设置任务，如上面所讨论的，这样通过这种方式学习的技能可以帮助代理者掌握未来任务的能力。结果是一种类似游戏的计算模拟。对这种内在奖励信号的使用进行了许多初步研究，在这一一般领域仍有令人兴奋的未来研究课题。
在未来的研究中需要注意的最后一个问题是开发方法，使在物理环境中嵌入增强学习因子是可以接受的安全的。这是未来研究最紧迫的领域之一，我们将在下一节中进一步讨论。

\section{人工智能的未来}

上世纪90年代中期，当我们正在撰写这本书的第一版时，人工智能正在取得重大进展，并对社会产生了影响，尽管人工智能的前景仍是推动发展的主要因素。机器学习是这一观点的一部分，但它尚未成为人工智能不可或缺的一部分。到今天，这一承诺已经转变为正在改变千百万人生活的应用程序，而机器学习作为一项关键技术已经有了自己的意义。当我们写第二版时，人工智能中一些最显著的发展已经涉及到强化学习，最著名的是“深层强化学习”——用深层人工神经网络的函数逼近来强化学习。我们正处于一波人工智能在现实世界中的应用浪潮的开端，其中许多应用将包括强化学习，深入地，否则，将以难以预测的方式影响我们的生活。
但是，大量成功的实际应用并不意味着真正的人工智能已经到来。尽管在许多领域取得了巨大的进步，人工智能与人类甚至其他动物的智能之间的鸿沟仍然巨大。在某些领域，甚至是像Go这样令人生畏的领域，都可以实现超人的性能，但开发像我们这样的系统仍然是一个巨大的挑战，这些系统是完整的、具有一般适应性和解决问题的技能、情感复杂性、创造力以及从经验中快速学习的能力。通过与动态环境的交互作用，强化学习，随着它在未来的发展，将成为具有这些能力的代理的一个重要组成部分。
强化学习与心理学和神经科学的联系(第14章和第15章)强调了它与人工智能的另一个长期目标的相关性:阐明关于大脑的基本问题以及它是如何从大脑中浮现出来的。强化学习理论已经有助于我们理解大脑的奖励、动机和决策过程，我们有充分的理由相信，通过它与计算精神病学的联系，强化学习理论将有助于治疗包括药物滥用和成瘾在内的精神障碍的方法。
强化学习对未来的另一个贡献是对人类决策的帮助。通过在模拟环境中强化学习而产生的政策，可以在教育、医疗、交通、能源和公共部门资源分配等领域为人类决策者提供建议。特别相关的是强化学习的关键特征，它考虑到决策的长期后果。这在《西洋双陆棋》和《Go》这类游戏中是非常明显的，在这些游戏中，强化学习的一些最令人印象深刻的结果已经被证明，但它也是影响我们的生活和我们的星球的许多高风险决策的属性。强化学习遵循指导人类决策的相关方法，这些方法在过去由许多学科的决策分析师开发。利用先进的函数逼近方法和大量的计算能力，增强学习方法有可能克服将传统的决策支持方法扩展到更大、更复杂的问题上的一些困难。

人工智能的快速发展已导致警告，人工智能对我们的社会，甚至对人类本身构成严重威胁。著名科学家和人工智能先驱Herbert Simon在2000年CMU的陶器研讨会上的演讲中预见到了我们今天听到的警告(Simon, 2000)。他永恒的承诺和危险之间的冲突的任何新知识,提醒我们普罗米修斯,希腊神话的现代科学的英雄,谁偷了来自上帝的火,造福人类,和潘多拉的盒子可以打开一个小和无辜的行动释放世界上数不清的危险。尽管西蒙承认这种冲突是不可避免的，但他敦促我们认识到，作为未来的设计者，而不仅仅是旁观者，我们所做的决定会使规模向普罗米修斯的方向倾斜。强化学习确实如此，它可以造福社会，但如果不小心使用，也可能产生不良后果。因此，涉及强化学习的人工智能应用的安全性是一个值得关注的课题。
强化学习代理可以通过与现实世界或者与现实世界的某个部分进行交互，或者通过这两个经验来源的混合来学习。模拟器提供了一个安全的环境，在这个环境中，一个代理可以探索和学习，而不会对自身或环境造成真正的损害。在大多数当前应用程序中，策略是从模拟经验中学习的，而不是直接与真实世界交互。除了避免不良的实际后果,从模拟学习经验可以为学习提供无限的数据,通常比需要以较低的成本获得实际经验,因为比实时模拟通常运行更快,更快地学习经常会出现比它依赖于真实的体验。
然而，强化学习的全部潜力需要强化学习媒介嵌入到真实世界的体验流中，在那里它们在我们的世界中行动、探索和学习，而不仅仅是在它们的世界中。毕竟，强化学习算法——至少我们在本书中关注的那些算法——是为了在网上学习而设计的，它们模仿了动物如何在非静止和敌对环境中生存的许多方面。在现实世界中嵌入强化学习因子可以实现人工智能增强和扩展人类能力的承诺。
主要原因希望强化学习代理行为和学习在现实世界中是它常常是困难的,有时是不可能的,来模拟现实世界的经验和足够的忠诚产生的政策,是否得到了强化学习或其他方法,工作的好时候安全指导实际行动。这尤其适用于那些动态依赖于人类行为的环境，如教育、医疗、交通和公共政策领域，这些领域肯定可以从改进的决策制定中获益。然而，真正需要注意的是，对于现实世界中的嵌入式代理，关于人工智能潜在危险的警告。
其中一些警告与强化学习特别相关。因为强化学习是基于优化的，它继承了所有优化方法的优缺点。缺点是设计目标函数，或者在强化学习中设计奖励信号，这样优化就产生了

避免不希望的结果。我们在第17.4节中说过，强化学习代理可以发现意想不到的方法来让他们的环境产生回报，其中一些可能是不受欢迎的，甚至是危险的。当我们指定我们希望一个系统只能间接学习的内容时，就像我们在设计一个强化学习系统的奖励信号时一样，我们将不知道在它的学习完成之前，代理将如何完成我们的愿望。这并不是强化学习的新问题;它在文学和工程学上都有着悠久的历史。例如，在歌德的诗歌《魔法师的学徒》(歌德，1878)中，《学徒》用魔法使一把扫帚完成了取水的工作，但由于学徒对魔法知识的不充分，导致了意想不到的洪水。在工程背景下，控制论的创始人诺伯特·维纳(Norbert Wiener)半个多世纪前通过讲述“猴爪”的超自然故事(维纳，1964)来警告这个问题:“……”它赋予你所要求的，而不是你应该要求的或你想要的”(第59页)。尼克·博斯特罗姆(Nick Bostrom)(2014)在《现代语境》(modern context)中也详细讨论了这个问题。任何有强化学习经验的人都可能看到他们的系统发现了意想不到的方法来获得大量奖励。有时意想不到的行为是好的:它以一种新的方式解决问题。在其他情况下，代理学习的内容违反了系统设计者可能从未考虑过的考虑。如果一个代理人在现实世界中采取行动，而没有机会对其行为进行审查，或者很容易打断其行为，那么仔细设计奖励信号是必不可少的。
尽管可能会产生意想不到的负面影响，优化已经被工程师、架构师和其他设计对世界产生积极影响的人使用了数百年。由于优化方法的应用，我们在我们的环境中有很多优点。为了降低优化的风险，已经开发了许多方法，如添加硬约束和软约束、将优化限制为鲁棒和风险敏感的策略，以及使用多个目标函数进行优化。其中一些方法已经被用于强化学习，需要更多的研究来解决这些问题。确保增强学习代理的目标与我们自己的目标一致仍然是一个挑战。
另一个挑战是，强化学习主体在现实世界中的行为和学习不仅仅是关于他们最终可能学到的东西，而是关于他们在学习时的行为。如何确保代理获得足够的经验来学习高性能策略，同时不损害其环境、其他代理或其自身(或者更实际地说，同时将损害的可能性保持在可接受的低水平)?这个问题对强化学习也不是新奇的或独特的。嵌入式强化学习的风险管理和缓解类似于控制工程师从使用自动控制开始就必须面对的情况，在这种情况下，控制器的行为可能会产生不可接受的、可能是灾难性的后果，如控制飞机或精细的化学过程。控制应用程序依赖于仔细的系统建模、模型验证和广泛的测试，并且有一个高度发达的理论体系，旨在确保在控制系统的动态不完全被控制的情况下，为使用而设计的自适应控制器的收敛性和稳定性。理论保证从来都不是铁板钉钉的，因为它们依赖于数学基础上的假设的有效性，但如果没有这个理论，再加上风险管理和缓解实践，自动控制——自适应和其他控制——就不会是这样

在改进我们所依赖的过程的质量、效率和成本效益方面，它今天是有益的。未来强化学习研究最紧迫的领域之一是采用和扩展控制工程中开发的方法，使强化学习剂充分嵌入物理环境中成为可接受的安全方法。
最后，我们回到西蒙的呼吁，让我们认识到我们是未来的设计师，而不仅仅是观众。通过我们作为个人所做的决定，以及我们对社会治理方式施加的影响，我们可以努力确保一项新技术可能带来的好处超过它可能带来的危害。在加强学习的情况下，有充分的机会可以做到这一点，这有助于提高我们星球上的生活质量、公平和可持续性，但这也能释放新的危险。目前的威胁是人工智能应用所造成的工作岗位的流失。仍然有充分的理由相信人工智能的好处可以超过它所造成的破坏。在安全性方面，强化学习可能产生的危害与在优化控制方法的相关应用中成功管理的危害并没有完全不同。随着增强学习在未来的应用程序中逐渐进入现实世界，开发人员有义务遵循为类似技术进化的最佳实践，同时扩展它们以确保Prometheus占上风。

\section{书目的和历史的言论}

17.1一般价值函数首先由Sutton和同事明确识别
(萨顿,1995;萨顿et al .,2011;莫达耶(Modayil, White and Sutton, 2013)。Ring (in preparation)用GVFs(“预测”)开发了一项广泛的思想实验，尽管还没有发表出来，但这一实验已经很有影响力了。
Jaderberg等人(2017)首次演示了增强学习中的多脑学习。Bellemare、Dabney和Munos(2017)的研究表明，预测更多关于奖励分配的事情可以显著地加速学习以优化预期，这是辅助任务的一个例子。此后，许多人开始从事这方面的研究。
经典条件作用的一般理论是学习过的预测，以及对预测的内在的、反射性的反应，这在心理学文献中并没有得到明确的阐述。Modayil和Sutton(2014)将其描述为机器人和其他代理的工程方法，称其为“巴甫洛夫控制”，暗指其根源于古典条件作用。

17.2作为选择的临时性抽象的行动过程的形式化是内含的
由Sutton、Precup和Singh(1999)指导，以Parr(1998)和Sutton(1995)的前期工作为基础，以及关于半千年发展目标的经典工作(例如，参见Puterman, 1994)。Precup(2000)博士论文充分发展了期权的思想。这些早期工作的一个重要限制是，它们没有使用函数逼近来处理偏离政策的情况。一般情况下，期权内学习需要策略外学习，这在当时无法用函数近似可靠地完成。虽然现在我们使用函数近似有很多稳定的非政策学习方法，但在这本书出版的时候，它们与期权思想的结合并没有得到显著的探索。Barto和Mahadevan(2003)和亨斯特(2012)回顾了时间抽象的选项形式主义和其他方法。
使用GVFs实现选项模型之前没有描述过。我们的报告使用Modayil、White和Sutton(2014)引入的技巧来预测政策终止时的信号。
在学习函数近似的选项模型的少数作品中，有Sorg和Singh(2010)、Bacon、Harb和Precup(2017)。
在文献中，选择和期权模型到平均奖励设置的扩展还没有被开发出来。

17.3 Monahan(1982)很好地介绍了POMDP方法。psr
实验由Littman, Sutton和Singh(2002)提出。OOMs是由Jaeger(1997, 1998, 2000)提出的。序列系统统一了PSRs、oom和其他许多工作，在Michael Thon的博士论文(2017)中被引入;索恩和Jaeger,2015)。时间关系网络的扩展。


由Tanner(2006)开发;Sutton和Tanner, 2005)然后扩展到选项(Sutton, Rafols，和Koop, 2006)。
用非马尔可夫状态表示的强化学习理论是由辛格、贾可拉和约旦明确提出的(1994年);Jaakkola, Singh，和Jordan, 1995)。Chrisman(1992)、McCallum(1993, 1995)、Parr和Russell(1995)、Littman、Cassandra和Kaelbling(1995)以及Lin和Mitchell(1992)提出了早期的增强学习方法。

17.4在强化学习中包含建议和教学的早期努力包括
Lin (1992)， Maclin和Shavlik (1994)， Clouse (1996)， Clouse和Utgoff(1992)。
Skinner的整形不应该与Ng、Harada和Russell(1999)引入的“基于潜力的整形”技术混淆。他们的技术已经被Wiewiora(2003)证明等同于提供值函数的初始逼近的简单思想，如(17.10)。

17.5我们推荐Goodfellow, Bengio和Courville(2016)的《铁饼》
今天的深度学习技巧。McCloskey和Cohen(1989)、Ratcliff(1990)和French(1999)提出了对ANNs进行灾难性干预的问题。重播缓冲区的概念是Lin(1992)提出的，并在Atari游戏系统的深度学习中得到了突出的应用(第16.5节，Mnih等，2013,2015)。
明斯基(1961)是最早发现表征学习问题的学者之一。
在少数几个考虑计划学习的作品中，大约有Kuvayev和Sutton(1996)、Sutton、Szepesvari、Geramifard、Bowling(2008)、Nouri和Littman(2009)、Hester和Stone(2012)。
在人工智能中，有必要在模型构建中进行选择，以避免规划放缓。一些经典的作品是由明顿(1990)、塔姆贝(Tambe)、纽维尔(Newell)和罗森布鲁姆(Rosenbloom)(1990)合著的。Hauskrecht、Meuleau、Kaelbling、Dean和Boutilier(1998)在具有确定性选项的MDPs中显示了这种影响。
施米德胡贝尔(1991a, b)提出，如果奖励信号是一个代理环境模型改善速度的函数，那么好奇号之类的东西会产生什么结果。Klyubin、Polani和Nehaniv(2005)提出的赋权函数是一种信息论上的对代理人控制环境能力的度量，这种能力可以作为一种内在的奖励信号。Baldassarre和Mirolli(2013)是研究人员从生物学和计算角度研究内在奖励和动机的一组贡献，包括对“内在激励的强化学习”的观点，用Singh、Barto和Chentenez(2004)引入的术语。参见Oudeyer和Kaplan(2007)、Oudeyer、Kaplan和Hafner(2007)和Barto(2013)。
