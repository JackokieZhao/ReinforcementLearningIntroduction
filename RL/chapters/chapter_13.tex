\chapter{第十三章 策略梯度方法}
\begin{summary}
	在这一章中，我们考虑一些新的东西。到目前为止，这本书中几乎所有的方法都是行动价值方法;他们学习行动的价值，然后根据估计的行动价值选择行动;如果没有行动价值的估计，他们的政策甚至不会存在。在本章中，我们考虑的方法是学习参数化策略，该策略可以在不考虑值函数的情况下选择操作。值函数仍然可以用于学习策略参数，但不需要进行操作选择。我们使用符号θ∈Rd吗?用于策略的参数向量。因此我们写π(|年代,θ)=公关{ = |圣= s,θt =θ}的概率是采取行动在时间t给定状态的环境在时间t s参数θ。如果一个方法使用了价值函数,那么价值函数的权向量w来标示∈Rd像往常一样,在v̂(s,w)。
	在这一章里,我们考虑学习策略参数的方法基于一些标量性能的梯度测量J(θ)的政策参数。这些方法试图最大化性能，所以它们的更新近似于J的梯度上升:
	
	θt + 1 =θt +α?∇J(θt), 					(13.1)
	在哪里?∇J(θt)∈Rd随机估计的预期接近性能的梯度测量的参数θt。所有遵循这个通用模式的方法都称为策略梯度方法，不管它们是否也学习一个近似值函数。学习逼近策略和值函数的方法通常被称为actor - critics方法，其中‘actor’是学习策略的参考，‘批评家’是学习的值函数，通常是状态值函数。首先，我们讨论情景情形，其中性能被定义为参数化策略下的开始状态的值，然后再考虑继续情形，其中性能被定义为平均回报率，如第10.3节所示。最后，我们可以用非常相似的术语表示这两种情况的算法。
	
	唯一的例外是2.8节的梯度班迪特算法。事实上，这一节经历了许多相同的步骤，在单状态强盗案例中，当我们在这里进行完整的MDPs时。复习这一节将是充分理解本章的充分准备。
\end{summary}

\section{政策近似值及其优点}
在策略梯度方法,策略可以参数化以任何方式,只要π(|年代,θ)是可微的参数,也就是说,只要∇π(|年代,θ)(列向量的偏导数的π(|年代,θ)关于θ的组件)为所有∈年代存在并且是有限的,一个∈(s),和θ∈Rd吗?。在实践中，为了确保探索，我们通常要求政策永远不会成为决定性的(即:π(|年代,θ)∈(0,1),对所有的年代,一个,θ)。在本节中，我们将介绍离散操作空间中最常见的参数化，并指出它相对于操作值方法的优点。基于策略的方法还提供了处理连续操作空间的有用方法，我们将在后面的第13.7节中对此进行描述。
如果操作空间是离散的,而不是太大,那么自然和常见的一种参数化是形成参数化数值偏好h(s,a,θ)∈R为每个政府行动。在每个状态中，具有最高偏好的操作被给定的被选择的最高概率，例如，根据指数软最大值分布:
 
在e≈2.71828是自然对数的基础。注意这里的分母是要求每个状态的动作概率之和为1。我们把这种政策参数化称为行动偏好中的软最大值。
操作首选项本身可以被任意参数化。例如,他们可能被深度计算人工神经网络(ANN),θ是所有网络的连接权值的向量(如16.6节中描述的AlphaGo系统)。或者偏好可以是线性的，
h(年代,θ)=θ?x(,), 					(13.3)
使用特征向量x(,)∈Rd ?由第9章描述的任何方法构造。
参数化的一个优点政策根据soft-max行动偏好是近似政策可以达到一个确定的政策,而ε-greedy行动选择在行动总是有一个ε值的概率随机选择一个行动。当然，可以根据基于动作值的软最大值分布进行选择，但仅凭这一点不能使策略接近确定性策略。相反，行动价值估计将收敛到它们相应的真实值，它们之间的差异是有限的，可以转化为除0和1以外的特定概率。如果soft-max分布包括温度参数,然后随着时间的推移,温度可以减少方法决定论,但是在实践中很难选择减少计划,甚至初始温度,没有更多的先验知识的真正的行动值比我们愿意承担。行动偏好是不同的，因为它们不接近特定的值;相反，他们被迫制定最优的随机政策。如果最优策略是确定性的，那么最优操作的首选项将被驱动到比所有次最优操作(如果参数化允许的话)高无穷大。

根据action偏好中的soft-max对策略进行参数化的第二个优点是，它允许选择具有任意概率的操作。在具有显著函数逼近的问题中，最佳逼近策略可能是随机的。例如，在信息不完全的纸牌游戏中，最优的玩法通常是用特定的概率做两件不同的事情，比如在扑克中虚张声势。行为-价值方法没有找到随机最优策略的自然方法，而策略近似方法可以，如示例13.1所示。
 

策略参数化可能比动作值参数化有一个最简单的优点，那就是策略可能是一个更简单的近似函数。问题在于他们的政策和行动价值职能的复杂性。对于某些人来说，动作-值函数更简单，因此更容易近似。对其他人来说，政策更简单。在后一种情况下，基于策略的方法通常会学习得更快，并产生更好的渐近策略(如俄罗斯方块);看到̧imşek,藻类́等,和Kothiyal,2016)。

最后，我们注意到，政策参数化的选择有时是向强化学习系统注入有关政策预期形式的先验知识的好方法。这通常是使用基于策略的学习方法的最重要的原因。

练习13.1利用你对gridworld的知识和它的动力学来确定一个精确的符号表达式，以确定选择正确动作的最佳概率。
例13.1。 					?


\section{政策梯度定理}

除了政策参数的实际优势ε-greedy行动选择,还有一个重要的理论优势。连续概率变化政策参数化行动顺利学习参数的函数,而在ε-greedy选择任意小的行动概率可能会发生戏剧性的变化估计行动值的变化,如果变化导致不同的操作有极大价值。很大程度上由于这种更强的收敛性，政策梯度方法比行动价值方法更有效。特别是，政策依赖于参数的连续性使得政策梯度方法能够近似梯度上升(13.1)。
情景和持续的情况下定义的业绩衡量,J(θ)是不同的,因此在某种程度上必须分别对待。尽管如此，我们将尝试统一地呈现这两种情况，并开发一个符号，以便主要的理论结果可以用一组方程来描述。
在这一节中，我们讨论情节的情况，我们将性能度量定义为情节开始状态的值。我们可以简化表示法而不丢失任何有意义的通用性，假设每一集都以某种特定的(非随机的)状态s0开始。然后，在情景性的例子中我们把性能定义为

J(θ)。= vπθ(s0), 					(13.4)

vπθπθ的真正价值函数,θ的政策决定。从这里在我们的讨论,我们将假定没有折扣(γ= 1)对于情景的情况,虽然出于完整性的考虑我们做装箱算法包括折扣的可能性。
使用函数逼近，以确保改进的方式更改策略参数似乎很有挑战性。问题是，性能既取决于操作选择，也取决于进行这些选择的状态的分布，而这两者都受到策略参数的影响。给定一个状态，策略参数对动作的影响，以及对奖励的影响，可以从参数化的知识中以相对直接的方式来计算。但政策对国家分布的影响是环境的函数，通常是未知的。当梯度依赖于政策变化对国家分布的未知影响时，我们如何能估计相对于政策参数的性能梯度?
幸运的是，对于这个挑战，有一个很好的理论答案，那就是政策梯度定理，它提供了梯度的解析表达式

对于不涉及状态分布的导数的策略参数(这是梯度上升所需的近似值(13.1))的性能。情景案例的政策梯度定理证明了这一点

∇J(θ)∝?
年代
μ(s)?
一个
qπ(s)∇π(|年代,θ), 					(13.5)

偏导数的梯度是列向量θ的组件,和π表示相对应的政策参数向量θ。这里的象征∝意味着“成正比”。在情景性的例子中，比例常数是一集的平均长度，在连续的例子中是1，所以这个关系实际上是一个等式。这里的分布μ(如第9和第10章)在政策分布在π(见199页)。在上一页的方框中证明了政策梯度定理。


\section{加强:蒙特卡罗政策梯度}

现在我们准备推出我们的第一个政策梯度学习算法。回想一下我们随机梯度上升的总体策略(13.1)，它要求获得样本，使样本梯度的期望与性能度量的实际梯度成参数的函数成正比。样本梯度只需要梯度成正比,因为可以吸收任何比例常数步长α,否则是任意的。政策梯度定理给出与梯度成正比的精确表达式;所需要的只是某种抽样方法，其期望等于或近似于这个表达式。注意右边的策略梯度定理是州加权求和的频率状态下发生的目标政策π;如果π是紧随其后,那么国家会遇到这些比例。因此

∇J(θ)∝?
年代
μ(s)?
一个
qπ(s)∇π(|年代,θ)


= Eπ

一个
qπ(St)∇π(|圣,θ)

. 					(13.6)


我们可以在这里停止并实例化我们的随机梯度上升算法(13.1)。

θt + 1
.
=θt +α

一个
问̂(圣,w)∇π(|圣,θ), 					(13.7)


在问̂qπ学到一些近似。这个算法,它被称为一个所有操作方法,因为它更新涉及到的所有行动,承诺和值得进一步研究的,但是我们的活期利息是经典的增强算法在时间t(威廉,1992)的更新包括在t时刻真正采取行动。
我们继续用引入St in(13.6)的方法来进行强化的推导，通过替换随机变量的可能值的总和。

下一个期望π,然后抽样期望。方程(13.6)包括一个适当的行动求和,但每一项不加权π(|圣,θ)需要一个期望在π。所以我们引入这样一个权重,在不改变平等,然后把总结项乘以π(|圣,θ)。继续(13.6)，我们有。
 

如往常一样，Gt是返回。括号中的最后一个表达式正是我们所需要的，一个量可以在每个期望等于梯度的时间步上抽样。利用这个样本实例化我们的一般随机梯度上升算法(13.1)，得到强化更新:
 

这个更新具有直观的吸引力。每一个增量都与一个返回的Gt和一个向量的乘积成比例，即采取实际行动的概率的梯度除以采取该行动的概率。矢量是参数空间中的方向，大多数增加了在未来访问状态时重复动作的概率，更新增加了这个方向的参数矢量与返回的比例，与动作概率成反比。前者是有意义的，因为它使参数朝有利于产生最高回报的操作的方向移动。后者是有意义的，因为在其他方面，经常被选择的操作具有优势(更新将更经常地朝着它们的方向进行)，并且可能会胜出，即使它们没有产生最高的回报。
注意，强化使用从时间t的完整返回，包括所有未来的奖励直到事件结束。从这个意义上来说，“强化”是一种蒙特卡罗算法，它只适用于情节结束后的所有更新(就像第五章中的蒙特卡罗算法)。
注意，伪代码最后一行中的更新与加固更新规则(13.8)有很大的不同。一个区别是,伪代码使用紧凑表达式∇lnπ(|圣,θt)部分向量∇π(|圣,θt)π(|圣,θt)(13.8)。这两个向量的表达式是等价的遵循从身份∇lnx =∇x
x
.

这个向量在文献中已经有了几个名称和符号;我们将它简单地称为资格向量。注意，这是算法中出现策略参数化的惟一位置。

第二个区别伪代码更新和加强更新方程(13.8)是,前者包括γt的因素。这是因为,正如前面提到的,在文本中我们把non-discounted案例(γ= 1)在我们给的装箱算法算法对于一般折扣情况。所有的想法都经过了适当的调整(包括对199页的方框)，但是涉及到额外的复杂性，会分散对主要想法的注意力。
∗13.2概括199页<锻炼,策略梯度公式(13.5),策略梯度定理的证明(325页),和步骤导致加强更新方程(13.8),这(13.8)最终的因素γt从而符合
伪代码中给出的一般算法。 					?

图13.1从示例13.1中显示了加固在短走廊网格世界上的性能。
 
-20年

-60 -40
总回报集
平均超过100分
G0

-80年

-90年
200年1
1000 800 600 400
集

图13.1:加固短走廊网格世界(示例13.1)。有一个很好的步骤
每集的总奖励接近开始状态的最优值。

加固作为一种随机梯度法，具有良好的理论收敛性。通过构造，一个事件的预期更新与性能梯度方向相同。这保证预期性能的改善α足够小,并收敛到局部最优标准随机近似条件下降低α。然而，作为蒙特卡罗方法的强化可能是高方差的，从而产生缓慢的学习。
练习13.3在第13.1节中，我们考虑了使用soft-max在action preferences(13.2)和线性action preferences(13.3)的策略参数化。对于这个参数化，证明合格向量是

∇lnπ(|年代,θ)= x(,)−?
b
π(b | sθ)x(年代,b), 					(13.9)

使用定义和基本微积分。 					?


\section{加强与基线}

政策梯度定理(13.5)可以推广到包括对任意基线b的作用值的比较:

∇J(θ)∝?
年代
μ(s)?
一个

qπ(s)−b(s)

∇π(|年代,θ)。 					(13.10)

基线可以是任何函数，甚至是随机变量，只要它不随a变化;这个方程仍然有效，因为减去的量是零:

一个
b(s)∇π(|年代,θ)=(s)∇吗?
一个
π(|年代,θ)= b(s)∇1 = 0。

使用带基线的策略梯度定理(13.10)可以使用前一节中类似的步骤派生更新规则。我们最后得出的更新规则是一个新的加固版本，包括一个通用基线:

θt + 1
.
=θt +α

Gt−b(圣)
θt ?∇π(|圣)
π(|圣θt)。 					(13.11)
因为基线可以一致为零，所以这个更新是加固的严格推广。通常，基线使更新的期望值保持不变，但是它会对其方差产生很大的影响。例如，我们在2.8节中看到，类似的基线可以显著降低梯度班迪特算法的方差(从而加速学习)。在bandit算法中，基线只是一个数字(到目前为止所看到的奖励的平均值)，但是对于MDPs，基线应该随状态而变化。在某些状态下，所有的行为都有高价值，我们需要一个高基线来区分高价值行为和低价值行为;在其他状态下，所有的操作都有低的值和低基线是合适的。
一个自然选择基线是一个估计的状态值,v̂(St,w),其中w∈Rm是权重向量学习通过前一章中提出的方法之一。

因为加强蒙特卡罗方法对学习策略参数,θ,似乎也自然地使用蒙特卡罗方法学习状态值权重,w。一个完整的强化与基线算法伪代码使用这种学习状态值函数作为基线在下面的框中给出。
 

该算法有两个步骤大小,表示αθ和αw(αθ(13.11)中的α)。选择的步长值(这里αw)相对比较容易;在线性情况下,我们对设置的经验法则,如αw = 0.1 / E ? ?∇v̂(St,w)?2μ(见9.6节)。更清楚的是如何设置策略的步长参数,αθ的最佳值的偏差范围取决于奖励和政策参数化。
 
-60 -40
总回报集
平均超过100分
G0



-80年


-90年

400 200 1
1000 800 600

集
图13.2:添加一个基线来加强可以使它更快地学习，如下面在短走廊网格世界中所示(示例13.1)。这里用于普通加固的阶梯尺寸是它表现最好的(最接近2的幂;见图13.1)。

图13.2比较了在短廊道网格字上有无基线的加固行为(示例13.1)。这里使用的近似状态值函数基线是v̂(s,w)= w。即w是一个单独的组件,w。


\section{Actor-Critic方法}

虽然增强-带基线的方法学习策略和状态-值函数，但我们不认为它是一个行为-批评方法，因为它的状态-值函数只被用作基线，而不是批评家。也就是说，它不用于引导(从随后状态的估计值更新状态的值估计值)，而仅用于更新估计值的状态的基线。这是一个有用的区别，因为只有通过自举，我们才会引入偏差和渐近依赖于函数近似的质量。如我们所见，通过自举和依赖状态表示引入的偏差通常是有益的，因为它减少了差异并加速了学习。使用基线增强是无偏见的，将渐进地收敛到局部最小值，但是像所有蒙特卡罗方法一样，它往往学习缓慢(产生高方差估计)，并且不方便在线实现或处理持续的问题。正如我们在本书前面所看到的，用时间差的方法可以消除这些不便，通过多步骤的方法，我们可以灵活地选择自举的程度。为了在策略梯度方法的情况下获得这些优势，我们使用了自适应批评家方法。
首先考虑一步行为-批评方法，类似于第6章中引入的TD方法，如TD(0)、Sarsa(0)和Q-learning。单步方法的主要吸引力在于它们是完全在线的和递增的，但是避免了资格跟踪的复杂性。它们是资格跟踪方法的一种特殊情况，不是一般的，但更容易理解。用一步回归法(采用学习状态值函数作为基线)代替完全回归(13.11)如下:

θt + 1
。=θt +αGt:t + 1−v̂(St,w)∇π(|圣,θt)
π(|圣θt) 					(13.12)
=θt +α

Rt + 1 +γv̂(w)圣+ 1−v̂(St,w)
θt ?∇π(|圣)
π(|圣θt)(13.13)
=θt +αδt
∇π(|圣θt)
π(|圣θt)。 					(13.14)
与此相匹配的自然状态函数学习方法是半梯度TD(0)。完整算法的伪代码在下一页顶部的框中给出。请注意，它现在是一个完全在线的增量算法，状态、行为和奖励在发生时进行处理，然后不再重新访问。

\section{持续问题的政策梯度}

如第10.3节所讨论的，对于没有发作边界的持续问题，我们需要根据每次步骤的平均奖励率来定义性能:
 
= lim
t→∞
E(t Rt | S0 A0:−1∼π)

=

年代
μ(s)?
一个
π(|)?
r s ?,
p(s ?)r,r | s,


其中μ是稳态分布在π,μ(年代)。= limt→∞公关{圣= | A0:t∼π},这是假定存在和独立S0(一个遍历性假设)。记住,这是特殊的分配下,如果您选择的行为根据π,你留在相同的分布:

年代
μ(s)?
一个
π(|年代,θ)p(s ?|年代,)=μ(s ?),对所有年代?∈年代。 					(13.16)


在下面的框中给出了持续情况下(向后视图)的actor -批评者算法的完整伪代码。
 
当然,在持续的情况下,我们定义值,vπ(年代)。= Eπ[Gt |圣= s]和qπ(年代)。= Eπ[Gt |圣=,=一个),对微分返回:

Gt。= Rt + 1 Rt + 2−−r(π)+(π)+ Rt + 3−r(π)+···。(13.17)有了这些交替的定义，情景案例(13.5)中给出的政策梯度定理对持续案例仍然成立。在下一页的方框里有证据。前后视图方程也保持不变。
 
\section{持续行动的政策参数化}

基于策略的方法提供了处理大型操作空间的实用方法，甚至是具有无限多个操作的连续空间。我们学习概率分布的统计，而不是计算每一个动作的学习概率。例如，操作集可能是实数，操作选择自正态(高斯)分布。
正态分布的概率密度函数通常是这样写的
 
−1 0 2 4 2−−4
0μ= 0μ= 0,μ=
−2μ=
2 0.2,1.0σ= 2,σ= 2 5.0,0.5σ= 2,σ=
μ、σ是正态分布的平均值和标准偏差,当然这里π是π数量≈3.14159。对几种不同均值和标准差的概率密度函数向右显示。p(x)是x点的概率密度，不是概率。它可以大于1;它是p(x)下的总面积，它的总和必须是1。一般来说，我们可以取p(x)下的积分对于任意范围的x值来得到x落在这个范围内的概率。
为了生成策略参数化，可以将策略定义为实值标量操作的正态概率密度，其均值和标准偏差由依赖于状态的参数函数逼近器给出。也就是说,

π(|年代,θ)。= 1
σ(年代,θ)√2πexp

−
(−μ(年代,θ))2 2σ(年代,θ)2

, 					(13.19)

μ的地方:S×Rd ?→R和σ:S×Rd吗?→R +两个参数化函数近似者。
为了完成这个示例，我们只需要为这些逼近者提供一个表单。我们把政策的参数向量分成两部分,θ=[θμθσ]?，其中一部分用于均值的近似，另一部分用于标准差的近似。均值可以近似为一个线性函数。标准差必须是正的，最好近似为线性函数的指数。因此

μ(年代,θ)。=θμ? xμ(s)和σ(年代,θ)。=经验

θσ? xσ(s)

, 					(13.20)

xμ(s)和xσ(s)是国家特征向量可能由第九章中描述的方法之一。有了这些定义，本章后面描述的所有算法都可以用来学习选择实值操作。
练习13.4表明，对于高斯政策参数化(13.19)，资格向量有以下两部分:

∇lnπ(|年代,θμ)=∇π(|年代,θμ)
π(|年代,θ)=
1σ(年代,θ)2

一个−μ(年代,θ)xμ(s)和

∇lnπ(|年代,θσ)=∇π(|年代,θσ)
π(|年代,θ)=

一个−μ(年代,θ)2
σ(年代,θ)2−1
xσ(年代)。?

练习13.5 bernoullil -logistic单元是一种在某些ANNs中使用的随机神经元样单元(9.6节)。它在t时刻的输入是特征向量x(St);在,它的输出是一个随机变量有两个值,0和1,公关在= { 1 } = Pt和公关在= { 0 } = 1−Pt(伯努利分布)。让h(年代,0,θ)和h(年代,1,θ)的偏好在国家年代单元的两个动作参数θ既定政策。假设行动偏好之间的差异是由单元的输入向量的加权和,也就是说,假设h(年代,1,θ)−h(年代,0,θ)=θ? x(s),θ是单元的权向量。

(一)表明,如果指数soft-max分布(13.2)是用来行动偏好转换为政策,然后Pt =π(1 |圣,θt)= 1 /(1 + exp(−θ?(t x(St)))(逻辑函数)

(b)的蒙特卡罗加强更新θtθt + 1返回Gt收到吗?

(c)表达资格∇lnπ(|年代,θ)Bernoulli-logistic单元,在一个方面,x(s),π(|年代,θ)通过计算梯度。

提示:分别为每个行动的导数计算对数首先对Pt =π(|年代,θt),将两个结果组合成一个表达式,取决于和Pt,然后用链式法则,指出物流的导数函数f(x)
f(x)(1−f(x))。 					?

\section{总结}

在这一章之前，这本书关注的是行动价值方法——意思是学习行动价值，然后用它们来决定行动选择的方法。另一方面，在本章中，我们考虑了学习参数化策略的方法，该策略允许在不考虑行动价值估计的情况下采取行动。特别是，我们已经考虑了策略梯度方法——即在每个步骤上更新策略参数的方法，以估计相对于策略参数的性能梯度。
学习和存储策略参数的方法有很多优点。他们可以学习具体的行动概率。他们可以学习适当水平的探索，并渐进地接近确定的政策。它们可以自然地处理连续的操作空间。所有这些事情都是简单的基于策略的方法,但尴尬或不可能ε-greedy一般方法和行为价值的方法。此外，在某些问题上，策略的参数表示比值函数更简单;这些问题更适合于参数化策略方法。
参数化策略方法也有一个重要的理论优势，其形式是策略梯度定理，它给出了一个精确的公式，用于描述不涉及状态分布导数的策略参数如何影响性能。这个定理为所有的政策梯度方法提供了理论基础。
强化方法直接遵循政策梯度定理。增加一个状态值函数作为基线减少了加固的方差而不引入偏差。使用自引导的状态值函数引入了偏置，但通常出于同样的原因，引导TD方法往往优于蒙特卡罗方法(大大减少了方差)。状态-值函数将信用赋值给批判——政策的行为选择，因此前者被称为批判者，后者被称为行动者，这些整体方法被称为行为-批判方法。
总体而言，策略梯度方法提供了一组与行动价值方法截然不同的优点和缺点。如今，在某些方面，人们对它们的理解还不那么透彻，但它们却是一个令人兴奋的话题和正在进行的研究。


\section{书目的和历史的言论}

我们现在看到的与政策梯度相关的方法实际上是在强化学习中最早被研究的一些方法(Witten, 1977;巴托，萨顿，安德森，1983年;萨顿,1984;威廉姆斯，1987年，1992年)和在前人的领域(Phansalkar和Thathachar, 1995年)。在20世纪90年代，他们基本上是被这本书的其他章节所关注的行动价值方法所取代。然而，近年来，对演员-批评家方法和一般的政策-梯度方法的注意又恢复了。除了我们在这里讨论的之外，还有自然梯度法(Amari, 1998;Kakade, 2002, Peters, Vijayakumar和Schaal, 2005;彼得斯和绍尔对,2008;Park, Kim and Kang, 2005;Bhatnagar, Sutton, Ghavamzadeh和Lee, 2009;参见Grondman, Busoniu, Lopes and Babuska, 2012)，确定性政策梯度方法(Silver等，

2014年)，off-policy gradient methods (Degris, White, and Sutton, 2012);以及熵的正则化(参见舒尔曼、陈和阿贝尔，2017)。主要应用包括特技直升机自动转盘和AlphaGo(第16.6节)。
我们在本章的介绍主要基于Sutton、McAllester、Singh和Mansour(2000)，他们引入了术语“政策梯度方法”。Bhatnagar等人(2009)提供了一个有用的概述。最早的相关作品之一是Aleksandrov, Sysoyev和Shemeneva(1968)。托马斯(2014)第一次意识到γt的因素,按照本章的装箱算法,需要打折的情景性问题。

13.1示例13.1和本章的结果与Eric一起开发
坟墓。

13.2这里和334页上的政策梯度定理是由Marbach首先得到的
tsiklis(1998, 2001)和Sutton等人(2000)。Cao和Chen(1997)也得到了类似的表达。其他早期的结果是由Konda和tsiklis(2000, 2003)、Baxter和Bartlett(2001)、Baxter、Bartlett和Weaver(2001)得出的。Sutton, Singh和McAllester(2000)开发了一些额外的结果。

13.3加强是威廉姆斯的功劳(1987,1992)。Phansalkar和Thathachar
(1995)证明了改进的增强算法的局部和全局收敛定理。
全行为算法首先出现在一篇未发表但广为流传的不完整论文中(Sutton, Singh，和McAllester, 2000)，然后Asadi, Allen, Roderick, Mohamed, Konidaris，和Littman(2017)对其进行了分析和实证研究，他们称之为“刻薄演员批评家”算法。Ciosek和Whiteson(2018)开发了对持续行动的扩展，他们称之为“预期政策梯度”。

13.4基线是在Williams(1987, 1992)的原始工作中引入的。Greensmith,
Bartlett和Baxter(2004)分析了一个可能更好的基线(参见Dick, 2015)。Thomas和Brunskill(2017)认为，一个依赖于行动的基线可以在不产生偏见的情况下使用。

13.5-6的评价方法是最早被调查的强化方法之一。
学习(威滕,1977;巴托，萨顿，安德森，1983年;萨顿,1984)。这里介绍的算法基于Degris、White和Sutton(2012)的工作。在文献中，演员-批评家方法有时被称为优势-演员-批评家方法。

第一个展示如何以这种方式处理连续操作的例子出现了
成为威廉姆斯(1987,1992)。第335页上的数字来自维基百科。

 
