\chapter{第15章 神经科学}

\begin{summary}
	
	神经科学是对神经系统的多学科研究:它们如何调节身体机能;控制行为;随着时间的推移，由于发展、学习和衰老而发生的变化;细胞和分子机制如何使这些功能成为可能。强化学习最令人兴奋的一个方面是来自神经科学的越来越多的证据表明，人类和许多其他动物的神经系统实现了与强化学习算法相对应的算法。本章的主要目的是解释这些相似之处，以及它们对动物奖励相关学习的神经基础的建议。
	强化学习和神经科学之间最显著的联系涉及到多巴胺，这是一种化学物质，与哺乳动物大脑中的奖赏过程密切相关。多巴胺似乎将时间差异(TD)错误传递到学习和决策发生的大脑结构中。这种平行关系可以用多巴胺神经元活动的奖赏预测误差假说来表达，这一假说是由计算强化学习的收敛和神经科学实验的结果所导致的。在这一章中，我们讨论了这个假设，导致它的神经科学发现，以及为什么它是对理解大脑奖励系统的重要贡献。我们还讨论了强化学习和神经科学之间的相似之处，它们与多巴胺/ td -错误的平行度相比，并没有那么显著，但它提供了一些有用的概念性工具，可以用来思考动物身上基于奖励的学习。强化学习的其他元素有可能影响神经系统的研究，但它们与神经科学的联系仍相对不发达。我们讨论了一些我们认为随着时间的推移会变得越来越重要的进化关系。
	正如我们在这本书的第一章(第1.7节)的历史部分中所概述的，强化学习的许多方面都受到神经科学的影响。本章的第二个目标是让读者了解大脑功能的概念，这些概念对强化学习的方法有帮助。根据大脑功能理论，强化学习的一些要素更容易理解。这一点对资格追踪的观点尤其正确，它是强化学习的基本机制之一，起源于神经突触的一种推测性质，神经细胞——神经细胞——通过这种结构相互交流。
	
	在这一章中，我们没有深入研究动物基于奖励的学习的神经系统的巨大复杂性:这一章太短，我们不是神经科学家。我们并没有试图去描述——甚至是说出——许多被认为与这些过程有关的大脑结构和通路，或者任何分子机制。我们也不公正的假设和模型是替代那些与强化学习非常一致的。在这一领域的专家之间存在着不同的观点，这并不奇怪。我们只能对这个引人入胜、不断发展的故事略知一二。尽管如此，我们希望这一章能让你们相信，一个非常有效的途径已经出现，它将强化学习及其理论基础与动物基于奖励的学习的神经科学联系起来。
	许多优秀的出版物涉及强化学习和神经科学之间的联系，其中一些我们在本章最后一节中引用。我们的治疗方法与大多数的不同，因为我们假设对强化学习的熟悉程度是在本书的前几章中提出的，但我们并没有假设神经科学的知识。我们先简单介绍一下神经科学的概念，以便对接下来的内容有一个基本的了解。
		
\end{summary}

\section{神经科学基础知识}

关于神经系统的一些基本信息有助于我们在本章中介绍的内容。我们后面提到的术语是斜体的。如果你已经有了神经科学的基础知识，跳过这一部分就不是问题。
神经元是神经系统的主要组成部分，是利用电信号和化学信号处理和传输信息的细胞。它们有很多种形式，但是神经元通常有一个细胞体、树突和一个轴突。树突是一种结构，它可以从细胞的身体分支接收来自其他神经元的输入(也可以在感觉神经元的情况下接收外部信号)。神经元的轴突是将神经元的输出传输到其他神经元(或肌肉或腺体)的纤维。一个神经元的输出包括一系列的电脉冲称为动作电位沿轴突运动。动作电位也称为脉冲，神经元在产生脉冲时就会发出脉冲。在神经网络模型中，通常使用实数来表示神经元的放电频率，即每一单位时间的平均峰值次数。
神经元的轴突可以广泛地分支，使神经元的动作电位达到许多目标。神经元轴突的分支结构称为神经元轴突柄。因为动作电位的传导是一个活跃的过程，就像引信的燃烧一样，当动作电位到达轴向分支点时，它会“点亮”所有传出分支上的动作电位(尽管传播到分支有时会失败)。因此，具有较大轴突杆的神经元的活动可以影响许多目标位点。

突触通常是位于轴突分支末端的一种结构，它调节一个神经元与另一个神经元之间的通信。突触将信息从突触前神经元的轴突传输到突触后神经元的树突或细胞体。除了少数例外，突触在突触前神经元的动作电位到达时释放出一种化学神经递质。(例外情况是神经元之间直接的电耦合，但这里我们不关心这些。)突触的神经递质分子从突触前释放一边扩散在突触间隙非常小的空间之间的突触前和突触后神经元,结束,然后结合受体表面的突触后神经元的兴奋或抑制其spike-generating活动,或以其他方式调整其行为。一种特殊的神经递质可能与几种不同类型的受体结合，每一种受体对突触后神经元产生不同的影响。例如，至少有五种不同的受体类型的神经递质多巴胺可以影响突触后神经元。许多不同的化学物质被确认为动物神经系统中的神经递质。
神经元的背景活动活动的水平,通常其燃烧速度,当神经元似乎并不由突触输入实验者感兴趣的相关任务,例如,当神经元的活动不与刺激传递到一个主题作为一个实验的一部分。由于来自更广泛的网络的输入，或者由于神经元或其突触内部的噪音，背景活动可能是不规则的。有时，背景活动是神经元内在动力过程的结果。一个神经元的阶段性活动，与它的背景活动相反，是由突触输入引起的脉冲活动组成的。缓慢变化的活动，经常以渐进的方式变化，无论是作为背景活动还是不作为背景活动，都被称为神经元的滋补活动。
突触释放的神经递质影响突触后神经元的强度或效力是突触的效力。神经系统可以改变通过经验的一种途径是通过改变突触的功效的结果组合突触前和突触后神经元的活动,有时存在神经调质,这是一种神经递质有影响以外,或除了直接快速激发或抑制。
大脑中有几个不同的神经调节系统，这些系统由神经元簇组成，这些神经元的轴突弧线分枝很广，每个系统都使用不同的神经递质。神经调节可以改变神经回路的功能，调节动机、觉醒、注意力、记忆、情绪、情绪、睡眠和体温。重要的是，神经调节系统可以分布一些像标量信号(如增强信号)这样的信号，以改变广泛分布的对学习至关重要的突触的操作。
突触功能的改变被称为突触可塑性。它是负责学习的主要机制之一。通过学习算法调整的参数或权重对应于突触效率。正如我们在下面详细介绍的，通过神经调节器多巴胺调节突触可塑性是大脑如何实现学习算法的一个合理机制，就像本书中描述的那样。

\section{奖励信号、强化信号、值和预测错误}

神经科学和计算强化学习之间的联系开始于大脑信号和在强化学习理论和算法中扮演重要角色的信号之间的平行关系。在第三章中，我们说过任何学习目标导向行为的问题都可以归结为表示行为、状态和奖励的三个信号。然而，为了解释神经科学和强化学习之间的联系，我们必须不那么抽象，并考虑其他与大脑信号在某些方面相对应的强化学习信号。除了奖励信号外，这些信号还包括强化信号(我们认为这与奖励信号不同)、值信号和传递预测错误的信号。当我们用函数来标记一个信号时，我们是在强化学习理论的背景下进行的，在强化学习理论中，信号对应于方程或算法中的一个项。另一方面，当我们提到大脑中的一个信号时，我们指的是一个生理事件，如动作电位的爆发或神经递质的分泌。用它的功能给神经信号贴上标签，例如把多巴胺神经元的阶段性活动称为强化信号，这意味着神经信号的表现与相应的理论信号相似。
发现这些通信的证据涉及许多挑战。与奖励处理相关的神经活动几乎可以在大脑的每个部分中找到，而且很难明确地解释结果，因为不同奖赏相关信号的表征往往彼此高度相关。实验需要仔细设计，以使一种与奖励相关的信号能够在一定程度上与其他信号区分开来，或者与奖励处理无关的大量其他信号区分开来。尽管存在这些困难，许多实验都是为了将强化学习理论和算法的各个方面与神经信号相协调，并建立了一些引人注目的联系。为了准备检查这些链接，在本节的其余部分，我们提醒读者，根据强化学习理论，各种奖励相关的信号意味着什么。
在我们对前一章末尾的术语的评论中，我们说Rt就像动物大脑中的奖励信号，而不是动物环境中的一个物体或事件。在强化学习中，奖励信号(以及代理环境)定义了强化学习代理试图解决的问题。在这方面，Rt就像动物大脑中的一个信号，将主要的奖励分配给整个大脑的各个部位。但是像Rt这样的单一的奖励信号不太可能存在于动物的大脑中。最好将Rt看作是一个抽象概念，它总结了大脑中许多系统产生的大量神经信号的总体影响，这些系统评估感觉和状态的奖励或惩罚性质。
强化学习中的强化信号不同于奖励信号。增强信号的功能是指导学习算法在代理策略、价值估计或环境模型中所做的更改。TD方法,例如,强化信号在时间t TD错误δt−1 = Rt +γV(St)−V(圣−1)。1
1我们6.1节中提到的,δt在我们的符号定义是Rt + 1 +γV(圣+ 1)−V(St),所以δt

某些算法的增强信号可能只是奖励信号，但对于大多数算法，我们认为增强信号是由其他信息调整的奖励信号，如TD误差的值估计。
状态值或动作值的估计值，即V或Q，指定从长期来看对代理是好是坏。他们预测的是一名经纪人在未来积累的总报酬。代理通过选择导致状态估计值最大的动作或选择行为估计值最大的动作来做出正确的决策。
预测误差测量预期和实际信号或感觉之间的差异。奖励预测误差(Reward prediction errors, RPEs)专门测量预期和收到的奖励信号之间的差异，当奖励信号大于预期时为正值，反之为负值。像(6.5)这样的TD错误是一种特殊的RPEs类型，表明当前和早期的长期回报预期之间存在差异。当神经科学家提到RPEs时，他们通常(虽然不是总是)指TD RPEs，我们简单地称之为TD错误贯穿本章。同样在本章中，TD错误通常不依赖于动作，而不是像Sarsa和Q-learning这样的算法在学习动作值时使用的TD错误。这是因为与神经科学最著名的联系是在没有行动的TD错误上，但我们并不是说排除可能的类似的联系，包括与行动相关的TD错误。(TD错误用于预测除了奖励以外的信号也很有用，但在这里我们并不关心这个问题。例如，Modayil, White和Sutton, 2014)。
人们可以问很多关于神经科学数据和这些理论定义的信号之间的联系的问题。观察到的信号更像是奖励信号、价值信号、预测吗
错误，强化信号，还是完全不同的东西?如果是错误信号，是RPE, TD错误，还是像Rescorla-Wagner错误(14.3)这样简单的错误?如果是TD错误，它是否依赖于像Q-learning或Sarsa的TD错误这样的行为?如上所示，探索大脑来回答这样的问题是极其困难的。但实验证据表明，一种神经递质，特别是神经递质多巴胺，会向RPEs发送信号，进一步说，多巴胺产生的神经元的阶段活动实际上传递了TD错误(关于阶段活动的定义，请参阅第15.1节)。这一证据导致了我们接下来描述的多巴胺神经元活动的奖励预测错误假说。


\section{奖励预测误差假设}

多巴胺神经元活动的奖赏预测误差假说提出，哺乳动物产生多巴胺的神经元的阶段性活动的功能之一，是在旧的和新的对预期未来奖赏的估计之间，将错误传递给整个大脑的目标区域。这个假设(虽然不是用这些确切的词)是由Montague、Dayan和Sejnowski(1996)首次明确提出的，他们展示了增强学习中的TD错误概念如何解释了阶段性的许多特征

直到t+ 1时才可用。TD的错误可以在t是δt−1 = Rt +γV(St)−V(圣−1)。因为我们认为时间步是非常小的，甚至是无穷小的，时间间隔，我们不应该把这一步的时间间隔看得太重要。

哺乳动物多巴胺神经元的活性。导致这一假设的实验是在20世纪80年代和90年代早期在神经学家Wolfram Schultz的实验室中进行的。第15.4节描述了这些有影响的实验，第15.6节解释了这些实验的结果如何与TD错误一致，本章末尾的书目和历史注释部分包含了关于这个有影响的假设的发展的文献指南。
Montague等(1996)将经典条件作用的TD模型的TD误差与经典条件作用下产生多巴胺的神经元的相活动进行了比较。回想一下14.2节,经典条件作用的TD模型基本上是semi-gradient-descent TD(λ)和线性函数近似算法。蒙塔古等人做了几个假设来建立这个比较。首先,因为TD错误可以负但神经元不能有消极的射速,他们认为多巴胺神经元活动的数量对应的是δt−1 + bt,bt在哪里背景神经元的放电频率。一个消极的TD错误对应于多巴胺神经元的放电率低于其本底水平
在每一个经典条件反射试验中，都需要第二个假设，以及它们如何被表示为学习算法的输入。这是我们在第14.2.4节中讨论的TD模型所讨论的问题。Montague等人选择了一个完整的串行化合物(CSC)表示，如图14.1的左列所示，但是短时间内信号序列会持续到美国开始，这是一个非零奖励信号的到达。这种表现使得TD错误模拟了多巴胺神经元的活动不仅可以预测未来的奖励，而且它对预期的奖励何时到来也很敏感。必须有某种方法来跟踪感官提示和奖励到达之间的时间。如果刺激产生一系列内部信号，这些信号在刺激结束后继续存在，如果在刺激结束后的每个时间步上都有不同的信号，那么在刺激结束后的每个时间步上都有不同的状态。因此，与状态相关的TD错误可以对试验中的事件时间敏感。
在这些关于背景放电率和输入表征的假设的模拟试验中，TD模型的TD错误与多巴胺神经元的阶段活动非常相似。在下面15.4节中，我们对这些相似点的详细描述，TD错误与多巴胺神经元活动的以下特征相平行:2)在早期的学习中，奖励前的中性线索不会引起大量的阶段性多巴胺反应，但随着不断的学习，这些线索会获得预测价值，并引发阶段性多巴胺反应;3)如果一个更早的线索可靠地先于一个已经获得了预测值的线索，阶段性的多巴胺反应就会转移到早的线索，停止后的线索;4)如果在学习后忽略了预期的奖励事件，那么在奖励事件发生后不久，多巴胺神经元的反应就会低于其基线水平。
虽然并不是每个多巴胺神经元都在舒尔茨和

2 TD错误相关的文献中多巴胺神经元的活动,他们δt一样我们δt−1 = Rt +γV(St)−V(圣−1)。

同事们在所有这些方面都表现得很好，大部分被监测的神经元的活动和TD的错误之间的惊人的对应关系为奖励预测错误假设提供了强有力的支持。然而，在某些情况下，基于假设的预测与实验中观察到的结果不相符。输入表征的选择对于TD错误与多巴胺神经元活动的某些细节，尤其是关于多巴胺神经元反应时间的细节匹配程度是至关重要的。我们在下面讨论了不同的想法，其中一些是关于输入表示和TD学习的其他特性，以使TD错误更适合数据，尽管主要的相似之处出现在了Montague等人使用的CSC表示中。总的来说，奖励预测错误假说在研究基于奖励的学习的神经科学家中得到了广泛的认可，并且在面对从神经科学实验中积累的结果时，它被证明是非常有弹性的。
神经科学实验的准备我们的描述支持奖励预测错误的假设,并提供一些背景,这样假设的意义可以欣赏,我们下一个礼物是什么了解多巴胺,影响大脑结构,以及它如何参与奖励学习。


\section{多巴胺}

多巴胺作为一种神经递质产生于哺乳动物中脑的神经元，它们的细胞主体主要位于哺乳动物中脑的两个神经元簇:黑质(SNpc)和腹侧被盖区(VTA)。在哺乳动物的大脑中，多巴胺在许多过程中扮演着重要的角色。其中最突出的是动机，学习，行动选择，大多数上瘾形式，精神分裂症和帕金森氏症。多巴胺被称为神经调节器，因为它除了直接快速刺激或抑制目标神经元外，还有许多功能。虽然关于多巴胺的功能和细胞效应的细节仍有很多未知，但很明显，它对哺乳动物大脑的奖赏处理是至关重要的。多巴胺并不是奖励过程中唯一的神经调节因子，它在厌恶情境中的作用——惩罚——仍然存在争议。多巴胺在非哺乳动物中也有不同的功能。但是没有人怀疑多巴胺对包括人类在内的哺乳动物的奖赏过程是必不可少的。
一种早期的传统观点认为，多巴胺神经元向与学习和动机有关的多个大脑区域传播奖励信号。这一观点源自詹姆斯·奥兹和彼得·米尔纳在1954年发表的一篇著名论文，文中描述了电刺激对老鼠大脑某些区域的影响。他们发现，对特定区域的电刺激在控制老鼠行为方面起到了非常强大的奖励作用:“……”通过这种奖励，对动物行为的控制是极端的，可能超过了以前在动物实验中使用过的任何其他奖励。”(Olds和Milner, 1954)。后来的研究表明，刺激在产生奖赏效应时最有效的部位会直接或间接地刺激多巴胺通路，而这些通路通常是由自然的奖赏刺激激发的。与这些相似的效果也在人体实验中被观察到。这些观察结果强烈地表明，多巴胺神经元的活动是奖励的信号。

但是，如果奖励预测错误假说是正确的——即使它只解释了多巴胺神经元活动的某些特征——这种对多巴胺神经元活动的传统看法并不完全正确:多巴胺神经元的阶段性反应表明奖励预测错误，而不是奖励本身。在强化学习的术语中,多巴胺神经元的相位的响应在时间t对应δt−1 = Rt +γV(St)−V(圣−1),而不是Rt。
强化学习理论和算法有助于调和奖励预测错误观点与传统观念认为多巴胺信号的奖励。在我们讨论的很多算法在这本书中,δ函数作为强化信号,这意味着它是学习的主要原因。例如,δ是经典条件作用的TD模型中的关键因素,δ是强化学习的信号值函数和政策actor-critic架构(章节13.5和15.7)。Action-dependent形式的δ强化信号q学习和撒尔沙。奖励信号Rt是一个关键组成部分δt−1,但它不是完整的行列式在这些算法的强化效果。额外的术语γV(St)−V(圣−1)的高阶强化部分δt−1,即使奖励(Rt ? = 0)发生,TD的错误可以沉默如果奖励是完全预测(这是完全解释下面的15.6节)。
仔细看看欧兹和米尔纳在1954年的论文，事实上，这篇论文主要是关于在仪器调节任务中电刺激的增强效果。电刺激不仅激发了老鼠的行为——通过多巴胺对动力的作用——还使老鼠迅速学会通过按压杠杆来刺激自己，而这是它们长时间经常做的事情。电刺激引起的多巴胺神经元活性增强了大鼠的杠杆按压。
最近使用光遗传学方法的实验确定了多巴胺神经元的阶段反应作为强化信号的作用。这些方法使神经科学家能够精确地控制清醒行为的动物在一毫秒内所选择的神经元类型的活动。光遗传学方法将光敏蛋白引入到选定的神经元类型中，使这些神经元能够通过激光的闪烁来激活或沉默。第一个实验使用多巴胺神经元optogenetic方法研究表明,optogenetic刺激生产阶段的多巴胺神经元激活小鼠足以条件老鼠喜欢的商会,他们收到这刺激比室的另一边,他们没有收到,或低的频率,刺激(蔡et al . 2009年)。在另一个例子中，Steinberg等人(2013年)利用多巴胺神经元的光遗传学激活，在预期有奖励刺激但在正常的多巴胺神经元活动停止时，在大鼠中制造人为的多巴胺神经元活动爆发。当这些停顿被人为的爆发所取代时，当响应通常因缺乏强化而减少时(在绝灭试验中)，当响应通常因预期的奖励而被阻断时(阻塞范式;14.2.1节)。
额外的多巴胺的增强功能的证据来自optogenetic与果蝇实验,除了这些动物多巴胺在哺乳动物的效果是相反的效果:光引发多巴胺神经元活动的行为就像电动脚冲击强化回避行为,至少在人口

多巴胺神经元的活化(clarichang - chang et al. 2009)。尽管这些optogenetic实验表明,相位的多巴胺神经元活动特别像一个TD错误,他们令人信服地表明,相位的多巴胺神经元活动行为就像δ行为(或者像-δ在果蝇行为)作为预测算法的强化信号(经典条件作用)和控制(工具性条件作用)。


一个神经元的轴突轴位轴突
多巴胺是一种神经递质。这些
轴突与目标神经元的大量树突产生突触联系
大脑区域。
改编自《神经科学杂志》，《松田》、《芙蓉》、《中村》、《平木》、《富士山》、《天宫》，2009年第29卷，第451页。
多巴胺神经元特别适合向大脑的许多区域传播强化信号。这些神经元有巨大的轴突，每一个都释放出比典型神经元的轴突多100到1000倍的多巴胺。SNpc或VTA多巴胺神经元的每一个轴突都会在目标大脑区域的神经元树突上产生大约50万个突触联系。
如果多巴胺神经元广播一个强化信号像强化学习的δ,因为这是一个标量信号,即。一个数字，SNpc和VTA中的所有多巴胺神经元或多或少都将被期望以相同的方式激活，这样它们就会在几乎同步的情况下，将相同的信号发送到它们的轴突目标的所有位置。尽管人们普遍认为多巴胺神经元确实是这样共同作用的，但现代证据表明，更复杂的情况是，不同的多巴胺神经元亚群对输入的反应不同，这取决于它们发出信号的结构和不同的结构
这些信号作用于目标结构的方式。多巴胺除了发出信号外，还具有其他功能，即使是多巴胺神经元也有信号RPEs，根据这些结构在产生强化行为中的作用，将不同的RPEs发送到不同的结构是有意义的。这是超出了我们对待任何细节在这本书中,但向量值RPE信号意义从强化学习的角度来看,当决策可以分解成单独的sub-decisions,或更普遍的是,作为一种解决结构版本的信贷分配问题:你如何分配信贷成功(或失败)负责决定在众多组件的结构可能是参与生产吗?我们将在下面的第15.10节中对此进行更多的讨论。
大多数多巴胺神经元的轴突与额叶皮层和基底神经节的神经元产生突触联系，大脑的区域参与自发运动、决策、学习和计划等认知功能。因为大多数的想法有关

多巴胺增强学习集中在基底神经节，而来自多巴胺神经元的连接在那里特别密集，我们集中在基底神经节。基底神经节是位于前脑基底部的一组神经元群，或核。基底神经节的主要输入结构称为纹状体。基本上所有的大脑皮层和其他结构都为纹状体提供输入。皮层神经元的活动传递了丰富的有关感觉输入、内部状态和运动活动的信息。皮层神经元的轴突在纹状体的主要输入/输出神经元的树突上形成突触联系，称为中刺神经元。纹状体的输出通过其他基底节核和丘脑回至皮质额区和运动区，使纹状体有可能影响运动、抽象决策过程和奖赏处理。纹状体的两个主要分支对强化学习很重要:背侧纹状体，主要涉及影响行为选择;腹侧纹状体，被认为是奖励处理的不同方面的关键，包括情感对感觉的分配。
中等大小的刺状神经元的树突上覆盖着刺状突起，其顶端的神经元轴突与突触有接触。同时，与这些脊髓的突触接触——在这种情况下，是多巴胺神经元的轴突(图15.1)。这种排列将皮质神经元的突触前活动聚集在一起，
 
 
图15.1:纹状体神经元的脊柱，显示皮层和多巴胺神经元的输入。
皮层神经元的轴突通过皮质纹状突触释放神经递质谷氨酸来影响纹状体神经元。VTA或SNpc多巴胺神经元的轴突通过脊柱(右下方)。这个轴突上的“多巴胺静脉曲张”会在脊柱茎处或附近释放多巴胺，这种排列将突触前的输入，突触后的活动聚集在一起
纹状神经元和多巴胺，这使得几种学习规则成为可能
层次的突触的可塑性。多巴胺神经元的每一个轴突都与大约50万个脊柱的茎产生突触接触。我们讨论中忽略的一些复杂性可以通过其他神经递质途径和多种受体类型来体现，比如D1和D2多巴胺受体，多巴胺可以在棘突和其他突触后部位产生不同的效果。《神经生理学杂志》，W.舒尔茨，1998年第80页，第10页。

中棘神经元突触后活动，多巴胺神经元的输入。这些刺的实际情况是复杂的，并不完全了解。图15.1显示了两种类型的多巴胺受体，谷氨酸受体——皮层输入的神经递质——以及各种信号相互作用的多种方式，暗示了复杂性。但越来越多的证据表明，从大脑皮层到大脑纹状体(神经科学家称其为皮质纹状体突触)的神经突触效率的变化，在很大程度上依赖于适当的多巴胺信号。

\section{奖励预测误差假设的实验支持}

多巴胺神经元对强烈的、新奇的或意想不到的视觉和听觉刺激做出反应，这些刺激会触发眼睛和身体的运动，但它们的活动与运动本身几乎没有关系。这是令人惊讶的，因为多巴胺神经元的退化是帕金森病的一个原因，其症状包括运动障碍，尤其是自我启动的运动障碍。Romo和Schultz(1990)、Schultz和Romo(1990)和Romo(1990)在多巴胺神经元活动与刺激引发的眼睛和身体运动之间的弱关系的驱使下，通过记录多巴胺神经元的活动和猴子移动手臂时的肌肉活动，迈出了实现奖励预测错误假说的第一步。
他们训练两只猴子把手从静止的位置伸到一个装着苹果、饼干或葡萄干的箱子里，这时猴子看到并听到箱子的门开了。猴子可以抓住食物，把食物送到嘴里。当一只猴子在这方面做得很好之后，它被训练做另外两项任务。第一个任务的目的是观察当运动自我启动时，多巴胺神经元会做什么。箱子是开着的，但上面盖着盖子，猴子看不见里面，但可以从下面伸进去。没有任何触发刺激出现，当猴子伸手去拿食物吃后，实验者通常(虽然不是总是)，安静地，不被猴子看见，把食物粘在坚硬的铁丝上，取代食物在箱子里。在这里，Romo和Schultz观察到的多巴胺神经元的活动与猴子的动作没有关系，但是当猴子第一次接触食物时，这些神经元中的很大一部分会产生相对应的反应。当猴子在没有食物的情况下触摸金属丝或探索垃圾箱时，这些神经元没有反应。这很好地证明了神经元对食物的反应，而不是对任务的其他方面的反应。
Romo和Schultz的第二个任务的目的是观察当运动被刺激物触发时会发生什么。这项任务使用了一个不同的箱子和一个可移动的盖子。打开垃圾箱的景象和声音引发了人们对垃圾箱的移动。在这种情况下，Romo和Schultz发现在经过一段时间的训练后，多巴胺神经元不再对食物的触摸做出反应，而是对食物箱的打开盖的视觉和声音做出反应。这些神经元的阶段性反应已经从奖励本身转移到刺激来预测奖励的有效性。在随后的一项研究中，Romo和Schultz发现了大部分多巴胺神经元的活动

在行为任务之外，被监控的人对打开垃圾箱的视觉和声音没有反应。这些观察结果表明，多巴胺神经元对运动的开始和刺激的感官特性都没有反应，而是在发出奖赏的信号。
舒尔茨的团队进行了许多涉及SNpc和VTA多巴胺神经元的额外研究。一组特定的实验结果表明，多巴胺神经元的相对应于TD误差，而不是像Rescorla-Wagner模型(14.3)那样的更简单的错误。在第一个实验中(Ljungberg, Apicella，和Schultz, 1992)，猴子们被训练在灯光作为“触发信号”被照亮以获得一滴苹果汁后，去压低杠杆。正如Romo和Schultz早些时候所观察到的，许多多巴胺神经元最初对奖励做出反应——果汁的下降(图15.2，顶部面板)。但是，随着训练的继续，这些神经元中的许多失去了奖赏反应，取而代之的是对预示奖赏的光线的反应(图15.2，中间面板)。随着持续的训练，杠杆按压变得更快，而对触发信号做出反应的多巴胺神经元数量减少。
在这项研究之后，同样的猴子接受了新任务的训练(Schultz, Apicella，和Ljungberg, 1993)。在这里，猴子面对着两个杠杆，每个杠杆上面都有一盏灯。其中一盏灯的照明是一个“指令提示”，指示两个杠杆中的哪一个
 

图15.2:多巴胺神经元的反应由最初的反应转变为最初的奖赏。
早期预测刺激。这些是被监测的多巴胺神经元在小时间间隔内产生的动作电位的数量的图，在所有被监测的多巴胺神经元(这些数据从23个神经元到44个神经元)上取平均数。图中:未预料到的苹果汁会刺激多巴胺神经元。中间:学习,多巴胺
神经元对预测奖励的触发线索产生反应，对奖励的传递失去反应。底部:在触发信号前增加一个指令提示1秒，多巴胺神经元将他们的反应从触发信号转移到之前的指令
提示。舒尔茨等人(1995)，麻省理工学院出版社。

\section{奖励预测误差假设389的实验支持}

会产生一滴苹果汁。在此任务中，指令提示在前一个任务的触发提示之前以1秒的固定间隔进行。猴子们学会了在看到触发信号之前不去触碰，多巴胺神经元的活动增加了，但是现在被监测的多巴胺神经元的反应几乎只发生在早期的指令信号上，而不是触发信号(图15.2，底部面板)。在这里，当这个任务被很好地学习之后，对指令信号做出反应的多巴胺神经元的数量又大大减少了。在完成这些任务的学习过程中，多巴胺神经元的活动从最初对奖励的反应转变为对早期的预测刺激的反应，首先是触发刺激，然后是更早的指示信号。re -

图15.3:在预期的时间后不久，多巴胺神经元的反应低于基线
奖励失败发生。图中:一滴苹果汁意想不到的分泌会激活多巴胺神经元。中间:多巴胺神经元对条件刺激(CS)做出反应，它可以预测奖励，但对奖励本身没有反应。底部:当CS预测的奖励没有发生时，多巴胺神经元的活动在预期奖励发生后不久就会低于基线。
在每个面板的顶部显示平均值
被监测的多巴胺神经元在指定时间间隔内产生的动作电位的数量。下面的光栅图显示了被监控的单个多巴胺神经元的活动模式;每个点代表一个动作电位。1997年3月14日，来自舒尔茨、大安和蒙太古，预测与奖励的神经基板，科学，第275卷，第5306期，第1593-1598页。经美国科学促进会许可转载。
随着时间的推移，它从后来的刺激中消失了。这种对早期奖励预测者的反应转移，而对后来的预测者失去反应是TD学习的一个标志(例如，参见图14.2)。
刚刚描述的任务揭示了多巴胺神经元活动与TD学习共享的另一个特性。猴子有时按错了钥匙，也就是说，钥匙除了指示的钥匙外，没有得到任何奖励。在这些试验中，许多多巴胺神经元在奖励通常的交付时间后不久，其放电率急剧下降，低于基线，而这是在没有任何外部提示来标记奖励通常的交付时间的情况下发生的(图15.3)。不知怎么的

猴子们在体内追踪奖励的时间。(反应时间是TD学习的最简单版本需要修改的一个领域，以解释多巴胺神经元反应时间的一些细节。我们将在下一节中讨论这个问题。
上述研究的观察结果让舒尔茨和他的团队得出结论，多巴胺神经元对未预料到的奖励、对奖励最早的预测因子做出反应，如果奖励或奖励的预测因子在预期时间内没有出现，那么多巴胺神经元的活动就会低于基线水平。熟悉增强学习的研究人员很快认识到，这些结果与TD错误在TD算法中的增强信号表现非常相似。下一节通过详细分析一个特定示例来研究这种相似性。


\section{TD错误/多巴胺的信件}
本节解释之间的通信TD误差δ和多巴胺神经元的阶段反应在刚刚描述的实验观察。我们检查δ如何变化的学习任务就像上面描述的一个猴子第一次见到一个指令信号和一个固定的时间后必须正确应对触发提示为了获得奖励。我们使用这个任务的一个简单的理想化版本，但是我们比平常更详细，因为我们想强调TD错误和多巴胺神经元活动平行的理论基础。
第一个简化的假设是，代理已经学会了获得奖励所需的行为。然后，它的任务就是精确地预测它所经历的状态的未来回报。然后，这是一个预测任务，或者更严格地说，是一个策略评估任务:学习一个固定策略的值函数(第4.1和6.1节)。要学习的值函数为每个状态分配一个值，该值预测如果代理根据给定的策略选择动作，那么返回值将跟随该状态的返回值，其中返回值是所有未来奖励的(可能折现的)总和。这是不现实的,因为猴子的情况,因为猴子的模型可能会在同一时间学习这些预测,它是学习正确采取行动(强化学习算法,学习政策以及价值功能,比如actor-critic算法),但这种情况下比一个简单的描述一个政策和值函数同时学习。
现在假设代理的经验分为多个试验，在每个试验中，相同的状态序列重复，在试验的每个时间步上都有不同的状态。进一步假设所预测的回报仅限于试验的回报，这使得试验类似于我们定义的强化学习。当然，在现实中，预测的回报并不局限于单一的试验，试验之间的时间间隔是决定动物学习什么的一个重要因素。这也适用于TD学习，但这里我们假设回报不会在多次试验中累积。考虑到这一点，像舒尔茨和他的同事所做的实验就相当于强化学习。(虽然在这个讨论中，我们将使用学期试验而不是插曲来更好地与实验相关。)

和往常一样，我们还需要对状态如何表示为学习算法的输入做出一个假设，这个假设会影响道明错误与多巴胺神经元活动的密切程度。我们稍后将讨论这个问题，但目前我们假设与Montague等人(1996)所使用的CSC表示方式相同，即在试验的每个步骤中，每个国家都有一个单独的内部刺激。这将过程简化为本书第一部分所涉及的表格情况。最后，我们假定代理使用TD(0)来学习一个值函数，V，存储在一个初始化为所有状态为零的查找表中。我们也认为这是一个确定的任务和折现系数,γ,几乎是一个,这样我们可以忽略它。
图15.4显示的时间课程R,V,δ在几个阶段的学习在这个政策评估的任务。时间轴表示在试验中访问状态序列的时间间隔(为了清晰起见，我们省略了显示单个状态)。在每次试验中，奖励信号为零，除非当代理到达奖励状态，在时间线的右端，当奖励信号变成一个正数时，R?TD学习的目标是预测在试验中访问的每个州的回报，在这个未折现的案例中，考虑到我们假设预测仅限于个别试验，这仅仅是R吗?为每个状态。
		t			R ?
					
	R				
		常规的预测因素	R	在这个区间	
	V				
	δ				
	δ				
R					

图15.4:TD的行为错误δ在TD学习阶段的特点是一致的多巴胺神经元的激活。(δ是TD错误可以在时间t,即。,δt−1)。
顶部:一个状态序列，以一个正则预测符的区间表示，后面跟着一个非零的奖励R?早期的学习:初始值函数,V,和最初的δ,起初等于R ?。学习完成:价值函数准确地预测未来的回报,δ是积极的最早预测状态,δ= 0时的非零回报。R ?省略了:当时预测省略奖励,δ变得消极。有关发生这种情况的完整解释，请参见文本。

奖励状态之前是奖励预测状态的序列，最早的奖励预测状态显示在时间线的左端。这就像在试验开始时的状态，例如，在舒尔茨等人(1993)的猴子实验中，用指令提示标记的状态。这是试验中第一个可靠地预测试验结果的状态。(当然，在现实中，在之前的试验中访问的状态甚至是更早的奖励预测状态，但是因为我们将预测限制在单独的试验中，这些不能作为试验回报的预测者。下面我们给出一个更令人满意的，尽管更抽象的描述，一个最早的奖励预测的状态。在试验中，最新的奖励预测状态是试验开始前的状态。这是图15.4中时间线最右端附近的状态。值得注意的是，试验的回报状态并不会预测试验的回报:这个状态的价值将会在接下来的所有试验中预测试验的回报，这里我们假设在这个情景式中是零。
图15.4显示了初审时间课程V和δ的图形标记的早期学习。“因为整个实验中奖励信号都是零，除了达到奖励状态时，所有V值都是零，所以TD误差也为零，直到变成R?”在有益的状态。在此之前因为δt−1 = Rt + Vt−Vt Rt + 0−−1 = 0 = Rt,哪个是零,直到它等于R ?当奖励。Vt和Vt−1分别估算值的州访问有时t,t−1审判。这个学习阶段的TD错误类似于训练开始时的多巴胺神经元对未预料到的奖励(如一滴苹果汁)的反应。
在第一次试验和所有后续试验中，TD(0)更新发生在每个状态转换上，如第6章所述。这连续地增加了奖励预测状态的值，增加了从奖励状态向后扩展的值，直到值收敛到正确的返回预测。在这种情况下(因为我们假设没有折现)正确的预测等于R?对于所有的奖励预测状态。这可以在图15.4中看到V标记为“学习完成”的图形，其中从最早到最近的奖励预测状态的所有值都等于R?早期奖励预测状态之前的状态的值仍然很低(图15.4显示为零)，因为它们不是奖励的可靠预测者。
当学习完成时，即当V达到其正确值时，与任何奖赏预测状态的转换相关的TD错误为零，因为预测现在是准确的。这是因为从reward-predicting状态过渡到另一个reward-predicting状态,我们有δt Rt + Vt Vt−−−1 = 1 = 0 + R ?−R ?= 0,从最新的reward-predicting状态转换到有益的状态,我们有δt Rt + Vt Vt−−−1 = 1 = R ? + 0−R ?= 0。另一方面，从任何状态到最早的奖励预测状态的TD误差是正的，因为这个状态的低值与后面奖励预测状态的大值不匹配。事实上,如果一个国家的价值之前最早reward-predicting状态是零,然后转换到最早的reward-predicting状态后,我们会δt Rt + Vt Vt−−−1 = 1 = 0 + R ?−0 = R ?。δ的学习完成的图在图15.4显示了这个正值最早reward-predicting状态,0和其他地方。

在过渡到最早的奖励预测状态时，TD阳性错误类似于多巴胺对最早的奖励刺激的持续反应。同样，当学习完成时，从最新的奖励预测状态到奖励状态的转换会产生零TD错误，因为最新的奖励预测状态的值如果正确，就会抵消奖励。与此类似的观察是，对完全预测的奖励而不是未预测的奖励，产生阶段性反应的多巴胺神经元更少。
学习后,如果奖励是突然省略,TD的错误在老时间去负的奖励,因为最新的价值reward-predicting状态然后过高:δt Rt + Vt Vt−−−1 = 1 = 0 + 0−R ?=−R ?如图所示,右端δR省略的图的如图15.4所示。这就像在舒尔茨等人(1993)的实验中所看到的，在预期的奖励被忽略时，多巴胺神经元的活动在低于基线的情况下减少，如图15.3所示。
最早的奖励预测状态的想法值得更多的关注。在上面描述的场景中，由于经验被划分为试验，我们假设预测仅限于单个试验，所以最早的回报预测状态总是试验的第一状态。很明显,这是人为的。一种更普遍的方式来考虑最早的奖励预测状态，那就是它是一个无法预测的奖励预测，而且可能有很多这样的状态。在动物的一生中，许多不同的状态可能先于最早的奖赏预测状态。然而，由于这些国家往往被其他不预测回报的国家所取代，它们的回报预测能力，也就是它们的价值，仍然很低。一个TD算法，如果在动物的整个生命中运行，也会更新这些状态的值，但是更新不会持续积累，因为，根据假设，这些状态都不会可靠地先于最早的奖励预测状态。如果他们中的任何一个这样做了，他们也将是回报预测国家。这也许可以解释为什么在过度训练的情况下，多巴胺的反应会减少到甚至是在试验中最早的奖励预测刺激。在过度训练的情况下，人们可能会认为，即使是一个没有预测的预测状态，也会通过与早期状态相关的刺激来预测:动物在实验任务内外与环境的相互作用会变得很平常。然而，当你打破常规，开始一项新的任务时，你会看到TD错误再次出现，就像在多巴胺神经元活动中观察到的那样。
上面描述的示例解释了为什么当动物在类似于我们的示例中的理想化任务中学习时，TD错误与多巴胺神经元的阶段活动具有相同的关键特征。但不是每个属性的相位的多巴胺神经元的活动一致所以整齐δ的属性。最令人不安的差异之一涉及奖励比预期更早发生时发生的情况。我们已经看到，忽略一个预期的奖励会在奖励的预期时间产生一个消极的预测错误，这对应于当这发生时多巴胺神经元的活动低于基线下降。如果奖励比预期来得晚，那么这就是一个意外的奖励，并产生一个积极的预测错误。这发生在TD错误和多巴胺神经元反应。但是当奖赏比预期来得更早时，多巴胺神经元就不会做TD错误所做的事情——至少是蒙塔古等人(1996)和我们在我们的例子中使用的CSC表示法。多巴胺

神经元确实会对早期的奖励做出反应，这与阳性的TD错误是一致的，因为那时奖励不会被预测出来。然而，当预期的奖励被忽略时，TD的错误是消极的，而与此相反，在TD模型预测的方式下，多巴胺神经元的活动并没有低于基线(Hollerman和Schultz, 1998)。动物大脑中正在发生的事情比单纯用CSC表示学习TD要复杂得多。
通过选择适用于TD算法的参数值，并使用除CSC表示之外的刺激表示，可以解决TD错误和多巴胺神经元活动之间的一些不匹配。例如，为了解决刚才描述的早期奖励不匹配问题，Suri和Schultz(1999)提出了CSC表示，在CSC表示中，早期刺激引发的内部信号序列被奖励的发生所抵消。Daw、Courville和Touretzky(2006)提出的另一个建议是，大脑的TD系统使用在感觉皮层进行的统计建模所产生的表征，而不是基于原始感觉输入的简单表征。Ludvig、Sutton和Kehoe(2008)发现，与使用微刺激(MS)表征的TD学习(图14.1)比使用CSC表征更适合早期奖励和其他情况下多巴胺神经元的活动。Pan、Schmidt、Wickens和Hyland(2005)发现，即使有CSC表示法，长时间的合格性跟踪也能提高TD误差在多巴胺神经元活动某些方面的适应性。一般来说，许多关于TD-error行为的细节依赖于资格跟踪、贴现和刺激表示之间的微妙交互。诸如此类的发现详细阐述并完善了奖励预测错误假说，但并没有驳斥其核心观点，即多巴胺神经元的阶段性活动具有明显的TD性错误信号。
另一方面,还有其他TD理论和实验数据之间的差异不是那么容易适应通过选择参数值和刺激表示这些差异的(我们提到一些书目的和历史的备注部分结束时,这一章),和更多的不匹配可能会发现为神经科学家进行更加精炼实验。但是奖励预测错误假说已经非常有效地作为一种催化剂来提高我们对大脑奖励系统如何工作的理解。复杂的实验被设计来验证或反驳来自假设的预测，而实验结果反过来又导致了TD误差/多巴胺假设的细化和细化。
这些发展的一个显著的方面是，强化学习算法和理论与多巴胺系统的特性紧密相连，从一个计算的角度出发，完全没有关于多巴胺神经的相关特性的知识——记住，TD学习及其与最优控制和动态规划之间的联系是在许多年之前发展起来的。
除了考虑到多巴胺神经元阶段活动的许多特征外，奖励预测错误假说还将神经科学与强化的其他方面联系起来

特别是学习使用TD错误作为强化信号的算法。神经科学还远未达到完成对电路的理解,分子机制,相位的多巴胺神经元的活动和功能,但证据支持奖励预测误差假设,随着证据表明,相位的多巴胺反应是学习的强化信号,表明大脑可能实现类似的actor-critic算法TD错误扮演至关重要的角色。其他的强化学习算法也是可行的，但是行为-批判算法特别适合哺乳动物大脑的解剖和生理学，正如我们在下面两个部分中所描述的。


\section{神经Actor-Critic}

行为批评算法学习策略和价值函数。“演员”是学习策略的组成部分，而“批评家”则是学习任何政策的成分，以“批评”演员的行为选择。批评家使用TD算法来学习参与者当前策略的状态值函数。价值函数允许评论家批评演员的行动选择通过发送TD错误,δ,演员。积极的δ意味着行动是“好”,因为它导致了国家好于预期值;负δ意味着行动是“坏”,因为它导致了一个国家与一个弱于预期值。根据这些评论，该演员不断更新其政策。
行为批评算法的两个显著特点是，大脑可能会实现这样的算法。首先，表演-批评家算法的两个组成部分——行动者和评论家——表明，条纹的两个部分——背侧和腹侧细分(15.4节)——对于基于奖励的学习来说都是至关重要的——可能分别起着类似演员和评论家的作用。actor - critics算法的第二个特性是，虽然TD错误对每个部分的学习都有不同的影响，但对于行动者和评论者来说，TD错误同时具有增强信号的双重作用。这与神经回路的几个特性非常吻合:多巴胺神经元的轴突瞄准纹状体的背侧和腹侧的亚区;多巴胺似乎对调节两种结构的突触可塑性至关重要;神经调节者，如多巴胺在目标结构上的作用取决于目标结构的性质，而不仅仅取决于神经调节因子的性质。
第13.5节提出了作为策略梯度方法的actor -批评家算法，但是Barto、Sutton和Anderson(1983)的actor -批评家算法更简单，作为一个人工神经网络(ANN)提出。这里我们描述了一种类似Barto等人的ANN实现，我们跟随Takahashi, Schoenbaum和Niv(2008)给出了一个关于这个ANN如何被大脑中的真实神经网络实现的示意图。我们把关于演员和评论家学习规则的讨论推迟到第15.8节，我们将它们作为政策梯度公式的特殊案例，讨论他们对多巴胺如何调节突触可塑性的建议。

图15.5a展示了一种作为ANN的actor -批评家算法的实现，该算法使用组件网络实现了参与者和批评者。评论家包含单个neuron-like单元,V,输出活动代表状态的值,一个组件显示为钻石标记TD计算TD错误结合V的输出与奖赏信号,与先前的状态值(所显示循环从TD钻石本身)。行动者网络有一层k行动者单位标记为Ai, i = 1，…每个动作单元的输出都是k维动作向量的一个分量。另一种选择是有k个单独的动作，一个由每个行动者单位命令，它们相互竞争要被执行，但是在这里我们将把整个a向量看作一个动作。

图15.5:角色评论家ANN和假设的神经实现。a)作为ANN的actor - critics算法。演员调整政策基于TDδ它收到的错误
评论家;使用相同的δ评论家调整州值参数。批评家从奖励信号R和当前状态值估计的变化中产生一个TD错误。演员是
没有直接访问奖励信号，评论家也没有直接访问行为。b)假定的行为-批评算法的神经实现。行动者和批判者的价值学习部分分别被置于纹状体的背侧和腹侧亚区。在VTA和SNpc上的多巴胺神经元传递TD的错误，以调节从皮质区输入到腹侧和背纹状体的突触效果的变化。从神经科学的前沿，vol. 2(1)， 2008, Y. Takahashi, G。
舒恩鲍姆(Schoenbaum)和Y. Niv，让批评者闭嘴:了解可卡因敏感化的影响
在行动者/评论家模型的背景下的背外侧和腹侧纹状体。

批评家和参与者网络都接收由代表代理环境状态的多个特性组成的输入。(请参阅第1章，强化学习因子的环境包括包含强化学习因子的“有机体”内部和外部的组件。)图中显示了标记为x1, x2的圆…， xn，显示两次以保持图形简单。代表突触效能的权重与每个特征xi与批评家单元V以及每个动作单元Ai之间的连接有关。评论家网络中的权重参数化值函数，参与者网络中的权重参数化策略。随着这些权重的变化，网络学习根据我们在下一节中描述的批评家和演员学习规则而变化。
评论家的电路产生的TD误差是评论家和行动者网络中改变权重的增强信号。如图15.5的线标记“TD误差δ”扩展所有的评论家和演员网络连接。这方面的网络实现,一起奖励预测误差假设和多巴胺神经元的活动这一事实是如此广泛分布广泛的这些神经元轴突乔木,表明一个actor-critic网络这样可能不会太牵强的作为一个假说犒赏学习如何在大脑中可能发生的。
图15.5b建议——非常示意图——根据Takahashi等人(2008)的假设，图中左边的ANN如何映射到大脑的结构上。假设将行动者和批判者的价值学习部分分别置于纹状体的背侧和腹侧亚区，即基底神经节的输入结构中。回顾第15.4节，背纹状体主要涉及影响行为选择，腹纹状体被认为是奖励处理的不同方面的关键，包括情感价值分配给感觉。大脑皮层和其他结构一起向纹状体发送信息，传递有关刺激、内部状态和运动活动的信息。
在这个假设的actor -评论大脑的实施过程中，腹侧纹状体向VTA和SNpc发送价值信息，在这些信息中，这些核中的多巴胺神经元将其与奖励的信息相结合，从而产生与TD错误相关的活动(尽管多巴胺能神经元究竟如何计算这些错误，目前还不清楚)。TD误差δ的线在图15.5成为标记在图15.5 b“多巴胺”,代表了广泛的分支轴突在腹侧被盖区多巴胺神经元的细胞体,SNpc。参考图15.1，这些轴突使突触与中刺神经元的树突相连，这是纹状体背侧和腹侧部的主要输入/输出神经元。向纹状体发送输入的皮层神经元的轴突在这些刺的顶端产生突触接触。根据假设，在这些脊柱中，从皮质区到纹状体的突触的功效的变化受学习规则的支配，这是由多巴胺提供的强化信号决定的。
图15.5b所示的假设的一个重要含义是，多巴胺信号不是像增强学习的标量Rt那样的“主”奖励信号。事实上，这个假设意味着，一个人不一定能够探测到大脑，并记录任何单个神经元活动中的信号。

许多相互关联的神经系统产生与奖励相关的信息，根据不同的奖励类型，不同的结构被招募。多巴胺神经元从许多不同的大脑区域接收信息，所以输入到SNpc和VTA，在图15.5b中标记为“奖励”的输入应该被认为是奖励相关信息的矢量，沿着多个输入通道到达这些细胞核的神经元。那么，理论上的标量奖励信号Rt可能对应的是，所有与奖励相关的信息对多巴胺神经元活动的净贡献。这是大脑不同区域的许多神经元活动模式的结果。
虽然图15.5b所示的行为-批判神经实现在某些方面可能是正确的，但它显然需要被细化、扩展和修改，才能成为一个完整的多巴胺神经元的阶段活动功能模型。本章末尾的历史和文献注释部分引用了一些出版物，这些出版物更详细地讨论了这一假设的经验支持和不足之处。我们现在详细地看看行动者和批评家学习算法对控制皮质纹状体突触效率变化的规则的建议。

\section{演员和评论家的学习规则}

如果大脑并实现类似actor-critic算法和假设的数量多巴胺神经元广播一个常见的强化信号corti-costriatal突触的背侧和腹侧纹状体如图15.5 b(这可能是一个简化我们上面提到的),然后这个强化信号以不同的方式影响这两个的突触结构。学习规则的评论家和演员使用相同的强化信号,TD误差δ,但其对学习的影响是不同的这两个组件。TD错误(结合资格跟踪)告诉参与者如何更新操作概率，以达到高值状态。学习的演员就像使用Law-of-Effect-type工具性条件作用学习规则(1.7节):演员保持δ尽可能积极的工作。另一方面，TD错误(与合格跟踪结合)告诉评论家改变值函数参数的方向和幅度，以提高其预测精度。评论家的作品减少δ的大小尽可能接近于零使用学习规则的TD模型经典条件作用(14.2节)。批评家和演员学习规则之间的差异是相对简单的，但是这种差异对学习有着深远的影响，对于演员-批评家算法的工作方式是至关重要的。区别仅仅在于每种学习规则使用的资格跟踪。
一套以上的学习规则可以在像图15.5b这样的神经网络中使用，但是，具体地说，这里我们关注的是在13.6节中提出的符合资格的跟踪问题的actor -批评家算法。在每一个从国家圣状态转换圣+ 1,采取行动和接收行动Rt + 1,这种算法计算TD错误(δ),然后更新资格跟踪向量(zw t和zθt)

评论家和演员的参数(w和θ),根据

δt = Rt v̂+ 1 +γ(圣+ 1 w)−v̂(St,w),zw t =λwzw t−1 +∇v̂(St,w),
zθt =λθzθ−1 +∇lnπ(|圣,θ),w←w +αwδtzw t,
θ←θ+αθδzθt,
在γ∈(0,1)是一个折现率参数,λwc∈[0,1],λwa∈[0,1]bootstrap-ping参数评论家和演员分别和αw > 0和αθ> 0类似步长参数。
认为近似值函数v̂单个线性neuron-like单元的输出,称为评论家单位和标记在图15.5 v。值函数是状态s、x(s) = (x1(s)的特征向量表示的线性函数…,xn(s))?，由权向量w = (w1)参数化。wn)?:

v̂(s,w)= w ? x(s)。 					(15.1)
每一个xi(s)就像神经元突触前信号，其效力为wi。批评家的重量增加根据αwδtzw t,上述规则的强化信号,δt,对应于一个多巴胺信号被广播给所有的评论家的突触。资格跟踪矢量,zw t,评论家单位的跟踪(平均最近的值)∇v̂(St,w)。因为v̂(s,w)是线性权重,∇v̂(St,w)= x(St)。
从神经学的角度来说，这意味着每个突触都有自己的合格痕迹，这是向量zw t的一个组成部分。突触的合格性痕迹是根据到达突触的活动水平累积的，即突触前活动水平，这里由到达突触的特征向量x(St)的组成部分表示。跟踪否则衰减到零速度由分数λw。只要符合条件的跟踪不为零，突触就可以进行修改。突触的效能是如何改变的，取决于突触到达的强化信号，而突触是合格的。我们称之为合格的痕迹就像这些批评家单位的突触的不偶然的合格的痕迹因为它们只依赖于突触前的活动而不依赖于突触后的活动。
批评家单位的突触的非偶然资格痕迹意味着评论家单位的学习规则实质上是第14.2节中描述的经典条件作用的TD模型。根据我们对批评家单元的定义和它的学习规律，图15.5a的批评家和Barto等人(1983)的《ANN》的批评家是一样的。显然，像这样的批评家只包含一个线性神经元单元是最简单的起点;这个批评家单元是一个更复杂的神经网络的代理，它能够学习更复杂的值函数。
图15.5a中的演员是k个神经元样的演员单元的单层网络，每一个接收的特征向量x(St)都是由评论家单元接收的。每个行动者单位j, j = 1，…k有自己的权向量,θj,但因为演员单位都是相同的,我们单位和省略的描述只有一个下标。一种方法为这些

按照上述方程中给出的行为-批判算法，每一个单位都是伯努利-逻辑单元。这意味着每个参与者单元的每次输出都是一个随机变量，取值为0或1。把值1想象成神经元放电，也就是说，释放一个动作电位。加权和,θ? x(St),一个单元的输入向量决定了单位的行动通过指数soft-max概率分布(13.2),这两个动作是物流功能:

π(1 | sθ)= 1−π(0 |年代,θ)= 1
1 + exp(−θ? x(s))。 					(15.2)
每个演员的重量单位是递增,如上所述,通过:θ←θ+αθδtzθt,在δ又对应于多巴胺信号:相同的强化信号,发送到所有的评论家的突触。图15.5显示了δt被广播给所有的突触演员单位(这使得这个演员网络团队强化学习代理,下面我们在15.10节讨论)。演员资格跟踪矢量zθt是一个跟踪最近的值)的(平均∇lnπ(|圣,θ)。要理解此资格跟踪，请参考第13.5条，它定义了这类单元，并要求您为其提供学习规则。锻炼让你表达∇lnπ(|年代,θ)的,x(s),π(|年代,θ)(任意状态和行动)通过计算梯度。对于t时刻实际发生的动作和状态，答案是

∇π(|圣,θ)= ?在−π(|圣,θ)? x(St)。 					(15.3)
与只积累突触前活动x(St)的批评家突触的非偶然性合格痕迹不同，行动者单元突触的合格痕迹也取决于行动者单元本身的活动。我们称之为偶然资格跟踪因为它取决于突触后活动。每一个突触上的合格痕迹都在不断地衰减，但随着突触前神经元的活动以及突触后神经元的激活而增加或减少。因子在−π(|圣,θ)(15.3)是积极的在= 1和消极。行动者单位资格痕迹中的突触后偶然性是批评家和行动者学习规则之间唯一的区别。通过保持信息采取了什么行动在什么州,或有资格的痕迹让信贷奖励(积极δ),或归咎于惩罚(负δ)之间分配的政策参数(药效的演员单位的突触)根据这些参数的贡献,单元的输出,可能影响了后来的δ值。或有资格的痕迹突触标记为他们应该如何修改改变单位的未来应对支持积极的δ值。
对于皮质纹状体突触的效率如何变化，批评家和演员学习规则提出了什么建议?这两个学习规则都与Donald Hebb的经典提议有关，即只要突触前信号参与激活突触后神经元，突触的效率就会提高(Hebb, 1949)。评论家和演员学习规则与赫伯的建议一致，即突触效能的变化取决于几个因素的相互作用。评论家学习规则的强化信号之间的互动是δ和资格痕迹,只取决于突触前的信号。神经学家称这为双因素学习规则，因为这是两个信号之间的相互作用

数量。演员学习规则,另一方面,是一个三因子学习规则,因为除了取决于δ,其资格依赖于突触前和突触后活动痕迹。然而，与Hebb的建议不同的是，这些因素的相对时序对于突触效率的改变至关重要，因为合格的迹象会介入，使得强化信号能够影响最近活跃的突触。
对于演员和评论家学习规则的信号时间的一些微妙之处值得关注。在定义神经元样的行为单元和批评单元时，我们忽略了需要突触输入来影响一个真正的神经元发射的时间。当突触前神经元的动作电位到达突触时，神经递质分子被释放出来，扩散到突触后神经元，在突触后神经元表面与受体结合;这激活了导致突触后神经元兴奋的分子机制(或者在抑制突触输入的情况下抑制其兴奋)。这个过程可能需要几十毫秒。但是，根据(15.1)和(15.2)，输入到一个批评家和演员单元的同时会产生单元的输出。在Hebbian-style可塑性的抽象模型中，忽略这样的激活时间是很常见的，在这种模型中，突触效率会随着突触前和突触后活动的简单产物而变化。更现实的模型必须考虑激活时间。
激活时间对于一个更现实的行动者单位来说尤其重要，因为它影响了偶然资格痕迹如何发挥作用，以便恰当地分配信贷以加强到适当的突触。表达式

在−π(|圣,θ)? x(St)
根据上面给出的参与者学习规则，定义或有资格的跟踪包括后突触因素。

在−π(|圣,θ)?和突触前因子x(St)这是
因为通过忽略激活时间，突触前活动x(St)参与导致突触后活动出现

在−π(|圣,θ)?。指定信用担保-
正确地说，定义合格跟踪的突触前因子必须是也定义跟踪的突触后因子的原因。一个更现实的行为人单位的或有资格追踪必须考虑激活时间。(激活时间不应与神经元接收受该神经元活动影响的增强信号所需的时间混淆。资格跟踪的功能是跨越这个时间间隔，通常比激活时间长得多。我们将在下一节中进一步讨论这个问题。
神经科学暗示了这个过程在大脑中的作用。神经科学家已经发现了一种Hebbian的可塑性，这种可塑性被称为“钉时依赖可塑性”(spike-timing-dependent plas可塑性，STDP)，它为大脑中类似于动作体的突触可塑性的存在提供了可能。STDP是一种hebbian式的可塑性，但突触效能的变化取决于突触前和突触后动作电位的相对时间。这种依赖性可以有不同的形式，但在研究最多的一种形式中，如果突触的尖峰在突触后神经元放电之前不久到达，突触的强度就会增加。如果时序关系逆转，突触后神经元触发后不久就会出现突触前突，那么突触的强度就会降低。STDP是一种Hebbian的可塑性，它考虑了神经元的激活时间，是类动作学习所需要的要素之一。


STDP的发现使神经科学家研究了STDP的三因素形式的可能性，即神经调节输入必须遵循适当的时间突触前和突触后峰值。这种形式的突触可塑性，称为奖赏调制STDP，很像这里讨论的参与者学习规则。正常STDP产生的突触改变，只有在突触前突刺紧跟着突触后的时间窗内有神经调节输入时才会发生。越来越多的证据表明，奖赏调制的STDP发生在背部纹状体的中等刺状神经元的棘突上，多巴胺提供神经调节因子——参与者学习发生在假定的行为批评算法的神经执行中，如图15.5b所示。实验已经证明了奖赏调节STDP，只有当神经调节脉冲到达一个时间窗时，突触前突尖峰紧接突触后的10秒钟内，皮质纹状体突触的效率才会发生持久的变化(Yagishita et al. 2014)。虽然这些证据是间接的，但这些实验指出，存在的偶然资格痕迹有长期的过程。产生这些痕迹的分子机制，以及可能低于STDP的更短的痕迹，还没有被理解，但是专注于时间依赖性和神经调节依赖性突触可塑性的研究仍在继续。
我们在这里描述的神经类行为单元，以及它的效益型学习规则，以某种更简单的形式出现在Barto等人的演员-评论家网络中(1983)。这个网络的灵感来源于生理学家a·h·克洛普夫(1972,1982)提出的“享乐神经元”假说。Klopf的假设并不是所有的细节都与我们已经了解到的突触可塑性相一致，但是STDP的发现和不断增加的STDP奖赏调节形式的证据表明Klopf的想法可能并没有太大的偏离。接下来我们讨论Klopf的享乐神经元假说。

\section{享乐神经元}

在他的享乐主义神经元假说中，Klopf(1972, 1982)推测，单个神经元试图最大限度地区分被视为奖励的突触输入和被视为惩罚的突触输入，它们通过调整自身行动潜力的奖励或惩罚结果的突触效率。换句话说，个体的神经元可以被训练成反应时序强化，就像动物可以被训练成工具调节任务一样。他的假设包括奖励和惩罚是通过同样的突触输入传递给神经元的，而突触输入会刺激或抑制神经元的刺产生活动。(如果Klopf知道我们今天对神经调节系统的了解，他可能会将增强作用赋予神经调节输入，但他希望避免任何集中的训练信息来源。)过去突触前和突触后活动的突触局部痕迹在Klopf的假设中具有关键作用，即通过后来的奖励或惩罚来修改突触。他推测这些痕迹是由每个突触的分子机制实现的，因此不同于突触前神经元和突触后神经元的电活动。在

在这一章的参考文献和历史评论部分，我们要注意到其他人提出的一些类似的建议。
Klopf具体地推测，突触的功效是通过以下方式改变的。当一个神经元触发动作电位时，所有活跃于动作电位的突触都有资格接受它们的效率变化。如果动作电位在适当的时间内随着奖励的增加而增加，那么所有符合条件的突触的效率都会增加。对称地说，如果行为潜力在适当的时间内随着惩罚的增加而增加，符合条件的突触的效率就会降低。这是通过在突触前和突触后活动(或者更确切地说，突触前活动和突触后活动的配对(突触前活动参与导致的突触后活动)上触发合格跟踪来实现的——我们称之为偶然合格跟踪。这本质上是前一节中描述的参与者单元的三因素学习规则。
资格的形状和时间进程跟踪Klopf的理论反映了许多反馈循环的持续时间的神经元是嵌入式,其中一些谎言完全在有机体的大脑和身体,而其他延伸通过机体的外部环境由其运动和感觉系统。他的想法是，突触的资格追踪的形状就像一个直方图，是神经元被嵌入的反馈回路的持续时间。然后，合格跟踪的峰值将出现在该神经元参与的最普遍的反馈回路的持续时间。算法使用的资格痕迹在这本书里描述的简化版Klopf最初的想法,是指数(或几何)减少功能控制的参数λ和γ。这简化了模拟和理论，但我们将这些简单的合格跟踪作为更接近Klopf原始概念的跟踪的占位符，通过改进信贷分配过程，在复杂的增强学习系统中具有计算优势。
Klopf的享乐主义神经元假说并不像一开始看起来那样难以置信。一个研究得很好的例子是一个寻求刺激而不寻求刺激的单一细胞是大肠杆菌。这种单细胞生物的运动受到环境中化学刺激的影响，即趋化行为。它在液体环境中游动，通过旋转附着在其表面的毛发状结构鞭毛。(是的,它旋转时他们!)细菌环境中的分子与细菌表面的受体结合。结合事件调节细菌逆转鞭毛旋转的频率。每一次倒转都会导致细菌在原地打滚，然后朝着一个随机的新方向前进。少量的化学记忆和计算导致当细菌游向其生存所需的高浓度分子(引诱剂)时鞭毛反转的频率降低，而当细菌游向有害分子(驱虫剂)的高浓度时，鞭毛反转的频率增加。其结果是，细菌倾向于坚持游上引诱剂梯度，并倾向于避免游上驱虫剂梯度。
刚才描述的趋化行为叫做趋化作用。这是一种反复试验的行为，尽管不太可能涉及到学习:细菌需要少量的短期记忆来检测分子浓度梯度，但它可能不会维持长期记忆。人工智能先锋奥利弗

塞尔弗里奇将这种策略称为“运行和旋转”，并指出它作为一种基本的适应性策略的效用:“如果事情越来越好，继续以同样的方式前进，否则就四处移动”(塞尔弗里奇，1978,1984)。类似地，人们可能会认为神经元“游动”(当然不是字面意义上的“游动”)是一种由它所嵌入的复杂反馈回路集合组成的媒介，其作用是获得一种输入信号，并避免其他信号。然而，与细菌不同的是，神经元的突触强度保留了有关其过去反复试验行为的信息。如果这个视图的一个神经元的行为(或只是一种类型的神经元)似乎是合理的,那么如何神经元的闭环特性与环境互动是很重要的,对于理解其行为,神经元的环境由其他的动物与环境的动物作为一个整体进行交互。
Klopf的快乐主义神经元假说超越了单个神经元是增强学习因子的观点。他认为，智能行为的许多方面都可以被理解为一群自私自利的享乐主义神经元的集体行为的结果，这些神经元在构成动物神经系统的庞大社会或经济系统中相互作用。无论这种神经系统的观点是否有用，强化学习因子的集体行为对神经科学都有意义。我们接下来讨论这个问题。

\section{集体强化学习}

强化学习因子群体的行为与社会和经济系统的研究密切相关，如果克劳夫(Klopf)的享乐主义神经元假说是正确的，那么神经科学也是如此。上述假设actor-critic算法如何可能实现在大脑中只勉强地址这一事实的影响背侧和腹侧纹状体的细分,演员和评论家的各自位置根据假设,每个包含数以百万计的媒介棘神经元的突触接受改变调制相位的多巴胺神经元的活动。
图15.5a中的actor是k actor单元的单层网络。该网络产生的行为是向量(A1, A2，···，Ak)?假定驱动动物的行为。功效的突触的变化取决于所有这些单位强化信号δ。因为演员单位试图使δ尽可能大,δ有效地为他们作为奖励的信号(在这种情况下钢筋是一样的奖励)。因此，每个行动者单位本身就是一个强化学习主体——如果你愿意的话，它是一个享乐神经元。现在,情况尽可能简单,假设每一个单位接收相同的奖赏信号在同一时间(尽管如此,如上所示,假设在所有层次的多巴胺释放突触在相同条件下和在同一时间可能是一个简化)。
强化学习理论能告诉我们什么，当强化学习主体的所有成员根据一个共同的奖励信号学习时会发生什么?多智能体强化学习领域考虑了多智能体群体学习的许多方面。虽然这个领域超出了这本书的范围，但我们相信它的一些基本概念和结果与思考有关

关于大脑的扩散神经调节系统。在多智能体强化学习(和博弈论)中，所有的智能体都试图最大化同时接收到的共同奖励信号的情形被称为合作博弈或团队问题。
使团队问题变得有趣和具有挑战性的是，发送给每个代理的共同奖励信号评估整个人群产生的活动模式，也就是说，它评估团队成员的集体行动。这意味着任何单独的行为人对奖励信号的影响都是有限的，因为任何单独的行为人只对共同奖励信号所评估的集体行为的一个组成部分做出贡献。在这种情况下，有效的学习需要解决一个结构性的信用分配问题:哪些团队成员，或团队成员，应该因为一个有利的奖励信号而得到赞扬，还是因为一个不利的奖励信号而受到指责?这是一个合作的游戏，或者一个团队的问题，因为代理人在寻求增加相同的奖励信号:在代理之间没有利益冲突。如果不同的代理接收到不同的奖励信号，那么这个场景将是一场竞争游戏，每个奖励信号再次评估人群的集体行动，每个代理的目标是增加自己的奖励信号。在这种情况下，代理之间可能存在利益冲突，这意味着对某些代理有利的行为对其他代理不利。甚至决定什么是最好的集体行动应该是一个不平凡的方面的博弈论。这种竞争环境可能也与神经科学有关(例如，为了解释多巴胺神经元活动的异质性)，但这里我们只关注合作或团队的情况。
一个团队中的每个强化学习代理如何学会“做正确的事情”，从而使团队的集体行动得到高度的奖励?一个有趣的结果是,如果每个代理可以有效学习尽管奖励信号被大量的噪音,损坏,尽管缺乏完整的状态信息,那么人口作为一个整体将学会产生集体行动,提高评估的常见奖励信号,即使代理不能相互沟通。每个代理都面临着它自己的强化学习任务，它对奖励信号的影响被其他代理的影响深深地埋入了噪声中。事实上，对于任何代理，所有其他代理都是其环境的一部分，因为它的输入，传递状态信息的部分和奖励部分，都取决于其他代理的行为。此外，由于无法访问其他代理的操作，甚至无法访问决定其策略的参数，每个代理只能部分地观察其环境的状态。这使得每个团队成员的学习任务非常困难,但是如果每个使用强化学习算法能提高奖励信号即使在这些困难的条件下,强化学习代理可以学会团队产生集体行动,改善随着时间的推移评价团队的共同奖励的信号。
如果团队成员是神经元样的单位，那么每个单位都必须有目标，随着时间的推移增加奖励的数量，就像我们在第15.8节中描述的，行动者单位所做的那样。每个单元的学习算法必须有两个基本特征。首先，它必须使用或有资格的追溯。回想一下，在神经术语中，偶然的资格跟踪是在突触前输入时启动(或增加)的

参与导致突触后神经元放电。与此相反，一个非偶然性的合格线索，是由突触前输入独立于突触后神经元的作用而启动或增加的。正如第15.8节所解释的那样，通过保存在哪些国家采取了什么行动的信息，或有资格的追溯，可以根据这些参数在决定代理人行动中所作的贡献的价值来分配给代理人的政策参数。通过类似的推理，团队成员必须记住它最近的行为，以便它可以根据随后收到的奖励信号增加或减少产生该行为的可能性。偶然资格跟踪的操作组件实现此操作内存。然而，由于学习任务的复杂性，或有资格仅仅是信贷分配过程中的一个初步步骤:单个团队成员的行为与团队奖励信号的变化之间的关系是一个统计相关性，必须在许多试验中加以估计。或有资格是这一过程中必不可少的但初步的步骤。
使用非偶然资格跟踪学习在团队设置中根本不起作用，因为它不提供一种将行为与随后的奖励信号变化联系起来的方法。非偶然资格跟踪对于学习预测是足够的，就像actor - critics算法的批评家部分所做的那样，但是它们不支持学习控制，就像actor组件必须做的那样。一个类似于criticas的群体的成员可能仍然会收到一个共同的强化信号，但是他们都将学会预测相同的数量(在一个针对某一因素的方法中，这将是当前政策的预期回报)。每个人口成员在学习预测预期收益方面的成功程度将取决于它所获得的信息，而这对人口的不同成员可能是非常不同的。不需要人口产生不同的活动模式。这不是一个定义在这里的团队问题。
团队问题中对集体学习的第二个要求是，团队成员的行为必须具有可变性，以便团队能够探索集体行动的空间。增强学习代理团队做到这一点的最简单方法是，让每个成员通过输出的持久性变化独立地探索自己的行为空间。这将导致整个团队改变其集体行动。例如，第15.8节中描述的一个参与者单元团队探索了集体行动的空间，因为每个单元的输出作为一个伯努利-逻辑单元，在概率上依赖于其输入向量分量的加权和。加权和偏差会使概率上升或下降，但总是存在可变性。由于每个单元都使用了一种增强策略梯度算法(第13章)，所以每个单元在随机探索自己的行动空间的同时，调整自己的权重，目标是使自己所经历的平均奖励率达到最大。可以证明，正如Williams(1992)所做的那样，一个伯努利-逻辑强化单元的团队针对团队共同奖励信号的平均速率实现了一个整体的策略梯度算法，其中的行为是团队的集体行为。
此外，Williams(1992)表明，当团队中的各单元相互连接形成多层ANN时，使用增援的伯努利逻辑单元的团队会提升平均奖励梯度。在这种情况下，奖励信号被广播到所有单位

网络，尽管报酬可能只取决于网络输出单元的集体行动。这意味着伯努利-逻辑强化单元的多层团队学习起来就像被广泛使用的错误反向传播方法训练的多层网络，但在这种情况下，反向传播过程被广播的奖励信号所取代。在实践中，错误反向传播方法要快得多，但是强化学习团队方法作为一种神经机制更有可能，特别是考虑到在15.8节中讨论的关于奖赏调制STDP的知识。
通过团队成员的独立探索进行探索是团队探索的最简单方式;如果团队成员协调他们的行动，将注意力集中在集体行动空间的特定部分，或者通过相互通信，或者通过响应共同的输入，则可以使用更复杂的方法。还有一种机制比或有资格的机制更复杂，以解决结构性信贷分配问题，这在团队问题中更容易解决，因为可能的集体行动在某种程度上受到限制。一个极端的例子是一个赢家通吃的安排(例如，大脑侧抑制的结果)，它限制了对只有一个或少数几个团队成员的集体行动。在这种情况下，获胜者会得到奖励或惩罚的奖励或责备。
合作游戏(或团队问题)和非合作游戏问题的学习细节超出了这本书的范围。本章末尾的参考文献和历史评论部分引用了一些相关的出版物，包括对集体强化学习对神经科学的影响的广泛引用。

\section{基于模型的大脑方法}

强化学习在无模型和基于模型的算法之间的区别被证明对思考动物学习和决策过程是有用的。第14.6节讨论了这一区别如何与习惯性和目标导向的动物行为相一致。上述关于大脑如何实现一种行为-批评算法的假设与动物的习惯性行为模式有关，因为基本的行为-批评方法是无模型的。什么神经机制负责产生目标导向行为，它们如何与那些潜在的习惯性行为互动?
研究与这些行为模式有关的大脑结构问题的一种方法是使老鼠大脑的某个区域失活，然后观察老鼠在结果减值实验中的行为(第14.6节)。这些实验的结果表明，上面描述的演员-批评家假设在把演员放在背纹状体中过于简单。使背侧纹状体的一部分——背外侧纹状体(DLS)失去活性，削弱了习惯学习，使动物更依赖目标导向的过程。另一方面，使背侧纹状体(DMS)失活会削弱目标导向的过程，要求动物更多地依赖于习惯学习。像这样的结果支持了啮齿动物的DLS更多地涉及无模型的过程，而它们的DMS则更多地涉及基于模型的过程。的结果

在类似的实验中，用功能性神经成像对人类受试者进行的研究，以及对非人类灵长类的研究，都支持这样的观点，即灵长类大脑中的类似结构在习惯性和目标导向的行为模式中存在差异。
其他研究发现，与基于模型的大脑前额叶皮层相关的活动，前额叶皮层的最前端部分与执行功能有关，包括计划和决策。具体涉及到的是眼窝前额皮质(OFC)，前额皮质的部分位于眼睛上方。人类的功能神经成像，以及猴子单个神经元活动的记录，揭示了OFC中与生物意义刺激的主观奖励价值相关的强烈活动，以及与行动预期的奖励相关的活动。尽管这些结果并非毫无争议，但表明OFC在目标导向的选择中扮演了重要角色。这可能对动物环境模型的奖励部分至关重要。
另一个涉及到基于模型的行为的结构是海马体，这是记忆和空间导航的关键结构。老鼠的海马体在老鼠以目标导向的方式在迷宫中穿行的能力中起着关键作用，这让托尔曼产生了动物在选择行为时使用模型或认知地图的想法(第14.5节)。海马体也可能是人类想象新体验能力的重要组成部分(Hassabis和Maguire, 2007;Barry Ólafsdóttir,萨利姆,哈萨比斯和施皮尔,2015)。
最直接地暗示海马体参与规划的发现——在决策过程中需要一个环境模型——来自于解码海马体神经元活动的实验，以确定空间海马体活动的哪个部分在每时每刻的基础上表现出来。当老鼠在迷宫中的一个选择点停下来时，海马体中空间的表现会沿着动物可以从那个点出发的可能路径向前(而不是向后)(Johnson and Redish, 2007)。此外，这些扫掠所代表的空间轨迹与老鼠随后的航行行为(Pfeiffer and Foster, 2013)密切相关。这些结果表明，海马体对于动物环境模型的状态转换部分至关重要，而海马体是一个系统的一部分，该系统使用模型来模拟未来可能的状态序列，以评估可能的行动过程的后果:一种规划形式。
上面描述的结果增加了大量关于目标导向或基于模型的学习和决策的神经机制的文献，但是许多问题仍然没有得到解答。例如，与DLS和DMS在结构上相似的区域如何能成为学习模式和行为模式的基本组成部分，这些模式和行为模式与无模型算法和基于模型的算法一样不同?独立的结构是否负责(我们称之为)环境模型的转换和奖励组件?是否所有的计划都是在决策时通过模拟未来可能的行动路线进行的，就像海马状突起的向前清扫活动所显示的那样?换句话说，是否都在计划类似于推出算法的东西(第8.10节)?或者是模型有时在后台进行精炼或重新计算值信息，如Dyna体系结构(第8.2节)所示?大脑如何在习惯的使用和目标导向系统之间进行仲裁?事实上，神经之间是否有明显的分离

基质的系统?
证据并没有指向最后一个问题的积极答案。多尔(Doll)、西蒙(Simon)和Daw(2012)总结了这种情况，他们写道:“基于模型的影响在大脑处理奖励信息的地方或多或少都是普遍存在的。”这包括多巴胺信号本身，除了奖励预测错误被认为是无模型过程的基础之外，它还可以显示基于模型的信息的影响。
通过强化学习的无模型和基于模型的区别，继续进行神经科学研究，有可能增强我们对大脑中习惯性和目标导向过程的理解。更好地理解这些神经机制，可能会导致将无模型方法和基于模型的方法结合在一起的算法，这种方法在计算强化学习中还没有被探索过。

\section{上瘾}

了解药物滥用的神经基础是神经科学的一个高度优先目标，有潜力为这一严重的公共健康问题开发新的治疗方法。一种观点认为，对药物的渴求是同样的动机和学习过程的结果，这些动机和学习过程使我们寻求满足我们生理需要的自然的有益体验。令人上瘾的物质，通过强烈的强化，有效地利用了我们的学习和决策的自然机制。这似乎是合理的，因为尽管不是所有的药物滥用都会直接或间接地增加多巴胺在纹状体神经元轴突终端区附近的水平，而纹状体的大脑结构与正常的基于奖励的学习密切相关(第15.7节)。但是与毒瘾相关的自毁行为并不是正常学习的特征。多巴胺介导的学习有什么不同呢?成瘾是正常学习的结果吗?在我们进化的历史中，我们基本上无法获得这些物质，所以进化不能选择它们的破坏性影响。或者成瘾物质会以某种方式干扰正常的多巴胺介导的学习吗?
多巴胺神经元活动的奖赏预测错误假说及其与TD学习的联系是一个模型的基础，这个模型是由Redish(2004)提出的，但肯定不是成瘾的全部特征。该模型基于可卡因和其他成瘾药物的使用会产生短暂的多巴胺增加的观察。在模型中,假设这种多巴胺增加增加TD错误,δ,无法取消了价值函数的变化。换句话说,而δ是减少的程度,一个正常的奖励是前期预测的事件(15.6节),δ由于上瘾刺激的贡献并不减少随着奖励的信号变得预测:不能“预测药物的回报。“模型通过阻止δ成为负奖励信号时由于一种上瘾的药物,从而消除TD学习状态的纠错功能与政府相关的药物。其结果是这些状态的值不受约束地增加，使导致这些状态的行动优先于所有其他状态。

与Redish的模型相比，上瘾行为要复杂得多，但是这个模型的主要思想可能是一个谜。或者这个模型可能具有误导性。多巴胺似乎在所有成瘾类型中都没有发挥关键作用，而且并不是每个人都同样容易产生成瘾行为。此外，该模型不包括伴随慢性药物服用而产生的许多回路和大脑区域的变化，例如，这些变化会导致药物在反复使用后效果逐渐减弱。成瘾还可能涉及基于模型的过程。尽管如此，Redish的模型说明了如何利用强化学习理论来理解一个主要的健康问题。同样地，强化学习理论在新的计算精神病学领域的发展中也有影响，该领域旨在通过数学和计算方法提高对精神障碍的理解。

\section{总结}

大脑奖赏系统所涉及的神经通路是复杂而不完全了解的，但是神经科学的研究正朝着理解这些通路及其在行为中的作用迅速进展。这项研究揭示了大脑奖励系统和强化学习理论之间惊人的对应关系。
多巴胺神经元活动的奖赏预测错误假说是由科学家提出的，他们认识到TD错误的行为与产生多巴胺的神经元的活动有惊人的相似之处，多巴胺是哺乳动物奖励相关的学习和行为所必需的神经递质。实验在1980年代末和1990年代在实验室的神经学家Wolfram舒尔茨表明多巴胺神经元响应的事件与大量的活动,称为相位的反应,只有在动物并不认为这些事件,这表明多巴胺神经元信号奖励预测错误,而不是奖励本身。此外，这些实验表明，当动物学习根据先前的感觉线索来预测一个有意义的事件时，多巴胺神经元的阶段活动转变为早期的预测线索，而减少为后期的预测线索。这与TD误差作为增强学习剂学习预测奖励的效果相似。
其他实验结果坚定地证明，多巴胺神经元的阶段性活动是学习的强化信号，通过多巴胺产生神经元的大量分支，到达大脑的多个区域。这些结果是一致的回报之间的区别我们信号,Rt,强化信号,TD的错误δt我们目前的大多数算法。多巴胺神经元的阶段反应是强化信号，而不是奖励信号。
一个突出的假设是，大脑执行某种类似于行为-批评算法的东西。大脑中的两种结构(纹状体的背侧和腹侧分支)在基于奖励的学习中都扮演着重要的角色，它们可能分别像演员和批评家一样发挥作用。TD错误是行动者和批评家的强化信号，这与多巴胺神经元轴突瞄准纹状体的背侧和腹侧亚区这一事实吻合得很好;多巴胺似乎是在两个结构中调节突触可塑性的关键;对神经调节器的目标结构的影响，如多巴胺，取决于目标结构的性质，而不仅仅是神经调节器的性质。

演员和批评家可以由神经单元组成的ANNs来实现，这些神经单元具有基于第13.5节描述的政策梯度演员-批评家方法的学习规则。这些网络中的每一个连接就像大脑中神经元之间的突触，学习规则对应于控制突触效率如何随着突触前神经元和突触后神经元活动的功能而变化的规则，以及与多巴胺神经元输入相对应的神经调节输入。在此设置中，每个突触都有自己的合格跟踪记录涉及该突触的过去活动。演员和评论家学习规则之间唯一的区别是,他们使用不同的资格的痕迹:评论家单元的痕迹non-contingent因为他们不涉及评论家单元的输出,而演员单位的痕迹或有,因为除了演员单位的输入,他们依靠演员单元的输出。假设在大脑中实施一个行为-批评系统，这些学习规则分别对应于控制皮质纹状体突触可塑性的规则，这些突触将信号从皮质传递到背侧和腹侧纹状体的主要神经元，这些突触也接受来自多巴胺神经元的输入。

一个演员单元的学习规则在演员-评论家网络中，与奖赏调节的spiker -时间依赖的可塑性密切相关。在spike-timing依赖性可塑性(STDP)中，突触前和突触后活动的相对时间决定了突触改变的方向。在奖赏调制的STDP中，突触的变化除了依赖于神经调节器(如多巴胺)外，还依赖于在满足STDP条件后能持续10秒的时间窗。越来越多的证据表明，奖赏调节的STDP发生在皮质纹状体突触，在这里，行动者的学习发生在假定的行为-批评系统的神经实施中，这增加了假设的合理性，即某些动物的大脑中存在着类似行为-批评系统的东西。

突触合格性的概念和行动者学习规则的基本特征来源于Klopf的“享乐神经元”假说(Klopf, 1972, 1981)。他推测，单个的神经元寻求获得奖励，并通过调整其突触的功效，以奖励或惩罚其动作电位的结果来避免惩罚。神经元的活动会影响其后期的输入，因为神经元内嵌在许多反馈回路中，其中一些在动物的神经系统和身体内，而另一些则通过动物的外部环境。Klopf认为，如果突触参与了神经元的激活(使其成为资格跟踪的偶然形式)，那么它们就暂时被标记为有资格进行修改。如果增强信号在突触合格时到达，则对突触的有效性进行修改。我们提到了一个细菌的趋化行为，它是一个单细胞的例子，它指导它的运动，以寻找一些分子并避开其他分子。

多巴胺系统的一个显著特征是，纤维向大脑的多个部分广泛释放多巴胺。虽然很可能只有一些多巴胺神经元群会发出同样的强化信号，但如果这个信号到达参与肌动类型学习的许多神经元的突触，那么情况就可以了

被建模为一个团队问题。在这种类型的问题中，一个增强学习代理集合中的每个代理接收相同的增强信号，该信号依赖于集合中的所有成员或团队的活动。如果每个团队成员都使用了一个足够强大的学习算法，那么即使团队成员之间没有直接的交流，团队也可以通过全局广播增强信号来提高整个团队的性能。这与多巴胺信号在大脑中的广泛分散是一致的，并且为训练多层网络提供了一种神经上可行的替代方法。
无模型强化学习和基于模型的强化学习之间的区别有助于神经科学家研究习惯性学习和目标导向学习和决策的神经基础。到目前为止的研究表明，他们的大脑中有一些区域比其他区域更参与一种类型的过程，但是目前的情况还不清楚，因为没有模型的和基于模型的过程在大脑中似乎并没有整齐地分开。许多问题仍未得到解答。也许最有趣的是，海马体，一个传统上与空间导航和记忆有关的结构，似乎参与了模拟未来可能的行动路线，作为动物决策过程的一部分。这表明，它是使用环境模型进行规划的系统的一部分。
强化学习理论也影响着对药物滥用背后神经过程的思考。基于奖励预测误差假设，建立了药物成瘾特征的模型。它提出，像可卡因这样的成瘾刺激物会使TD学习产生与毒品摄入有关的行为价值的无限增长。这还远不是一个完整的成瘾模型，但它说明了如何从计算的角度提出理论，可以通过进一步的研究来检验。计算精神病学的新领域同样侧重于计算模型的使用，其中一些模型来自于强化学习，以更好地理解精神障碍。
本章仅触及强化学习的神经科学与计算机科学与工程强化学习的发展如何相互影响的表面。增强学习算法的大多数特性都是纯粹基于计算的考虑，但也有一些特性受到神经学习机制的假设的影响。值得注意的是，随着实验数据的积累，大脑的奖励过程，许多纯计算动机的增强学习算法的特点，都是与神经科学数据一致的。计算强化学习的其他特性,如资格痕迹和团队的能力强化学习代理学习集体行动的影响下一个globally-broadcast强化信号,也可能变成平行实验数据作为神经科学家继续解开半球动物的学习和行为的神经基础。

\section{书目的和历史的言论}

在这本书中，关于学习和决策的神经科学与强化学习的方法之间的相似性的出版物数量是巨大的。我们只能举出一小部分。《新和合》(2009)、《大安》(2008)、《金彻》(2011)、《勒德维格》、《贝勒玛》、《皮尔森》(2011)、《沙阿》(2012)都是不错的起点。
强化学习理论与经济学、进化生物学和数学心理学一起，正在帮助建立人类和非人类灵长类的神经机制选择的定量模型。这一章以学习为中心，只略微涉及决策的神经科学。Glimcher(2003)引入了“神经经济学”领域，强化学习有助于从经济学的角度研究决策的神经基础。参见《一瞥》和《费尔》(2013)。Dayan和Abbott(2001)关于神经科学的计算和数学建模的文本包括强化学习在这些方法中的作用。Sterling和Laughlin(2015)研究了学习的神经基础，研究了能够有效适应行为的通用设计原则。

15.1基础神经科学有许多很好的论述。坎德尔,施瓦兹、Jessell
Siegelbaum和Hudspeth(2013)是一个权威和非常全面的来源。

15.2 Berridge和Kringelbach(2008)回顾了奖励和快乐的神经基础，
指出奖励处理有许多维度和涉及许多神经系统。空间阻止了对Berridge和Robinson(1998)有影响的研究的讨论，他们区分了刺激的享乐影响，他们称之为“喜欢”，以及他们称之为“渴望”的动机效应。Hare O 'Doherty、Camerer、Schultz和Rangel(2008)从经济学的角度研究了价值相关信号的神经基础，区分了目标值、决策值和预测错误。决策价值是目标价值减去行动成本。参见兰格尔,卡默勒,蒙塔古(2008),兰赫尔和兔(2010),彼得斯和布鲁里溃疡̈秋儿(2010)。
15.3多巴胺神经元活动的奖赏预测误差假说是最多的
舒尔茨、大安和蒙塔古(1997)在显著位置上进行了讨论。这个假设是由蒙塔古、达扬和塞杰诺斯基(1996)首次明确提出的。正如他们所说的假设，它指的是奖励预测误差(RPEs)，但不是特别针对TD误差;然而，他们对假设的发展清楚地表明，他们指的是TD错误。最早认识到的TD-error/多巴胺连接，我们知道的是Montague, Dayan, Nowlan, Pouget和Sejnowski(1993)，他们提出了一种由Schultz组的多巴胺信号的结果驱动的，由TD-error调制的Hebbian学习规则。Quartz、Dayan、Montague和Sejnowski(1992)在一篇摘要中也指出了这种联系。Montague和Sejnowski(1994)强调了预测在大脑中的重要性，并概述了如何通过弥漫性神经系统(如多巴胺系统)实现由TD错误调制的Hebbian学习预测。托诺尼,Friston Reeke、斯波恩Edelman(1994)提出了一个大脑中价值依赖性学习的模型，在这个模型中，突触的变化是由全球神经调节信号(虽然他们没有单独列出多巴胺)提供的td样错误所介导的。mont -tague, Dayan, Person和Sejnowski(1995)提出了一个利用TD误差的蜜蜂觅食模型。该模型基于Hammer, Menzel和同事的研究(Hammer and Menzel, 1995;哈默(1997)显示神经调节物质章鱼胺在蜜蜂中起强化信号的作用。Montague等人(1995)指出，多巴胺可能在脊椎动物的大脑中起着类似的作用。Barto (1995a)将actor -批评家架构与basal-ganglionic电路联系起来，并讨论了TD学习与Schultz团队的主要结果之间的关系。胡克、亚当斯(Adams)和巴托(Barto)(1995)建议道明学习和表演评论家架构如何映射到基底神经节的解剖学、生理学和分子机制。Doya和Sejnowski(1998)扩展了他们关于鸟鸣学习模型的早期论文(Doya和Sejnowski,1995)，包括一个与多巴胺识别的td样错误，以加强选择要记忆的听觉输入。O 'Reilly和Frank(2006)和O 'Reilly、Frank、Hazy和Watz(2007)认为，阶段性多巴胺信号是RPEs而不是TD错误。为了支持他们的理论，他们引用了变量间刺激间隔的结果，这些结果与简单的TD模型的预测不匹配，以及观察到二阶条件作用之外的高阶条件作用很少被观察到，而TD学习也不是那么有限。Dayan和Niv(2008)讨论了强化学习理论和奖励预测错误假说如何与实验数据相一致的“善、恶、丑”。格里姆彻(2011)回顾了支持奖励预测误差假说的实证结果，强调了该假说对当代神经科学的重要性。

15.4 Graybiel(2000)是一篇关于基底神经节的简介。提到的实验涉及optogenetic多巴胺神经元的激活是由蔡,张Adamantidis,存根,Bonci,de Lecea和戴瑟罗斯(2009),Steinberg Keiflin,Boivin,威滕,戴瑟罗斯,和Janak(2013),和Claridge-Chang Roorda,Vrontou,Sjulson,李,赫希,Miesenböck(2009)。Fiorillo、Yun和Song(2013)、Lammel、Lim和Malenka(2014)、Saddoris、Cacciapaglia、Wightmman和Carelli(2015)等研究表明，多巴胺神经元的信号特性是针对不同目标区域的。rpe信号神经元可能属于多种多巴胺神经元中的一种，它们具有不同的靶点和不同的功能。Eshel, Tian, Bukwich，和Uchida(2016)发现在小鼠的经典条件作用下，在侧VTA中多巴胺神经元的奖赏预测错误反应的同质性，尽管他们的结果并不排除在更广泛的区域内的反应多样性。Gershman、Pesaran和Daw(2009)研究了强化学习任务，这些任务可以用单独的奖励信号分解成独立的部分，并在人类神经成像数据中找到证据，表明大脑利用了这种结构。

15.5舒尔茨1998年的调查文章是一个很好的之间́e到非常广泛的文学关于奖励预测多巴胺神经元的信号。Berns, McClure, Pagnoni和Montague (2001)， Breiter, Aharon, Kahneman, Dale, and Shizgal (2001)， Pagnoni, Zink, Montague和Berns (2002)， O 'Doherty, Dayan, Friston, Critchley，和Dolan(2003)描述了脑功能成像研究支持在人脑中存在像TD错误这样的信号。

15.6本节大致按照Barto(1995年a)的说法解释TD错误是如何模拟的舒尔茨小组对多巴胺神经元的阶段反应的主要结果。

15.7这部分主要是基于高桥、舍恩鲍姆、和合本(2008)和《圣经》(2009)。据我们所知，Barto(1995年a)、Houk、Adams和Barto(1995年)首先推测了在基底神经节中可能实现的行为批判算法。O 'Doherty、Dayan、Schultz、Deichmann、Friston和Dolan(2004)在对人体进行仪器调节时进行功能性磁共振成像的基础上，提出行动者和批评家最可能分别位于背侧和腹侧纹状体。Gershman、Moustafa和Ludvig(2014)关注了时间在基底神经节强化学习模型中的表现方式，讨论了各种计算方法对时间表示法的证据和含义。本节所描述的演员-评论家架构的假设神经实现中几乎没有关于已知的基底神经节解剖学和生理学的细节。除了Houk、Adams和Barto(1995)的更为详细的假设之外，还有一些其他的假设包括与解剖学和生理学更具体的联系，并声称可以解释更多的数据。这些假设包括Suri和Schultz (1998, 1999)， Brown, Bullock和Grossberg (1999)， Contreras-Vidal和Schultz (1999)， Suri, Bargas, Arbib (2001)， O’reilly and Frank (2006)， O’reilly, Frank, Hazy，和Watz(2007)。乔尔(Joel, Niv)和鲁平(Ruppin)(2002)批判性地评估了其中几个模型的解剖学合理性，并提出了一种替代方案，以适应一些被忽视的基底神经节回路的特征。

15.8此处讨论的演员学习规则比in中的规则要复杂得多Barto等人早期的演员-评论家网络(1983)。Actor-unit资格痕迹在网络的痕迹就在××(St)而不是完整的(−π(|圣,θ))x(St)。这项工作没有受益于第13章提出的政策梯度理论或威廉姆斯的贡献(1986年，1992年)，他展示了伯努利-逻辑单元的ANN是如何实现政策梯度方法的。Reynolds和Wickens(2002)提出了皮质纹状体通路中突触可塑性的三因素规则，其中多巴胺调节皮质纹状体突触效能的变化。他们讨论了这种学习规则的实验支持和可能的分子基础。的示范spike-timing-dependent可塑性(STDP)是归因于马克拉姆,陆̈bke,Frotscher,和Sakmann(1997),从早期的实验证据
Levy和Steward(1983)等人认为突触前和突触后峰值的相对时间对于诱发突触效率的改变至关重要。Rao和Sejnowski(2001)提出STDP是如何由TD-like机制产生的，这种机制存在于具有非偶然资格的突触上，持续时间约为10毫秒。Dayan(2002)认为这需要一个错误，就像Sutton和Barto (1981a)经典条件作用的早期模型，而不是一个真正的TD错误。大量关于奖赏调制STDP的文献中有代表性的出版物有Wickens (1990)， Reynolds和Wickens (2002)， calab西方雷西，Picconi, Tozzi和Di Filippo(2007)。Pawlak和Kerr(2008)的研究表明，多巴胺对于诱导STDP是必要的。参见Pawlak, Wickens, Kirkwood和Kerr(2010)。Yagishita、Hayashi-Takagi、Ellis-Davies、Urakubo、Ishii和Kasai(2014)发现，多巴胺只在STDP刺激后0.3秒至2秒的时间窗内促进小鼠中等刺状神经元的脊柱增大。Izhikevich(2007)提出并探讨了使用STDP时序条件触发或有资格跟踪的想法。Frémaux Sprekeler,郭士纳(2010)提出的理论条件成功的学习规则基于reward-modulated STDP。

15.9 Klopf的享乐主义神经元假说(Klopf 1972, 1982)启发了我们的演员-批评家算法实现为一个具有单个神经元样单元的ANN，称为actor单元，实现一个类似于效率的学习规则(Barto, Sutton, and Anderson, 1983)。与Klopf的突触-局部资格相关的想法已经被其他人提出。Crow(1968)提出，皮质神经元突触的变化对神经活动的后果是敏感的。他强调需要解决神经活动的时间延迟和它对突触可塑性的奖赏调节形式的影响，他提出了一种偶然的资格形式，但与整个神经元相关，而不是单个的突触。根据他的假设，一波神经活动导致在波中所涉及的细胞的短期变化，这些细胞是从没有被激活的细胞的背景中挑选出来的。这样的细胞是
由于对奖励信号的短期改变而变得敏感……以这样一种方式，如果这样的信号在变化的衰减时间结束之前发生，细胞间的突触连接就会变得更加有效。(乌鸦,1968)乌鸦反对先前的提议,反射神经回路扮演这一角色,指出奖励的影响信号电路将“…建立突触联系导致混响(也就是说,那些参与活动时的奖励信号),而不是那些路径导致自适应运动输出。Crow进一步假设奖励信号是通过一个“独特的神经纤维系统”传递的，这可能是Olds和Milner(1954年)所使用的神经纤维系统，它可以将突触连接“从短期转变为长期形式”。
在另一种有远见的假设中，米勒提出了一种类似于效果的学习规则，其中包括了突触局部的或有资格的追踪:

…据设想，在特定的感官环境下，神经B碰巧会触发“有意义的活动爆发”，然后转化为运动行为，
然后改变情况。必须假定有意义的
在神经层面上，突发对它自己的所有突触都有影响
在那个时候很活跃……因此，初步选择要加强的突触，虽然还没有真正加强它们。
…加强信号……做最后的选择…和完成
确定的变化在适当的突触。(米勒,1981,p . 81)

米勒的假说还包括critic-like机制,他称之为“感官分析器装置,”根据经典条件作用工作原理为神经元提供强化信号,这样他们会学习从低股价较高的国家,从而预测TD的使用错误的强化信号actor-critic架构。米勒的想法不仅与Klopf的想法相似(除了明显地调用了一个独特的“增强信号”之外)，还预测了奖赏调制STDP的一般特性。
Seung(2003)称之为“享乐主义突触”(hedonistic synapse)的一个相关但不同的观点是，突触以效应定律的方式分别调整了释放神经递质的可能性:如果释放后有奖励，释放的可能性就会增加，如果释放失败后奖励会减少。这与明斯基1954年在普林斯顿大学博士论文中使用的学习计划本质上是一样的，他把这种类似突触的学习元素称为SNARC(随机神经模拟强化计算器)。偶然资格也与这些想法有关，尽管它取决于单个突触的活动而不是突触后神经元。也有关于Unnikrishnan和Venugopal(1994)的提议，该提议使用了Harth和Tzanakou(1974)的基于相关的方法来调整ANN weights。
Frey和Morris(1997)提出了“突触标记”的概念，用于诱导突触效能的长期增强。尽管与Klopf的资格相似，他们的标签假设是由一个突触的暂时强化组成的，这个突触可以通过随后的神经元激活转化为一个长期的强化。O 'Reilly and Frank(2006)和O 'Reilly (Frank)、Frank、Hazy和Watz(2007)的模型使用工作记忆来桥接时间间隔而不是资格痕迹。Wickens和Kotter(1995)讨论了突触资格的可能机制。He, Huertas, Hong, Tie, Hell, Shouval, Kirkwood(2015)提供了证据，证明了时序过程中皮层神经元突触中是否存在偶然性合格的痕迹，就像那些合格的痕迹Klopf假设的那样。
Barto(1989)讨论了使用与细菌趋化有关的学习规则的神经元的隐喻。Koshland对细菌趋化性的广泛研究的部分原因是细菌的特征和神经元特征的相似性(Koshland, 1980)。参见伯格(1975)。Shimansky(2009)提出了一个
突触学习规则有点类似于上面提到的，每个突触都像一个趋化细菌。在这种情况下，一组突触“游动”到突触权重值的高维空间吸引物。Montague, Dayan, Person和Sejnowski(1995)提出了一种蜜蜂觅食行为的化学模拟模型，涉及神经调质章鱼胺。

15.10加强学习主体在团队和游戏中的行为研究
问题有很长的历史，大致分为三个阶段。据我们所知，第一阶段是由俄国数学家和物理学家m.l. Tsetlin进行的。他的一组作品在1966年去世后被出版为《采编林》(1973)。我们的第1.7和4.8节涉及到他关于学习自动机的研究。Tsetlin收藏还包括学习自动机的研究团队和游戏问题,导致以后的工作在这一领域使用随机学习自动机作为被纳兰德拉和Thathachar(1974、1989),Viswanathan和Narendra(1974),Lakshmivarahan和Narendra(1982),纳兰德拉和惠勒(1983),和Thathachar Sastry(2002)。Thathachar和Sastry(2011)是一个更全面的描述。这些研究大多局限于非联想学习自动机，这意味着它们没有涉及到联想或语境的强盗问题(2.9节)。
第二阶段从学习自动机的扩展到联想，或情境，案例开始。Barto、Sutton、Brouwer(1981)和Barto和Sutton(1981)在单层ANNs中实验了联合随机学习自动机，并向其广播了一个全局增强信号。学习算法是Harth和Tzanakou(1974)的Alopex算法的联合扩展。Barto等人将这种学习关联搜索元素(ase)称为神经元类元素。Barto和阿南丹(1985)介绍了一个关联强化学习算法称为关联reward-penalty(AR−P)算法。将随机学习自动机理论与模式分类理论相结合，证明了算法的收敛性。Barto(1985、1986)和Barto和约旦(1987)描述的结果与基于“增大化现实”技术团队−P单元连接成多层人工神经网络,显示他们可以学习非线性函数,XOR等,globally-broadcast强化信号。Barto(1985)广泛地讨论了ANNs的这种方法，以及这种学习规则是如何与当时文献中的其他人相关的。Williams(1992)对这门学习规则进行了数学分析和拓展，并将其应用于训练多层神经网络的误差反向传播方法。Williams(1988)描述了将反向传播和强化学习结合起来进行ANNs训练的几种方法。威廉姆斯(1992)表明,AR−P算法的一个特例是一个增强的算法,虽然得到了更好的结果与普通AR−P算法(Barto,1985)。

增强学习剂团队的第三个兴趣阶段受到了对多巴胺作为一种广为传播的神经调节器的作用的理解的增强和对奖赏调节STDP存在的推测的影响。与早期的研究相比，这项研究更关注突触可塑性的细节和神经科学的其他约束。出版物包括以下(按时间顺序和按字母顺序):巴特利特和巴克斯特(1999、2000),谢和Seung(2004),巴拉和梅尔(2007),Farries和Fairhall(2007),御马(2007),Izhikevich(2007),Pecevski,马斯河,和Legenstein(2008),Legenstein,Pecevski,和马斯河(2008),Kolodziejski,Porr,和我们̈rgö水獭(2009),Urbanczik森(2009),和Vasilaki Frémaux,Urbanczik,森,郭士纳(2009)。NowéVrancx,De Hauwere(2012)回顾了最近的多智能体强化学习在更广阔的领域发展

Yin和Knowlton(2006)回顾了货币贬值专家的研究成果
对啮齿类动物的研究表明，习惯性行为和目标导向行为(心理学家使用这个短语)分别与背外侧纹状体(DLS)和背侧纹状体(DMS)的处理有关。Valentin, Dickinson和O 'Doherty(2007)在《结果-贬值》中对人类受试者的功能成像实验结果表明，眶额皮层(OFC)是目标导向选择的重要组成部分。Padoa-Schioppa和Assad(2006)的单单元录音支持了OFC在编码价值指导选择行为方面的作用。Rangel, Camerer，和Montague(2008)和Rangel and Hare(2010)从神经经济学的角度回顾了大脑如何做出目标导向的决策。Pezzulo、van der Meer、Lansink和Pennartz(2014)回顾了内部生成序列的神经科学，并提出了这些机制如何可能是基于模型规划的组成部分的模型。Daw和Shohamy(2008)提出，虽然多巴胺信号与习惯性的或无模型的行为有很好的联系，但是其他的过程涉及目标导向的，或基于模型的行为。由Bromberg-Martin、Matsumoto、Hong和Hikosaka(2010)所做的实验数据表明，多巴胺信号包含与习惯和目标导向行为相关的信息。多尔(Doll)、西蒙(Simon)和多瓦(Daw)(2012)认为，在大脑中，可能没有明确的划分机制来服务于习惯性学习和目标导向的学习和选择。

15.12 Keiflin和Janak(2015)回顾了TD错误和成瘾之间的联系。
Nutt, Lingford-Hughes, Erritzoe和Stokes(2015)批判性地评估了成瘾是由多巴胺系统紊乱引起的假设。Montague、Dolan、Friston和Dayan(2012)概述了计算精神病学领域的目标和早期努力，Adams、Huys和Roiser(2015)回顾了更近期的进展。
